### From https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/lora3d/

1. Introduction
   Recently, numerous 3D geometric foundation models have emerged as promising solutions 
   for various 3D computer vision tasks such as 3D reconstruction, camera pose estimation, and novel view rendering. 
   These models can rapidly establish cross-view correspondences and directly predict 3D scene geometry 
   from sparse RGB images.
   Generally, due to extensive Transformer pretraining on large datasets, they show strong zero-shot performance 
   across new tasks.

   However, the performance of these pretrained models can degrade under challenging conditions. 
   For instance, DUSt3R's pairwise reconstruction accuracy significantly drops when certain regions are observed 
   from only a single viewpoint. 
   This performance degradation is attributed to the intrinsic complexity of 3D shape inference tasks, 
   which ideally require much larger datasets to fully capture the distribution of real-world 3D data. 
   Nevertheless, high-quality training data is scarce due to the difficulty in annotating real 3D scenes, 
   limiting the performance of pretrained models.

   To address this issue, this paper proposes an efficient self-calibration technique. 
   Using only sparse RGB images, the method specializes a pretrained 3D foundation model to a target scene‚Äîwithout
   requiring manual labeling, camera calibration, or external priors.
   Specifically, by leveraging multi-view consistency of 3D point locations, 
   the model‚Äôs predictions are refined and selected to generate pseudo-labels. 
   To ensure the accuracy of these pseudo-labels, the authors develop a robust global optimization method 
   that aligns and calibrates multi-view predictions while correcting prediction confidence. 
   Since the calibrated confidence strongly correlates with pseudo-label accuracy, 
   the method enables selecting high-confidence data for LoRA fine-tuning of the pretrained model.

   LoRA3D, the proposed method, completes the self-calibration process in under 5 minutes on a single GPU 
   and can achieve performance improvements of up to 88%.

2. Preliminaries
   -a. DUSt3R
       DUSt3R takes a pair of RGB images (ùêº_ùëñ,ùêº_ùëó) as input and directly predicts point maps ùëã_(ùëñ,ùëñ),ùëã_(ùëó,ùëñ)‚ààùëÖ^(ùêª√óùëä√ó3)
       and confidence maps ùê∂_(ùëñ,ùëñ),ùê∂_(ùëó,ùëñ)‚ààùëÖ^(ùêª√óùëä√ó1):
       (ùëã_(ùëñ,ùëñ),ùê∂_(ùëñ,ùëñ)),(ùëã_(ùëó,ùëñ),ùê∂_(ùëó,ùëñ)) = DUSt3R(ùêº_ùëñ,ùêº_ùëó)
       The point maps represent 3D coordinates for views ùëñ and ùëó in the coordinate frame of view ùëñ

3. Recovering Camera Parameters
   Camera intrinsics can be recovered from the predicted point maps. 
   Assuming a pinhole camera model with square pixels and the principal point at the image center, 
   the focal length ùëì_ùëñ of camera ùëñ can be estimated by solving:
   ùëì^‚àó_ùëñ = arg min_(ùëì_ùëñ) ‚àë(ùëù=1 to ùëù=ùêªùëä)ùê∂^ùëù_(ùëñ,ùëñ)‚à•(ùë¢‚Ä≤_ùëù,ùë£‚Ä≤_ùëù)‚àíùëì_ùëñ((ùëã^ùëù_(ùëñ,ùëñ)[0], ùëã^ùëù_(ùëñ,ùëñ)[1] / ùëã^ùëù_(ùëñ,ùëñ)[2]))‚à•
   where (ùë¢‚Ä≤_ùëù,ùë£‚Ä≤_ùëù)=(ùë¢_ùëù‚àíùëä/2,ùë£_ùëù‚àíùêª/2)
   The relative camera pose between image pairs (ùêº_ùëñ,ùêº_ùëó) is estimated by comparing predictions for both directions. 
   Using point maps ùëã_(ùëñ,ùëñ) and ùëã_(ùëñ,ùëó), the relative pose ùëá_(ùëñ,ùëó)‚ààùëÜùê∏(3) and point map scale 
   ùúé_(ùëñ,ùëó) are estimated by:
   (ùëá_(ùëñ,ùëó),ùúé_(ùëñ,ùëó))^‚àó=arg min_(ùëá_(ùëñ,ùëó),ùúé_(ùëñ,ùëó)) ‚àë_ùëù ùê∂^ùëù_(ùëñ,ùëñ)ùê∂^ùëù_(ùëñ,ùëó)‚à•ùúé_(ùëñ,ùëó)ùëá_(ùëñ,ùëó)ùëã^ùëù_(ùëñ,ùëñ)‚àíùëã^ùëù_(ùëñ,ùëó)‚à•^2

4. Multi-view Point Map Alignment
   DUSt3R refines initial estimates by minimizing 3D-3D projection errors between a global point map 
   ùúí and transformed predictions:
   (ùúí,ùëá,ùúé)^‚àó=arg min_(ùúí,ùëá,ùúé) ‚àë_((ùëñ,ùëó)‚ààùê∏) ‚àë_(ùë£‚àà{ùëñ,ùëó}) ‚àë_(ùëù=1 to ùëù= ùêªùëä) ùê∂^ùëù_(ùë£,ùëñ)‚à•ùúí^ùëù_ùë£‚àíùúé_(ùëñ,ùëó)ùëá_(ùëñ,ùëó)ùëã^ùëù_(ùë£,ùëñ)‚à•
   The global point map ùúí^ùëù_ùë£ is derived via depth back-projection:
   ùúí^ùëù_ùë£=ùëá_ùë£ ùêæ^(-1)_ùë£ ùê∑_ùëù (ùë¢_ùëù,ùë£_ùëù,1)^‚ä§ = ùëá_ùë£ùê∑_ùëùùëì_ùë£(ùë¢‚Ä≤_ùëù,ùë£‚Ä≤_ùëù,1)^‚ä§
   Thus, the optimization problem becomes:
   (ùëá,ùúé,ùëì,ùê∑)^‚àó=arg min_(ùëá,ùúé,ùëì,ùê∑)‚àë_((ùëñ,ùëó)‚ààùê∏ ‚àë_(ùë£‚àà{ùëñ,ùëó})‚àë_(ùëù=1 to ùëù=ùêªùëä)ùê∂^ùëù_(ùë£,ùëñ)‚à•ùëá_ùë£ùê∑_ùëùùëì_ùë£(ùë¢‚Ä≤_ùëù,ùë£‚Ä≤_ùëù‚Ä≤,1)^‚ä§ ‚àí ùúé_(ùëñ,ùëó)ùëá_(ùëñ,ùëó)ùëã^ùëù_(ùë£,ùëñ)‚à•
   ùëá_(ùëñ,ùëó) and ùëá_ùëñ represent the same transformation but are separated for optimization flexibility.
   Optimization is done via gradient descent with a constraint ‚àè_(ùëñ,ùëó)ùúé_(ùëñ,ùëó)=1 to avoid degenerate 
   ùúé_(ùëñ,ùëó)=0

5. Method
   -a. Self-Calibration Pipeline
       The process begins by predicting point and confidence maps for all image pairs using pretrained DUSt3R. 
       When views overlap little, DUSt3R predictions may contain errors or outliers, 
       and the predicted confidence may not accurately reflect prediction quality. 
       Thus, relying directly on confidence for pseudo-label selection can hurt performance.

       However, each 3D point in a scene is jointly observed from many image pairs. 
       This allows using accurate predictions to refine erroneous ones. 
       The authors develop a robust multi-view alignment method to optimize point maps and calibrate 
       prediction confidence. 
       The refined maps and calibrated confidence scores are then used to generate pseudo-labels for images 
       {ùêº_ùëñ}^ùëÅ_ùëñ=1, which are in turn used to fine-tune DUSt3R via LoRA.

   -b. Robust Multi-view Alignment with Confidence Calibration
       The authors incorporate predicted confidence into global optimization by reparameterizing each confidence score 
       ùê∂^ùëù_(ùë£,ùëñ) as a learnable weight ùë§^ùëù_(ùë£,ùëñ). Although initial confidence may be inaccurate, 
       it still contains useful signal. 
       Therefore, they introduce a regularization term to encourage weights to remain close to the predicted
       confidence and avoid trivial solutions.

       This formulation aligns with the Geman-McClure robust M-estimator, which uses regularization terms to keep 
       weights near 1 in least-squares optimization. The revised objective becomes:
       (ùëá,ùúé,ùëì,ùê∑,ùëä)^‚àó=arg min_(ùëá,ùúé,ùëì,ùê∑,ùëä)‚àë_((ùëñ,ùëó)‚ààùê∏)‚àë_(ùë£‚àà{ùëñ,ùëó} ‚àë_(ùëù=1 to ùëù=ùêªùëä)ùë§^_(ùë£,ùëñ)‚à•ùëí^ùëù_(ùë£,ùëñ)‚à•+ùúá(np.root(ùë§^ùëù_(ùë£,ùëñ)) - np.root(ùê∂^ùëù_(ùë£,ùëñ)))^2
       where:
       ùëí^ùëù_(ùë£,ùëñ)=ùëá_ùë£ùê∑_ùëùùëì_ùë£(ùë¢‚Ä≤_ùëù,‚Ä≤ùë£_ùëù,1)^‚ä§‚àíùúé_(ùëñ,ùëó)ùëá_(ùëñ,ùëó)ùëã^ùëù_(ùë£,ùëñ)
       Instead of using gradient backpropagation to update weights, the authors derive a closed-form update rule 
       for faster recalibration:
       ùë§^ùëù_(ùë£,ùëñ)=ùê∂^ùëù_(ùë£,ùëñ) / (1+‚à•ùëí^ùëù_(ùë£,ùëñ)‚à•/ùúá)^2
       Predictions consistent across views retain confidence similar to their original estimates; 
       inconsistent ones are heavily down-weighted, improving alignment and pseudo-label accuracy.

   -c. Multi-view Pseudo-label Generation
       Pseudo-labels are created from the refined point maps and calibrated confidences. 
       To do this, global optimization results are transformed into local image pair coordinate frames. 
       Depth maps ùê∑_ùëù are back-projected into 3D, then transformed between views:
       ùëã^~_(ùëó,ùëñ)^ùëù=ùëá^(‚àó‚àí1)_ùëñ ùëá^‚àó_ùëó ùê∑^‚àó_ùëù ùëì^‚àó_ùëó(ùë¢‚Ä≤_ùëù,ùë£‚Ä≤_ùëù‚Ä≤,1)‚ä§, for¬†ùëù‚àà{ùëù‚à£ùë§^(‚àóùëù)_(ùëó,ùëñ)>ùë§_(cutoff)}
       A threshold of ùë§_(cutoff)=1.5 works well in most scenes. Dynamic points violating multi-view 
       consistency are automatically filtered, making LoRA3D robust to moving objects.

   -d. Fine-tuning with LoRA
       The pretrained DUSt3R is fine-tuned using LoRA on the pseudo-labeled data, 
       employing the same loss function as original training. 
       DUSt3R weights are frozen, while LoRA injects low-rank trainable matrices into Transformer layers‚Äîreducing 
       trainable parameters significantly.

       This approach improves memory/runtime efficiency and avoids catastrophic forgetting of pretraining. 
       Through extensive experiments, fine-tuning all attention weights with rank-16 LoRA achieves 
       the best trade-off between performance and efficiency‚Äîmatching full fine-tuning performance 
       while reducing parameter count by over 99%.

       With rank-16 LoRA, fine-tuning on 10 images (batch size 2) converges in under 3.5 minutes with 
       <20GB GPU memory, and each LoRA adapter is under 18MB.

