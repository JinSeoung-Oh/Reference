# Regularisation-based approaches
1. Soft Masking of Parameters: Soft-masking techniques mask and adjust the gradients of each parameter during the training process
   SPG (Soft-masking of Parameter-level Gradient flow)
   -1. Train the model on each task until convergence.
   -2. After training, calculate the “importance” of each parameter for the task
   -3. Soft-mask parameters based on their accumulated importance, making important parameters less likely to change during the learning of new tasks.
   
   Step 1. Training the First Task
   Step 2. Calculate Parameter Importance for the First Task
   step 3. Accumulating Importance Across Tasks
   step 4. Training Subsequent Tasks, combined loss and the soft-masking mechanism
   step 5. Soft-Masking Special Cases
           1. Feature Extractor: Gradients of parameters in the shared feature extractor are modified based on their specific accumulated importance
           2. Classification Head: For the classification head, gradients are modified based on the average importance of the feature extractor

#######################################################################################################################
## pseudocode(Calculate Parameter Importance for the First Task for step 2):
import torch

def compute_final_importance(model, loss_function, data_loader):
    # Get a single batch from the data loader
    inputs, labels = next(iter(data_loader)) 

    # Forward and backward pass to calculate the gradients for all parameters
    outputs = model(inputs)
    loss = loss_function(outputs, labels)
    loss.backward()
    
    importances = []

    # Calculate importance based on the gradients
    for param in model.parameters():
        if param.grad is not None:  # Gradients may be None for some unused parameters
            normalized_grad = (param.grad - torch.mean(param.grad)) / torch.std(param.grad)
            importance = torch.tanh(normalized_grad)
            importances.append(importance)

    return torch.stack(importances).mean(dim=0)
########################################################################################################################
## pseudocode(soft-masking mechanism)
import torch

accumulated_importance = # calculated at the end of each task

for epoch in range(num_epochs):
  for x, y in train_loader:
            
    # Forward Pass: Calculate the loss for the current task using the proper loss function
    logits = new_model(x)
    loss_current_task = nn.CrossEntropyLoss()(logits, y)
            
    # Forward Pass: Calculate the additional losses for previous tasks (CHI mechanism)
    loss_previous_tasks = 0
    for prev_task_id in range(task_id):
        logits_prev = old_model(x, prev_task_id)
        loss_previous_tasks += logits_prev.sum()
            
    # Combine the losses
    combined_loss = loss_current_task + loss_previous_tasks
            
    # Backward Pass
    optimizer.zero_grad()
    combined_loss.backward()
            
    # Update the accumulated importance
    for param, acc_imp in zip(model.parameters(), accumulated_importance):
        grad = param.grad
        acc_imp = torch.max(acc_imp, torch.abs(grad)) 

    # Soft-masking the gradients before taking an optimization step
    for param, imp in zip(model.parameters(), accumulated_importance):
        param.grad *= (1 - importance)
            
    optimizer.step()
######################################################################################################################

## Apply it in LLM
https://arxiv.org/pdf/2302.03241.pdf
Introduces a technique called DAS (Continual DA-pre-training of LMs with Soft-masking) for continual learning in the pre-training stage of a large language model
SPG used gradients to determine the importance of each parameter, and then applied the calculated 
importance value to mask the gradient adjustments of parameters during training

The importance is calculated for each “unit” in the network, where a unit could be a neuron or an attention head
Proxy loss function (“Proxy KL-divergence loss”):
  *Take a subset of the new domain we’re wanting to train on and feed it twice through the model to get two different representations. 
   These representations will differ a bit due to the existing dropout masks in the Transformer architecture.
  *Compute the KL-divergence between these two representations.
