from https://towardsdatascience.com/the-current-state-of-continual-learning-in-ai-af4a05c42f3c

## Continual learning is the ability to pause the model training process, save the model’s current state, and then later resume training on new data. 
   The model should be able to generalise well to new data, while still maintaining its ability to generalise to old data

## The 5 sub-categories of continual learning techniques
In, https://arxiv.org/pdf/2302.00487.pdf states training strategies for continual learning can be divided into 5 sub categories
1. Regularisation-based approach
   This approach adds constraints or penalties to the learning process during the training process.
2. Optimisation-based approach
   This technique focuses on modifying the optimisation algorithm.
3. Representation-based approach
   This aims to learn a shared feature representation across different tasks, helping the model generalise better to new but related tasks.
4. Replay-based approach
   This involves storing some data or learned features from previous tasks and replaying them during training on new tasks to maintain performance on earlier learned tasks. 
   In other words, mixing both the old and new datasets when training on new tasks.
5. Architecture-based approach
   In this approach, the network architecture is dynamically adjusted, often by growing or partitioning, delegating different parts of the network to different tasks


# Regularisation-based approaches
1. Soft Masking of Parameters: Soft-masking techniques mask and adjust the gradients of each parameter during the training process
   SPG (Soft-masking of Parameter-level Gradient flow)
   -1. Train the model on each task until convergence.
   -2. After training, calculate the “importance” of each parameter for the task
   -3. Soft-mask parameters based on their accumulated importance, making important parameters less likely to change during the learning of new tasks.
   
   Step 1. Training the First Task
   Step 2. Calculate Parameter Importance for the First Task
   step 3. Accumulating Importance Across Tasks
   step 4. Training Subsequent Tasks, combined loss and the soft-masking mechanism
   step 5. Soft-Masking Special Cases
           1. Feature Extractor: Gradients of parameters in the shared feature extractor are modified based on their specific accumulated importance
           2. Classification Head: For the classification head, gradients are modified based on the average importance of the feature extractor

#######################################################################################################################
## pseudocode(Calculate Parameter Importance for the First Task for step 2):
import torch

def compute_final_importance(model, loss_function, data_loader):
    # Get a single batch from the data loader
    inputs, labels = next(iter(data_loader)) 

    # Forward and backward pass to calculate the gradients for all parameters
    outputs = model(inputs)
    loss = loss_function(outputs, labels)
    loss.backward()
    
    importances = []

    # Calculate importance based on the gradients
    for param in model.parameters():
        if param.grad is not None:  # Gradients may be None for some unused parameters
            normalized_grad = (param.grad - torch.mean(param.grad)) / torch.std(param.grad)
            importance = torch.tanh(normalized_grad)
            importances.append(importance)

    return torch.stack(importances).mean(dim=0)
########################################################################################################################
## pseudocode(soft-masking mechanism)
import torch

accumulated_importance = # calculated at the end of each task

for epoch in range(num_epochs):
  for x, y in train_loader:
            
    # Forward Pass: Calculate the loss for the current task using the proper loss function
    logits = new_model(x)
    loss_current_task = nn.CrossEntropyLoss()(logits, y)
            
    # Forward Pass: Calculate the additional losses for previous tasks (CHI mechanism)
    loss_previous_tasks = 0
    for prev_task_id in range(task_id):
        logits_prev = old_model(x, prev_task_id)
        loss_previous_tasks += logits_prev.sum()
            
    # Combine the losses
    combined_loss = loss_current_task + loss_previous_tasks
            
    # Backward Pass
    optimizer.zero_grad()
    combined_loss.backward()
            
    # Update the accumulated importance
    for param, acc_imp in zip(model.parameters(), accumulated_importance):
        grad = param.grad
        acc_imp = torch.max(acc_imp, torch.abs(grad)) 

    # Soft-masking the gradients before taking an optimization step
    for param, imp in zip(model.parameters(), accumulated_importance):
        param.grad *= (1 - importance)
            
    optimizer.step()
######################################################################################################################
