LLM : Large, general-purpose ianguage models can be pre-trained and then fin-tuned for specific purposes
      Are trained to solve common language problems(text classification / QA / Document Sumarizaton, text generation)

prompt design : Prompts involve instructions and context passed to a language model to achieve a desired task

prompt engineering : prompt engineering is the practice of developing and optimizing prompts to efficiently use language models for a variety of applications

** 3 main kinds of LLM

1. Generic (or Raw) language Models : Theses predict the next word(technically token) based on the language in the training data

2. Instruction Tuned : Trained to predict a response to the instrunctions given in the input

3. Dialog Tuned : Trained to have a dialog by predicting the next response
                  - It is a special case of instrunction tuned where requests are typically framed as questions to a chat bot
                  - It is a further specialization of instruction tunning that is expected to be in the context of a longer back and forth conversation, and typically works better with natural question-like phrasings

## Chain of Thought Reasoning
Chain-of-thought is several attractive properties as an approach for facilitating reasoning in language models
1. First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.

2. Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a modelâ€™s computations that support an answer remains an open question).

3. Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.

4. Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting

## Arithmetic Reasoning
One class of tasks where language models typically struggle is arithmetic reasoning (i.e., solving math word problems).
Arithmetic Reasoning is the ability to understand and solve mathematical problems using basic arithmetic operations such as addition, subtraction, multiplication, and division. 
This skill involves the application of logic, critical thinking, and problem-solving techniques to perform calculations and analyze numerical relationships

## Commonsense Reasoning
It involves reasoning about physical and human interactions under the presumption of general background knowledge.

## PETMP
Parameter-Efficient Fine-Tuning (PEFT) enables you to fine-tune a small subset of parameters in a pretrained LLM. 
The main idea is that you freeze the parameters of a pre-trained LLM, add some new parameters, and fine-tune the new parameters on a new (small) training dataset. 
Typically, the new training data is specialized for the new task you want to fine-tune your LLM
