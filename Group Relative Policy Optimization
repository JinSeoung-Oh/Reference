### From https://medium.com/data-science-in-your-pocket/what-is-grpo-the-rl-algorithm-used-to-train-deepseek-12acc19798d3

1. Overview and Intuition
   Group Relative Policy Optimization (GRPO) is a reinforcement learning method designed to update an agentâ€™s policy in a stable and efficient way. 
   Instead of looking at actions one by one, GRPO takes a â€œmini-batchâ€ or group approach: it samples multiple actions from the current policy, 
   compares how they perform, and then adjusts the policy parameters to encourage actions that outperformed the group average 
   while discouraging worse actions.

   -a. Motivation:
       -1. Traditional policy gradient methods can have high variance if each action is considered in isolation.
       -2. GRPO groups actions and controls policy updates with a KL divergence constraint, ensuring stable incremental changes.
   -b. High-Level Steps:
       -1. Sample multiple actions from the policy for a given state.
       -2. Evaluate each actionâ€™s reward.
       -3. Compare each actionâ€™s performance to the group average (the advantage).
       -4. Adjust policy parameters to favor actions with higher advantage, but limit drastic shifts using a KL divergence penalty.

2. Illustrative Example
   Imagine a robot in a maze choosing among three paths (A, B, C). It tries each path several times 
   (e.g., path success rates: A = 66.7%, B = 33.3%, C = 100%). Based on these outcomes:

   -a. Compare: The robot sees path C yields the best success rate.
   -b. Small Adjustments: It increases the probability of choosing path C but doesnâ€™t completely drop paths A or B; it still explores them.
   -c. Batch/Group Updates: The robot accumulates performance data (e.g., over multiple trials) before updating, avoiding overreaction 
                            from a single data point.
   This approach is similar to â€œmini-batch gradientâ€ updates in supervised learning, reducing variance by pooling data from multiple action samples.

3. Mathematical Formulation
   3.1 Policy and Objective
       -a. Policy ğœ‹_ğœƒ(ğ‘âˆ£ğ‘ ): Given state ğ‘ , the policy outputs a distribution over actions ğ‘, controlled by parameters ğœƒ.
       -b. Goal: Maximize expected cumulative reward ğ½(ğœƒ). Typically, we define:
           ğ½(ğœƒ) = ğ¸_(ğœâˆ¼ğœ‹_ğœƒ)[âˆ‘_ğ‘¡ ğ‘Ÿ(ğ‘ _ğ‘¡,ğ‘_ğ‘¡)]
           where ğœ is a trajectory of states and actions.
   3.2 Group Sampling
       -a. Action Group {ğ‘_1, â€¦,ğ‘_ğ‘}: From a single state ğ‘ , sample ğ‘ actions from ğœ‹_ğœƒ
       -b. For each sampled action ğ‘_ğ‘–, we observe a reward ğ‘…(ğ‘_ğ‘–)
   3.3 Advantage Computation
       -a. Advantage ğ´(ğ‘_ğ‘–): Measures whether action ğ‘_ğ‘– performed better or worse than the group average.
       -b. In practice, advantage might be computed as:
           ğ´(ğ‘_ğ‘–) = ğ‘…(ğ‘_ğ‘–) âˆ’ ğ‘…_groupâ€¾, where ğ‘…_groupâ€¾ is some baseline such as the mean reward in the group.
   3.4 Policy Update
       -a. Update Rule: Increase probability of actions with positive advantage and decrease probability of those with negative advantage.
       -b. The KL Divergence constraint ensures that the new policy ğœ‹_(ğœƒ_new) does not drastically diverge from the old one ğœ‹_(ğœƒ_old). 
           This fosters stable learning.
   3.5 Overall GRPO Objective
       max_ğœƒ ğ¸[Advantage Ã— ProbabilityÂ Ratio],
       subject to a limit on KL divergence between old and new policies. The method thus balances improvement with caution against large leaps.

4. Why GRPO Works
   -a. Group Comparison: By evaluating actions within a group, the algorithm reduces the variance of policy gradient estimates,
                         making updates more reliable.
   -b. Controlled Policy Shifts: The KL divergence constraint ensures that updates arenâ€™t too large, preventing instability or 
                                 â€œcatastrophic forgetting.â€
   -c. Efficiency: GRPO avoids enumerating all actions; it samples a small set, which is computationally more tractable in environments 
                   with large action spaces.

5. Applying GRPO to Train Large Language Models (LLMs)
   Though often described in a context like robotics or games, GRPO can be used for fine-tuning LLMs via reinforcement learning:

   -a. Group Sampling: For a given prompt (state), the LLM samples multiple candidate responses (actions).
   -b. Reward Scoring: A reward model or human feedback is used to assign each response a numeric score, measuring â€œqualityâ€ or â€œalignment.â€
   -c. Advantage Calculation: Compare each responseâ€™s reward to the groupâ€™s average, identifying which responses are better.
   -d. Policy Update: Update the LLMâ€™s parameters (policy) to favor higher-reward responses. Use KL divergence constraints 
                      to avoid diverging too far from the original model distribution.
   -e. Iterative Training: Repeat this procedure over many prompts and sets of responses, continually improving the LLMâ€™s ability to produce helpful,
                           coherent answers.

   Result: An LLM that is incrementally refined to produce more aligned or high-quality outputs, while maintaining stability of style and knowledge.

6. Summary
   Group Relative Policy Optimization (GRPO) is a reinforcement learning technique that:

   -a. Collects a group of action samples from the current policy,
   -b. Evaluates each actionâ€™s reward,
   -c. Computes advantages by comparing actions within that group,
   -d. Updates the policyâ€™s parameters with small, stable steps enforced by a KL divergence constraint.

   In practical terms, it stabilizes policy gradient learning and reduces variance by focusing on relative performance within a group.
   When applied to Large Language Models, GRPO can fine-tune the modelâ€™s outputs (responses) in a stable way, 
   improving quality while limiting extreme, abrupt changes.

   By balancing exploration (still sampling less-favored actions occasionally) with exploitation (boosting probabilities of higher-reward actions),
   GRPO offers a robust path to gradually refine both robots in a maze and LLMs producing text, 
   all under the same reinforcement learning framework.

