From https://towardsdatascience.com/graph-based-prompting-and-reasoning-with-language-models-d6acbcd6b3d8

## Basic
1. Introduction to Prompting Techniques:
   Chain of thought (CoT) and tree of thought (ToT) prompting have improved the reasoning abilities of LLMs by enforcing a step-by-step response to problems.
   These techniques assume linear patterns in reasoning, while human thinking often involves non-linear leaps and connections between ideas.

2. Transformer Architecture:
   The transformer architecture, initially proposed for Seq2Seq tasks, has evolved for various use cases, including vision transformers, encoder-only transformers, and decoder-only transformers.
   The encoder-decoder transformer consists of bidirectional self-attention in the encoder and masked self-attention and cross-attention in the decoder.

3. Encoder-only and Decoder-only Variants:
   Encoder-only architectures (e.g., BERT) eliminate the decoder and are effective for discriminative language tasks.
   Decoder-only architectures are popular in generative LLMs like GPT variants, relying on masked self-attention and feed-forward transformations.

4. AI with Graph-Structured Data:
  Graph-structured data, like social networks or molecules, requires special model architectures such as graph convolutional networks (GCNs).
  GCNs apply feed-forward transformations to node embeddings and aggregate neighboring features to capture both node properties and graph structure.
  The Graph Attention Network (GAT) is an extension of GCN that uses attention mechanisms for weighted feature aggregation.

5. Multi-Modal CoT Reasoning:
   Muti-modal chain of thought prompting involves a two-stage approach for reasoning problems with both textual and visual inputs.
   It utilizes a T5 architecture and fine-tuning for solving tasks.

## Beyond Chain (or Tree) of Thought:
   Chain of thought (CoT) prompting has limitations, such as generating rationales in a left-to-right fashion, hindering recovery from early mistakes.
   Tree of thought (ToT) prompting allows backtracking but still models reasoning as a linear process.
   Recent advancements extend CoT and ToT prompting to graph-structured data, representing reasoning processes as graphs instead of chains or trees.

# Graph-of-thought reasoning (GOTR)
  Its two-stage reasoning process and the incorporation of textual, visual (optional), and thought graph inputs. 
  The framework employs separate encoders for each input modality and utilizes a unique architecture with multiple encoders and a decoder

  -1. Two-Stage Reasoning Framework:
      -Stage 1 (Rationale Generation): 
             The model takes the input text and generates a problem-solving rationale, similar to a chain of thought.
      -Stage 2 
            (Final Answer Generation): The generated rationale is concatenated with the input text, forming a longer input for the second stage. 
            The model then produces a final answer.
  -2. Input Modalities:
      -Text: Normal textual input for prompt-based reasoning tasks.
      -Image (Optional): An associated image related to the reasoning task, though the framework can function without it.
      -Thought Graph: A graph representing named entities and their relationships extracted from the input text using off-the-shelf tools (CoreNLP framework).

  -3. Thought Graph Generation:
      The thought graph is constructed based on the input text, representing named entities and their relationships.
      Subject-verb-object triplets are extracted from the text, and coreference resolution is performed to unify duplicate entities.

  -4. Encoding the Inputs:
      Separate encoders are used for each input modality: transformer encoder for text, vision transformer for images, and a GAT-based encoder for the thought graph.
      GOTR deviates from typical causal language model architectures by adopting a prefix-based language modeling approach with multiple encoder models.

  -5. GAT-Based Encoder for Thought Graph:
      The encoder for the thought graph is based on Graph Attention Network (GAT) architecture, utilizing attention mechanisms 
      for information aggregation between neighboring nodes.

  -6. Fusing Representations:
      Cross-attention mechanism is employed to fuse textual features with both image and thought graph features.
      Gated fusion layer is used to combine image, text, and graph features into a single representation for input to the decoder.

# The Graph of Thought (GoT) 
  The Graph of Thought (GoT) prompting framework described above introduces a novel approach to modeling the reasoning process of a language model. 
  It represents the thoughts generated by a language model as nodes in a graph, with vertices connecting these nodes to represent dependencies.
  The GoT framework is designed to handle cases where individuals try multiple chains of thought and combine insights from different chains, 
  which may not be easily captured by a tree-structured thought pattern.

  -1. Graph Representation of Reasoning Process:
      Each node in the graph corresponds to an individual thought generated by the language model.
      Edges in the graph represent relationships between thoughts, indicating that one thought was generated using another as input.
      The graph allows for the representation of complex networks of thoughts, capturing non-linear and recursive thinking patterns.

  -2. Thought Transformations:
      Thought transformations refer to modifications made to the graph, involving the addition of new vertices or edges.
      Three primary types of thought transformations are identified: Aggregation, Refinement, and Generation.
      These transformations enable the arbitrary modification and advancement of the language model's reasoning process.

  -3. Scoring and Ranking:
      Evaluator functions are used to assign scores to thoughts, and a ranking function selects the most relevant thoughts based on the entire graph.
      Both scoring and ranking take into consideration the entire graph, acknowledging that the quality of a thought might depend on other thoughts.

  -4. Implementation Modules:
      The GoT framework is implemented using several LLM-powered modules:
         1) Prompter: Prepares prompts for the LLM, encoding the graph structure.
         2) Parser: Extracts relevant information from LLM outputs, forming the state stored within each thought.
         3) Scorer: Verifies thought states, assigns scores based on correctness conditions.
         4) Controller: Coordinates the reasoning process, selects thought transformations, and decides the progress of the reasoning process.

  -5. Use Cases:
      GoT prompting is applied to various tasks, including sorting a list of digits, computing the intersection of sets, keyword counting, and document merging.
      The framework is particularly effective for tasks that can be broken into smaller, solvable sub-problems, especially those based on a merge-based approach.

  -6. Evaluation:
      Theoretical analysis of latency and volume properties shows that GoT prompting has less latency and greater volume compared to prior techniques.
      Empirical evaluations on sorting tasks demonstrate fewer errors compared to other techniques such as CoT and ToT prompting.
      However, GoT prompting may have a higher total cost of deriving a solution compared to more straightforward approaches.

  -7. When GoT Works Well:
      GoT works well for tasks that can be decomposed into smaller, solvable sub-problems and merged for a final solution (merge-based problems).
      The decision to use GoT depends on the problem's nature, the ease of decomposition, and the acceptable cost of the reasoning process.
