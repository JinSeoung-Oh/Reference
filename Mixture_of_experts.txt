## Mixture of experts, MoE or ME for short, is an ensemble learning technique that implements the idea of training experts on subtasks of a predictive modeling problem

see : https://github.com/lucidrains/mixture-of-experts/blob/master/mixture_of_experts/mixture_of_experts.py
https://machinelearningmastery.com/mixture-of-experts/

It involves decomposing predictive modeling tasks into sub-tasks, training an expert model on each, developing a gating model that learns which expert to trust based on the input to be predicted, and combines the predictions

Although the technique was initially described using neural network experts and gating models, 
it can be generalized to use models of any type. 
As such, it shows a strong similarity to stacked generalization and belongs to the class of ensemble learning methods referred to as meta-learning.

There are four step for this approach
1. Division of a task into subtasks.
2. Develop an expert for each subtask.
3. Use a gating model to decide which expert to use.
4. Pool predictions and gating model output to make a prediction.

# Subtasks
The first step is to divide the predictive modeling problem into subtasks. This often involves using domain knowledge
The division of the task into subtasks is not obvious, a simpler and more generic approach could be used

# Expert Models
An expert is designed for each subtask
The mixture of experts approach was initially developed and explored within the field of artificial neural networks, 
so traditionally, experts themselves are neural network models used to predict a numerical value in 
the case of regression or a class label in the case of classification
Each experts receive the same input pattern (row) and make a prediction.

# Gating Model
A model is used to interpret the predictions made by each expert and to aid in deciding which expert to trust for a given input
This is called the gating model, or the gating network, given that it is traditionally a neural network model
The gating network takes as input the input pattern that was provided to the expert models and outputs the contribution 
that each expert should have in making a prediction for the input.
The gating network and the experts are trained together such that the gating network learns when to trust each expert to make a prediction. 
This training procedure was traditionally implemented using expectation maximization (EM). 
The gating network might have a softmax output that gives a probability-like confidence score for each expert.

# Pooling Method
Finally, the mixture of expert models must make a prediction, and this is achieved using a pooling or aggregation mechanism. 
This might be as simple as selecting the expert with the largest output or confidence provided by the gating network
Alternatively, a weighted sum prediction could be made that explicitly combines the predictions made by each expert and the confidence estimated by the gating network.
