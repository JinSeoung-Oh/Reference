### From https://shuaiguo.medium.com/using-physics-informed-neural-networks-as-surrogate-models-from-promise-to-practicality-3ff13c1320fc

1.  PINN Architecture & Loss Formulation
    1.1 Network Setup
        -a. Inputs: (ğ‘¥,ğ‘¡,ğœƒ), where:
            -1. ğ‘¥: spatial coordinates (vector in ğ‘…^ğ‘‘)
            -2. ğ‘¡: time (scalar).
            -3. ğœƒâˆˆğ‘…^ğ‘: parametric inputs (e.g., Reynolds number, material constants).
       -b. Outputs: state variables ğ‘¢^(^)(ğ‘¥,ğ‘¡,ğœƒ)âˆˆğ‘…^ğ‘ (e.g., velocity, pressure, temperature fields).
       -c. Neural network: a multi-layer perceptron (MLP) with layers ğ¿â‰¥4, widths 64+ neurons, using smooth activations like Tanh or Softplus to aid in differentiation.
    1.2 Loss Components
        -a. Data Loss
            For ğ‘_ğ‘‘ data points {ğ‘¥^ğ‘–_ğ‘‘,ğ‘¡^ğ‘–_ğ‘‘,ğœƒ^ğ‘–_ğ‘‘,ğ‘¢^ğ‘–_ğ‘‘}:
            ğ¿_data=1/ğ‘_ğ‘‘ âˆ‘_(ğ‘–=0 to ğ‘–=ğ‘_ğ‘‘)âˆ¥ğ‘¢^(^)(ğ‘¥^ğ‘–_ğ‘‘,ğ‘¡^ğ‘–_ğ‘‘,ğœƒ^ğ‘–_ğ‘‘)âˆ’ğ‘¢^ğ‘–_ğ‘‘âˆ¥^2
        -b. Physics (PDE) Loss
            Consider governing PDEs:
            -1. Example: Heat equation
                âˆ‚ğ‘¢/âˆ‚ğ‘¡ âˆ’ ğœ…Î”_ğ‘¥ğ‘¢=0
            -2. Residual ğ‘Ÿ(ğ‘¥,ğ‘¡,ğœƒ)=âˆ‚_ğ‘¡ğ‘¢^ âˆ’ ğœ…Î”_ğ‘¥ğ‘¢^
            -3. Collocation loss over ğ‘_ğ‘ points (ğ‘¥^ğ‘—_ğ‘,ğ‘¡^ğ‘—_ğ‘,ğœƒ^ğ‘—_ğ‘):
                ğ¿_PDE=1/ğ‘_ğ‘ âˆ‘(ğ‘—=0 to ğ‘—=ğ‘_ğ‘)âˆ¥ğ‘Ÿ(ğ‘¥^ğ‘—_ğ‘,ğ‘¡^ğ‘—_ğ‘,ğœƒ^ğ‘—_ğ‘)âˆ¥^2
        -c. Boundary / Initial Condition Loss (if applicable)
            If boundary data is available, include:
            ğ¿_BC=1/ğ‘_ğ‘ âˆ‘(ğ‘˜=1 to ğ‘˜=ğ‘_ğ‘)âˆ¥ğ‘¢^ (ğ‘¥^ğ‘˜_ğ‘,ğ‘¡^ğ‘˜_ğ‘,ğœƒ^ğ‘˜_ğ‘)âˆ’ğ‘¢^ğ‘˜_ğ‘âˆ¥^2
        -d. Total Loss
            Balance all components with weights ğœ†:
            ğ¿=ğ¿_data + ğœ†_PDE ğ¿_PDE + ğœ†_BC ğ¿_BC
    1.3 Training Workflow
         -a. Sampling: Generate training (data), collocation, boundary datasets.
         -b. Model definition: Initialize weights, choose activation.
         -c. Loss computation: Use AD to compute âˆ‚_ğ‘¡, âˆ‡_ğ‘¥,  Î”_ğ‘¥
         -d. Optimization: Train with Adam or L-BFGS for thousands of epochs.
         -e. Validation: Evaluate on separate test sets to check physical accuracy and generalization.

2. Parametric PINNs & Parameter Embedding
   -a. Add ğœƒ as separate input channels.
   -b. Network input shape becomes (ğ‘‘+1+ğ‘)
   -c. Learned mapping:
       ğ‘¢^:(ğ‘¥,ğ‘¡,ğœƒ)â†¦stateÂ space
   -d. Allows continuous multi-query evaluation over parameter space without retraining.

3. Sample Industrial Applications â€” In-Depth Case Studies
   3.1 Parametric Study: Ghosh et al. 2024
       -a. Problem
           Predict velocity ğ‘¢(ğ‘¥,ğ‘¡) and pressure ğ‘(ğ‘¥,ğ‘¡) in turbulent flow across geometry parameter ğœƒ (e.g., nozzle size, Re number).
       -b. Key Components
           -1. Data: CFD simulations at ~50 (ğœƒ) samples, each with ~10k spatial points.
           -2. PINN Setup: Input (ğ‘¥,ğ‘¡,ğœƒ); Outputs (ğ‘¢,ğ‘)
           -3. Physics: Navierâ€“Stokes PDE:
                        âˆ‚ğ‘¢/âˆ‚ğ‘¡+ğ‘¢â‹…âˆ‡ğ‘¢+âˆ‡ğ‘âˆ’ğœˆÎ”ğ‘¢=0, âˆ‡â‹…ğ‘¢=0
           -4. Training: ~200k data + 300k collocation points + boundary/initial conditions.
           -5. Results: Surrogate reproduces fields with <1% relative error, and provides ~10kÃ— speed-up over CFD.
   3.2 Uncertainty Quantification: Panahi et al. 2024
       -a. Problem
           Track pollutant breakthrough curves with uncertain sorption parameters ğœƒ
       -b. Method
           -1. Stage-wise training: First trained on deterministic PDE; then added parameter randomness.
           -2. Monte Carlo: 10â¶ samples using PINN surrogate â†’ fast histogram estimation.
           -3. Outcome: Computed probability distribution of output at <1 minute (vs days via simulators).
   3.3 Heat Sink Optimization: Cai et al.
       -a. Objective
           Minimize peak GPU temperature ğ‘‡_max(ğœƒ) subject to pressure drop constraint 
           Î”ğ‘ƒ(ğœƒ)â‰¤threshold
       -b. PINN Structure
           -1. Inputs: fin dimensions ğœƒ, spatial and temporal coordinates.
           -2. Output: Temperature field ğ‘¢(ğ‘¥,ğ‘¡)
           -3. Used for:
               -1) Objective eval: direct forward pass gives ğ‘‡_max
â¡               -2) Gradient: AD yields âˆ‡ğœƒ ğ‘‡_max, enabling L-BFGS.
       -c. Results
           Converged ~50x faster than optimization via CFD loops.
   3.4 MPC in Soft Robotics: Habich et al. 2025
       -a. System
           Multi-joint soft arm with nonlinear PDE dynamics.
       -b. Controller
           MPC using surrogate for prediction over horizon ğ‘‡
       -c. PINN Use
           -1. Inputs: current state, control variables ğ‘¢(ğ‘¡+Î”ğ‘¡)
           -2. Outputs: next state prediction.
           -3. Training: Data + PDE constraint losses.
       -d. Achievements
           -1. â‰¤ 1.3Â° tracking error.
           -2. Real-time control (<50â€¯ms prediction latency).
   3.5 Anomaly Detection: Khakpoor et al. 2025
       -a. Problem
           Detect microgrid faults using physics model.
       -b. Implementation 
           -1. PINN trained on healthy data (voltages ğ‘¢).
           -2. Predicts expected voltage; compute residual:
               ğ‘Ÿ=ğ‘¢_sensorâˆ’ğ‘¢^
           -3. Classifier or threshold verifies anomaly.
       -c. Outcome
           Early detection, fault type classification w/ high accuracy and low false alarm rate.

4. Advanced Enhancements
   4.1 Bayesian PINNs
       Incorporate epistemic uncertainty in weights:
       max ğ‘ƒ(ğ‘Šâˆ£ğ‘‘ğ‘ğ‘¡ğ‘)âˆğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘âˆ£ğ‘Š)ğ‘ƒ(ğ‘Š)
       Output includes confidence intervals in predictions.
   4.2 Multifidelity PINNs
       -a. Low-fidelity simulations (coarse mesh) provide cheap data;
       -b. High-fidelity simulations (fine mesh) used selectively.
       -c. Combined using multifidelity loss, yielding faster convergence with better physical accuracy.
   4.3 Transfer Learning in PINNs
       Pre-train on related PDE setup, then fine-tune with fewer data. Helps with faster learning in new yet similar systems.
   4.4 Active Learning
       -a. Iteratively:
           -1. Evaluate PINN uncertainty (e.g., gradient norm of residual).
           -2. Run simulator at highest-uncertainty point.
           -3. Add to training dataset â†’ retrain.
               Feedback loop reduces dataset requirement.

5. When & How to Use Surrogate PINNs
   -a. When it Fits
       -1. Known physics (PDE/ODE).
       -2. Costly simulations.
       -3. Desire for continuous parameter-space evaluation.
       -4. Need for gradients (optimization/control).
       -5. Scarce data.
   -b. Quick Implementation Checklist
       -1. Survey PDE & boundary conditions.
       -2. Decide inputâ€“output modalities.
       -3. Select network architecture (depth, width, activation).
       -4. Generate data and collocation points.
       -5. Construct weighted loss with AD-driven derivatives.
       -6. Train with optimizer schedule (Adam, L-BFGS).
       -7. Validate on held-out conditions.
       -8. Deploy surrogate for specific use case (MC, optimization, MPC, monitoring).

6. Final Takeaways
   -a. Surrogate PINNs embed physics directly in data-driven models.
   -b. They offer speed, accuracy, and physical consistency.
   -c. Applicable to design, uncertainty, control, and monitoring.
   -d. Advanced techniques like Bayesian inference, multifidelity, and active learning push performance further.
   -e. Fully differentiable, enabling end-to-end integration into optimization/control systems.

