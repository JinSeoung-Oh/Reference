From https://chat.openai.com/c/55bbb19a-d00d-4d5a-9dcf-c8d28d6a2530

###################################################################################################################################################################
The key distinctions and reasons why Prompt-RAG is considered "vector embedding-free"

1. No Requirement for Vector Embeddings:
   Traditional RAG models rely on creating vector embeddings for input queries to perform semantic search and retrieval.
   In contrast, Prompt-RAG does not require this step. It directly accesses a Table of Contents (ToC) 
   and generates responses without the need for vector embeddings.

2. Simplified Retrieval Process:
   Traditional RAG involves the retrieval of information based on vector embeddings, which can be computationally intensive.
   Prompt-RAG simplifies the retrieval process by directly accessing a ToC and retrieving relevant sections of the document 
   without the intermediary step of creating vector embeddings.

3. Efficiency in Contextual Reference:
   Prompt-RAG uses a preprocessing step to create a ToC from domain-related documents.
   The ToC serves as a contextual reference for generating responses, and the use of large language models (LLMs) 
   helps in maintaining efficiency without relying on vector embeddings.

4. Reduced Complexity and Resource Requirements:
   By eliminating the need for vector embeddings, Prompt-RAG reduces the complexity of the model and the computational resources required for the retrieval process.
   This can lead to more efficient and faster response generation, particularly in scenarios where real-time or low-latency processing is essential.
#######################################################################################################################################################################

Prompt-RAG is introduced as a vector database/embeddings-free approach for optimizing Large Language Models
(LLMs) in domain-specific implementations, inspired by the Retrieval-Augmented Generation (RAG) model.

1. RAG Overview:
   RAG combines generative features with information retrieval.
   Overcomes limitations of generative models by integrating a large language model (LLM) with contextual information.
   Input queries are converted into vector embeddings to retrieve relevant data from a vectorized database.
   Generative component uses retrieved external data to generate contextually informed responses.

2. Prompt-RAG Overview:
   Does not require chunking or vector embeddings.
   Directly accesses a Table of Contents (ToC) for contextual reference and response generation.

3. Prompt-RAG Steps:
   -1. Preprocessing:
       Creates a ToC from domain-related documents.
       ToC can be manually created or generated by an LLM.
       LLM context-window size influences ToC size.
       Document formatting (removing headers, footers, etc.) reduces token size.
   -2. Heading Selection:
       User query and ToC are part of the prompt.
       LLM selects contextually relevant headings.
       Multiple headings can be narrowed down by summarizing the text.
   -3. Retrieval-Augmented Generation (RAG):
       Retrieves document portions under selected headings.
       Injects reference text into the prompt for in-context response generation.
       The injected text size must be smaller than the LLM's context window size.
       LLM can be used to summarize or truncate the injected text based on context window size and token usage.

# Traditional RAG disadvantages:
   1. Optimising document chunk size and managing overlaps can be a challenge.
   2. Updating chunks and embeddings as data changes to maintain relevance.
   3. Not optimised for minority language implementations
   4. Additional cost of running embeddings
   5. Cumbersome for smaller implementations
   6. Technically more demanding

# Traditional RAG advantages compared to Prompt-RAG:
   1. Scales well
   2. More Data Centric Approach
   3. Bulk Data Discovery and data development will remain important for enterprise implementations.
   4. Semantic clustering is an important aspect of data discovery in general, and a good first-step into implementing RAG.

# Prompt-RAG Advantages:
   1. Well suited for smaller, less technical implementations and minority languages.
   2. Ideal for a very niche requirement and implementation
   3. In the case of a chatbot, certain intents can be routed to a Prompt-RAG implementation
   4. Simplification
   5. Can serve as a first foray into full RAG implementations
   6. Non-gradient approach
   7. Inspectability and Observability
   8. A data discovery & data design tool aimed at optimising Prompt-RAG can add significant value.

#Prompt-RAG Disadvantages:
  1. Data design is still required.
  2. Context window size is an impediment.
  3. Token usage and cost will be higher; this needs to be compared to embedding model token cost.
  4. Scaling and introducing complexity will demand a technical framework.
  5. Dependant on LLM inference latency and token usage cost.
  6. Content Structure needs to be created. The study largely focusses on documents which already has a Table of Contents.

Example of PromptRAG
################################
Current context: {history}

Question: {question}

Table of Contents: {index}

Each heading (or line) in the table of contents above represents a fraction 
in a document. Select the five headings that help the best to find out the 
information for the question. List the headings in the order of importance 
and in the format of
'1. ---
 2. ---
 ---
 5. ---'.

Don't say anything other than the format.

If the question is about greetings or casual talks, just say 
'Disregard the reference.'.”
#############################
You are a chatbot based on a book called {Book Name}. 

Here is a record of previous conversations:

{history}

Reference: {context}

Question: {question}

Use the reference to answer the question.

The reference above is only fractions of '<>'.

Be informative, gentle, and formal.

If you can't answer the question with the reference, just say like 
'I couldn't find the right answer this time'.

Answer in {Language of Choice}:
##############################
You are a chatbot based on a book called {Book Name}. 

Here is a record of previous conversation for your smooth chats.:

{history}

Question: {question}

Answer the question.

Be informative, gentle, and formal. 
Answer in {Language of Choice}:”

