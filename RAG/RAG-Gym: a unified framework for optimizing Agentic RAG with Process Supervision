### From https://medium.com/@techsachin/rag-gym-a-unified-framework-for-optimizing-agentic-rag-with-process-supervision-75fa61cbe7f5
### From https://arxiv.org/abs/2502.13957
### From https://github.com/RAG-Gym/RAG-Gym

1. Introduction
   -a. Context and Motivation:
       Traditional RAG (Retrieval-Augmented Generation) systems rely on static retrieval mechanisms. 
       This limits their effectiveness when answering complex, knowledge-intensive questions that require iterative, 
       sequential information-seeking. 
       In contrast, agentic reasoning and search can adapt to the evolving needs of a query. 
       However, most existing methods depend heavily on prompt engineering without direct process-level supervision.
   -b. Main Contribution:
       The paper introduces RAG-Gym, a unified optimization framework that enhances agentic RAG systems through fine-grained
       process supervision at each search step. 
       By integrating process rewards into the decision-making loop, RAG-Gym aims to guide the information-seeking agent 
       more effectively.

2. Key Contributions
   -a. RAG-Gym Framework:
       -1. Unified Optimization: Presents a framework to optimize agentic RAG systems by supervising each step of 
                                 the search process.
       -2. ReSearch Agent: Introduces a novel agent architecture, ReSearch, which unifies answer reasoning and search query
                           generation. This synergy leads to state-of-the-art performance on knowledge-intensive tasks.
       -3. Process Reward Models: Demonstrates that using trained process reward models as verifiers significantly 
                                  improves the performance of search agents.
       -4. Comprehensive Analysis: Provides detailed analysis on process supervision sources, the transferability of reward
                                   models across different LLMs, and the scaling laws affecting agentic RAG performance.

3. RAG-Gym Framework: A Nested MDP Approach
   -a. Knowledge-Intensive QA as a Nested MDP:
       -1. State Space (S):
           -1) Each state ğ‘ _ğ‘¡ is defined as a tuple (ğ‘„,ğ»_ğ‘¡), where ğ‘„ is the original question and ğ»_ğ‘¡
               is the history of information-seeking (a sequence of query-document pairs).
           -2) The state space encompasses the entire question space, all possible queries, and the document collection.
       -2. Action Space (A):
           -1) Actions at time ğ‘¡ can be either a new search query (from ğ´_ğ‘) or a final predicted answer (from 
               ğ´_ğ‘).
           -2) The agentâ€™s policy ğœ‹ğ‘“(ğœƒ)(â‹…âˆ£ğ‘ _ğ‘¡) determines which action to take based on the current state.
       -3. IR Environment:
           -1) The environment is modeled as an information retrieval (IR) system that maps a query to a set of relevant
               documents.
           -2) The IR systemâ€™s configuration (e.g., the number of documents retrieved) influences the subsequent state
               transition.
       -4. MDP Workflow:
           -1) Starting with an initial state ğ‘ _1=(ğ‘„,âˆ…), the agent samples actions iteratively.
           -2) With each search query, the retrieved documents update the history, transitioning the state until an answer
               is produced, at which point the episode terminates.
       -5. Reward Structure:
           -1) The final reward is determined by the correctness of the final answer.
           -2) Immediate rewards are assigned at each step based on the quality of the action 
               (e.g., valid format, similarity of predicted patch to the oracle patch).
           -3) The overall objective is to maximize the expected cumulative reward over the episode.

4. Enhancing Search Agents with Process Supervision
   -a. Process Reward Data Collection:
       -1. Trajectory Sampling:
           -1) The language agent generates a sequence of actions based on its current policy.
           -2) At each step, multiple candidate actions are generated, and an external annotator (or ranking framework) 
               selects the best action.
       -2. Filtering:
           Only trajectories leading to a correct final answer are retained, ensuring that the process reward data is 
           high-quality.
   -b. Agent Tuning Methods:
       -1. Supervised Fine-Tuning (SFT):
           Uses the selected actions from the process reward data to minimize the negative log-likelihood of the chosen action
           given the state.
       -2. Direct Preference Optimization (DPO):
           Employs contrastive learning by forming preference pairs (preferred vs. less-preferred actions) and 
           optimizing to favor high-reward actions.
       -3. Process Reward Modeling (PRM):
           Trains a separate reward model ğ‘Ÿğœ™(ğ‘ _ğ‘¡,ğ‘_ğ‘¡) to predict process rewards using a contrastive loss, 
           ensuring that preferred actions are clearly distinguished from others.

5. The ReSearch Agent
   -a. Core Idea: 
       The ReSearch agent unifies reasoning and search in a single framework, leveraging process supervision to improve 
       both the quality of the final answer and the intermediate search steps.
   -b. Key Components:
       -1. History Knowledge Summarization:
           -1) The agent summarizes the retrieved documents from the information-seeking history ğ»_ğ‘¡ into a structured, 
               refined knowledge representation ğ»_ğ‘¡â€²
           -2) This step filters out irrelevant details and mitigates the challenge of handling long contexts.
   -c. Answer Reasoning:
       -1) Using the summarized knowledge, the agent performs structured reasoning to generate a candidate answer.
       -2) It verifies that all claims in its reasoning are supported by the evidence from ğ»ğ‘¡â€²
       -3) If unverified claims are found, these gaps trigger further search queries.
   -d. Search Query Generation:
       -1) The agent automatically generates new search queries to retrieve additional information to verify any unverified 
           claims.
       -2) This iterative process continues until the candidate answer is fully substantiated or the retrieval budget is 
           exhausted.

6. Experimental Results and Analysis
   -a. Process Supervision Impact:
       -1) Process supervision methods (SFT, DPO, PRM) consistently improved agent performance compared to a zero-shot baseline.
       -2) Among these, PRM (Process Reward Modeling) achieved the highest gains, outperforming zero-shot baselines by up
           to 25.6% in metrics like average F1 scores.
   -b. Performance of the ReSearch Agent:
       -1) ReSearch demonstrates strong zero-shot capabilities, indicating that aligning reasoning with query generation is 
           highly effective.
       -2) With process supervision, ReSearch achieved state-of-the-art performance with an average Exact Match (EM) score of
           54.31% and an average F1 score of 62.41% across various datasets.
   -c. Reward Model Transferability:
       -1) Experiments with GPT-4o-mini and Llama-3.1â€“8B based process reward models showed that reward models trained on 
           one LLM can effectively transfer to others, ensuring consistent action selection improvements across tasks.
   -d. Scaling Insights:
       -1) Training time scaling experiments indicate that increasing the number of training samples improves performance, 
           though gains converge after a certain point.
       -2) Inference time studies reveal that sampling more actions generally improves performance,
           highlighting the importance of balancing exploration with latency constraints.

7. Conclusion
   The RAG-Gym framework and its ReSearch agent demonstrate that:
   -a. Process Supervision: Fine-grained supervision at each step of the search process significantly enhances 
                            the performance of information-seeking agents.
   -b. Unified Reasoning and Search: Integrating answer reasoning with search query generation into a unified agent
                                     (ReSearch) leads to state-of-the-art performance on knowledge-intensive QA tasks.
   -c. Scalability and Transferability: Process reward models are transferable across different LLM architectures, 
                                        and the benefits scale with more training data and sampled actions.
   In summary, RAG-Gym provides a unified, optimized framework for agentic RAG systems that addresses the limitations
   of static retrieval by incorporating process-level supervision. 
   This approach not only improves intermediate reasoning and query generation but also sets new performance benchmarks
   on challenging tasks, paving the way for more adaptive and robust information-seeking AI agents.


