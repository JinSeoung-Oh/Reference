### From https://medium.com/@techsachin/meta-agent-search-automated-system-design-creation-for-llm-based-agentic-systems-a5e2de9f3f46
### Paper: https://arxiv.org/abs/2408.08435
### Code: https://github.com/ShengranHu/ADAS

1. Overview of Automated Design of Agentic Systems (ADAS)
   -a. ADAS is an approach that uses a search algorithm to automatically discover and design agentic systems. 
       These systems are generated by optimizing an evaluation function over a defined search space. ADAS is structured around three primary components:

       i) Search Space:
          This defines the set of agentic systems that can be represented. Only agents whose components—especially their control flow—are representable in the chosen 
          formulation can be discovered. For example, approaches like PromptBreeder modify only text prompts while keeping the control flow constant.

       ii) Search Algorithm:
           This component determines how the ADAS explores the search space. Given that the space is often very large or unbounded, 
           it must effectively balance the exploration-exploitation tradeoff.

       iii) Evaluation Function:
            This defines the criteria for assessing candidate agents based on ADAS-specific objectives such as performance, cost, latency, or safety. 
            A common method is to calculate the agent’s accuracy on validation data to predict its performance on unseen data.

2. Meta Agent Search
   The Meta Agent Search algorithm is a key example of an ADAS approach. In this method:

   -a. A “meta” agent iteratively programs new agents.
   -b. Each generated agent is tested on task performance and its results (e.g., success rate or F1 score with a 95% bootstrap confidence interval) are recorded.
   -c. The new agent is then added to an archive of discovered agents.
   -d. The meta agent uses this archive to inform and refine subsequent iterations.
   -e. The process incorporates self-reflection iterations where the meta agent refines its proposals for novelty and correctness and further refines proposals
       when code execution errors occur.
   -f. Throughout the iterations, agents are evaluated on validation data and the process continues until a maximum iteration count is reached.

   A figure (not shown here) in the text illustrates example agents discovered during different runs and highlights the prompt used by the meta agent
   (with highlighted variables) to generate new agents.

3. Experiments
   The text details several experiments demonstrating the effectiveness of Meta Agent Search:

   i) Case Study: ARC Challenge
      -a. Objective:
          Evaluate how Meta Agent Search discovers novel agentic systems that can outperform state-of-the-art hand-designed agents on the Abstraction and Reasoning 
          Corpus (ARC) challenge. The ARC challenge tests general intelligence by requiring the agent to:
          -1. Learn from multiple examples of visual input-output grid patterns.
          -2. Infer the transformation rules behind these patterns.
          -3. Predict the output grid given a test input.

      -b. Setup:
          -1. Agents are required to write code for the transformation rule rather than providing direct answers.
          -2. A framework provides tool functions to evaluate the generated transformation code.
          -3. Data is sampled from “Public Training Set (Easy)” with grid dimensions ≤ 5×5, and separate validation and test sets are created.
          -4. Each agent’s validation and test accuracy is computed over multiple runs (to reduce stochastic variance).
          -5. Meta Agent Search runs for 25 iterations, with the meta agent using GPT-4 while discovered agents and baselines are evaluated using GPT-3.5 for cost efficiency.

      -c. Baselines:
          The discovered agents are compared against five state-of-the-art hand-designed agents:
          -1. Chain-of-Thought (COT): Instructs the agent to output intermediate reasoning.
          -2. Self-Consistency with COT (COT-SC): Ensembles multiple answers for a more accurate final answer.
          -3. Self-Refine: Uses iterative self-reflection to correct previous mistakes.
          -4. LLM-Debate: Leverages debates between different LLMs to arrive at better answers.
          -5. Quality-Diversity: A simplified version of Intelligent Go-Explore that produces and ensembles diverse solutions. 
              These baselines also serve as initial seeds for the Meta Agent Search archive.

      -d. Results and Analysis:
          Meta Agent Search progressively discovers agents that outperform all hand-designed baselines. 
          Evaluation results (median accuracy and 95% bootstrap confidence intervals) on a held-out test set show superior performance, 
          with the best-discovered agent adopting a complex feedback mechanism to refine answers effectively.

   ii) Reasoning and Problem-Solving Domains
       -a. Setup:
           Meta Agent Search is also evaluated on four benchmarks:
           -1. DROP: For Reading Comprehension.
           -2. MGSM: For Math capability in a multilingual setting.
           -3. MMLU: For Multi-task Problem Solving.
           -4. GPQA: For solving hard, graduate-level science questions.

       -b. Baselines:
           The same baselines from the ARC challenge are used, along with two additional baselines that focus on enhancing reasoning:
           -1. Step-back Abstraction: Instructs agents to first consider underlying principles.
           -2. Role Assignment: Assigns different roles to foundational models (FMs) for improved answers.

       -c.Results and Analysis:
          Experiments demonstrate that Meta Agent Search outperforms state-of-the-art hand-designed agents across these domains. 
          Notably, there are substantial improvements in F1 scores for Reading Comprehension and accuracy rates in Math, with smaller gaps in multi-task and science domains.

   iii) Generalization and Transferability
        -a. Transferability Across Foundation Models:
            -1. The top three agents (as evaluated using GPT-3.5 on ARC) are transferred to other models such as Claude-Haiku, GPT-4, and Claude-Sonnet.
            -2. Results show that the agents discovered via Meta Agent Search consistently outperform the hand-designed agents, 
                with the best agent reaching nearly 50% accuracy on ARC when using Claude-Sonnet.
        -b. Transferability Across Domains:
            -1. Top agents from the MGSM domain are transferred to four popular math domains (GSM8K, GSM-Hard, SVAMP, ASDiv) and also to three non-math domains.
            -2. Significant performance improvements are observed; for instance, the best agents improve accuracy by 25.9% and 13.2% on GSM8K and 
                GSM-Hard respectively compared to the baselines.
            -3. Additionally, agents discovered in the math domain show effective transferability to non-math domains.

4. Conclusion
   The work demonstrates that with current API access to powerful foundational models, it is feasible to program powerful ADAS algorithms without the need 
   for expensive hardware such as GPUs. 
   The study motivates further research into "safe-ADAS," which would ensure that the search process does not execute harmful code and avoids creating dishonest 
   or harmful agents.

   The key takeaway is that by defining agents in code and using a “meta” agent to program new agents iteratively, Meta Agent Search can automatically discover 
   high-performing agentic systems. Experiments show that these discovered agents not only outperform state-of-the-art hand-designed agents across various domains 
   but also transfer well across different foundation models and task domains.

