### From https://medium.com/@florian_algo/ai-innovations-and-insights-24-rstar-simrag-and-mr2ag-1018d109b4fb

1. rStar: Two Minds, One Solution
   -a. Vivid Description:
       -1. rStar is likened to two students collaborating on a math problem.
           -1) The Generator: Explores multiple solution approaches by mapping out reasoning paths using Monte Carlo Tree Search (MCTS).
           -2) The Discriminator: Reviews each step, verifying correctness much like a peer proofreading a draft.
       -2. Together, they use mutual verification to narrow down the most likely correct solution.
   -b. Overview of the Process:
       -1. Self-Generator: Enhances a target Small Language Model (SLM) to generate potential reasoning paths via MCTS.
       -2. Discriminator: Uses another SLM to provide unsupervised feedback on each candidate path, guided by partial hints.
       -3. Final Selection: Based on this feedback, the target SLM chooses the final reasoning path as the solution.
   -c. MCTS Details:
       -1. Search Tree Construction:
           -1) The root node represents the original question (x).
           -2) Edges correspond to actions (a), and child nodes are intermediate steps (s) generated by the SLM.
           -3) A complete candidate solution trajectory (t) is formed by concatenating these steps from the root to a terminal (leaf) node.
       -2. Reward Function:
           -1) Inspired by AlphaGo, the reward of a terminal node (and its preceding nodes via backpropagation) is determined by whether the final answer is correct.
           -2) Initially, all unexplored nodes have a score of zero. As terminal nodes produce correct answers, their rewards are backpropagated, 
               with a final score based on self‑consistency (majority voting confidence).
   -d. Selection Process:
       -1. Starting from the root, the MCTS proceeds through phases of selection, expansion, simulation, and backpropagation.
       -2. A default rollout policy (with multiple rollouts) is used to improve reward estimation, while Upper Confidence Bounds applied to Trees (UCT) 
           balance exploration and exploitation during node selection.
   -e. Commentary:
       -1. rStar is notable for its innovative, cost‑effective use of a small language model without the need for fine‑tuning.
       -2. Its approach mirrors AlphaGo Zero’s self‑play mechanism, and its open‑source code is comprehensive and well‑documented.
       -3. Concerns:
           -1) The manually designed, fixed action space may lack dynamic adjustment for varying tasks, possibly limiting generality and adaptability.
           -2) The termination conditions for the reasoning process are not clearly defined—they rely on external settings (such as simulation count or time limits), 
               which might restrict efficiency when the reasoning space is large or computational resources are constrained.

2. SimRAG: A Self‑Improving Student That Learns by Generating and Answering Its Own Questions
   -a. Vivid Description:
       -1. SimRAG is compared to a student who, instead of just reading, actively creates questions to test and deepen their understanding.
       -2. This “self‑questioning” approach enables the model to generate its own training examples (question‑answer pairs) that help improve its performance.
   -b. Overview:
       -1. SimRAG is designed to address challenges faced by traditional RAG (Retrieval‑Augmented Generation) systems when adapting to specialized fields 
           such as science or medicine—domains often characterized by data scarcity and distribution shifts.
       -2. It is a self‑improving method that leverages unlabeled data to generate high‑quality question‑answer pairs, thereby enhancing model performance.
   -c. Two‑Stage Fine‑Tuning Framework (Referenced in Figure 3):
       -1. Stage One:
           -1) The model is fine‑tuned on retrieval‑related tasks, including instruction‑following, question‑answering, and search‑related tasks.
       -2. Stage Two:
           -1) The model generates pseudo‑labeled tuples by first extracting candidate answers from a corpus and then generating candidate questions
               conditioned on both the document and the answer.
           -2) These pseudo‑labeled examples are filtered using round‑trip consistency before being used to further fine‑tune the model.
   -d. Commentary:
       -1. SimRAG’s key innovation is that its second‑stage fine‑tuning data is obtained through a self‑supervised process.
       -2. However, the quality of the generated pseudo‑labels can be inconsistent or inaccurate, meaning that additional quality control mechanisms 
           are necessary to ensure reliable performance.

3. mR2AG: Intelligent Navigation System for Knowledge‑Based VQA
   -a. Vivid Description:
       -1. mR2AG is compared to an intelligent navigation system that helps a user reach a destination efficiently by planning the best route and 
           cutting out unnecessary details.
       -2. It incorporates two core innovations:
           -1) Retrieval‑Reflection: Decides whether external knowledge retrieval is necessary for a given query (akin to choosing whether to drive or take a walk).
           -2) Relevance‑Reflection: Selects the most relevant evidence from retrieved passages, similar to highlighting key landmarks along a chosen route.
   -b. Overview:
       -1. Current methods for Visual‑dependent and Knowledge‑based VQA (Visual Question Answering) tasks include:
           -1) Multimodal LLMs: Use both image and text inputs but often struggle with up‑to‑date knowledge.
           -2) Multimodal RAG: Rely on retrieval, but they can suffer from unnecessary retrieval, poor evidence identification, and complex filtering.
       -2. mR2AG aims to adaptively determine when retrieval is needed and to locate the most useful context for generating accurate answers.
       -3. The framework is divided into three stages:
           -1) Retrieval‑Reflection: Determines if external knowledge retrieval is necessary based on the query.
           -2) Relevance‑Reflection: Filters and identifies the most relevant evidence passages.
           -3) Answer Post‑Processing: Ranks and post‑processes answers from multiple passages by combining scores based on retrieval, relevance, and confidence.
       -4. Additionally, mR2AG is compared against a naive baseline (mRAG) that does not incorporate any reflection, 
           illustrating the benefits of its innovative reflective strategies.


