## From https://medium.com/@techsachin/rstar-self-play-reasoning-approach-to-improve-reasoning-capabilities-of-smaller-language-models-038636e24ec7
## https://arxiv.org/abs/2408.06195
## https://github.com/zhentingqi/rStar

1. Introduction
   rStar is a novel approach introduced to enhance the reasoning capabilities of Smaller Language Models (SLMs) during inference. 
   The key motivation is that SLMs struggle with self-exploration in reasoning tasks,
   where they often get trapped in low-quality reasoning steps or have difficulty determining which reasoning paths lead to correct answers.

2. High-Level Overview
   -1. Target SLM: Uses Monte Carlo Tree Search (MCTS) with human-like reasoning actions to create higher-quality reasoning trajectories.
   -2. Discriminator SLM: Another SLM with similar capabilities acts as a discriminator to verify the correctness of the reasoning paths generated by the target SLM.
   -3. Mutual Agreement: When both models agree on a reasoning trajectory, it is considered mutually consistent and thus more likely to be correct.

3. Methodology
   -1. Problem Formulation
       The task is broken down into a multi-step reasoning generation, where a target SLM incrementally builds a search tree (T) using MCTS.
       The root node represents the question, edges represent actions, and child nodes represent intermediate steps generated by the model. 
       Each path from the root to a terminal node forms a potential solution trajectory.
   -2. Challenges for SLMs in Reasoning
       - Large solution space: Existing MCTS-based methods struggle with generalization due to limited diversity.
       - Difficult Reward Mechanism: Verifying correctness at each intermediate step is challenging, particularly because of the lack of ground truth. 
                                     Self-consistency methods like majority voting often lead to poor results for SLMs

4. MCTS Rollout: Self-Generating Reasoning Trajectories
   Rich Set of Human-Like Reasoning Actions
   The paper introduces a set of five actions aimed at enabling the SLM to solve complex reasoning problems:

   - A1: Propose a one-step thought: Generates the next reasoning step based on the current problem context.
   - A2: Propose remaining thought steps: Prompts the model to generate all remaining steps in a chain-of-thought (CoT) style.
   - A3: Propose next sub-question with answer: Breaks complex questions into smaller sub-questions, solving each sequentially.
   - A4: Answer the sub-question again: Re-answers the sub-question to improve accuracy using few-shot chain-of-thought.
   - A5: Rephrase the question/sub-question: Rephrases the question to clarify any misunderstood conditions.

5. Reward Function
   Self-rewarding is excluded to avoid randomness in scoring intermediate steps.
   Inspired by AlphaGo, each intermediate node is scored based on its contribution to the final correct answer, 
   with higher rewards given to steps that often lead to the right solution. This makes these actions more likely to be selected in future MCTS expansions.

6. Solution Generation with MCTS Rollout
   Starting from the root node, multiple searches are conducted, each involving node selection, expansion, simulation, and back-propagation.
   Upper Confidence Bounds applied to Trees (UCT) is used for node selection to balance exploration and exploitation.
   The search continues until reaching a terminal node or maximum depth. All resulting trajectories from the rollouts are considered candidate solutions.

7. Reasoning Trajectory Selection with Mutual Consistency
   -1. Mutual Reasoning Consistency
       A second SLM acts as a discriminator, checking the validity of reasoning trajectories generated by the target SLM.
       The discriminator is prompted with the earlier reasoning steps from the trajectory and tasked with completing the remaining steps.
       If the discriminator’s answer matches the original trajectory’s final solution, the trajectory is considered validated.
   -2. Final Trajectory Selection
       The target SLM selects the final trajectory from the validated ones by calculating the score of each trajectory,
       which is the product of its reward and the confidence score of the terminal node.
       The trajectory with the highest final score is selected as the solution.

8. Evaluation Results
   -1. Baselines
       - Single-round CoT prompting: Includes zero-shot and few-shot CoT.
       - Multi-round CoT prompting: Using the self-consistency (SC) method with majority voting over 8, 64, or 128 sampled answers.
       - Self-improving approaches: Like Tree-of-Thought (ToT) and RAP.
  - 2. Results
       - SLMs with rStar showed remarkable improvement in reasoning capabilities.
       - For example, LLaMA2–7B initially had a 12.51% accuracy on GSM8K (few-shot CoT), but with rStar, it improved to 63.91%, 
          approaching the performance of fine-tuned models.

rStar consistently outperformed other methods across diverse tasks, including mathematical and logical reasoning.
In particular, rStar achieved up to 38.37% higher accuracy than ToT and 16.39% higher than RAP on the GSM8K dataset.
On the challenging MATH-500 dataset, rStar demonstrated a +9.14% improvement compared to state-of-the-art baselines.

9. Conclusion
The rStar method presents a generator-discriminator self-play approach that significantly enhances reasoning capabilities for SLMs without requiring fine-tuning. 
By integrating MCTS and human-like reasoning actions, the model effectively solves complex reasoning tasks while maintaining accuracy. 
The mutual reasoning consistency mechanism ensures higher quality solutions by cross-verifying reasoning trajectories, 
leading to state-of-the-art performance across various tasks and models, including models like LLaMA2–7B and Phi3-Mini-4k-Instruct.

This method opens new doors for improving reasoning in smaller language models, which often struggle with exploring solution spaces effectively. rStar shows that SLMs have the potential for strong reasoning capabilities, even without domain-specific fine-tuning.







