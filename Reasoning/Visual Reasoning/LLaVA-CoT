### From https://medium.com/@techsachin/llava-cot-first-vision-language-model-with-step-by-step-reasoning-capabilities-similar-to-gpt-o1-52a64c9a8ffc
### https://arxiv.org/abs/2411.10440
### https://github.com/PKU-YuanGroup/LLaVA-CoT

1. Overview and Motivation
   Existing Vision-Language Models (VLMs) often struggle with complex visual question-answering (VQA) tasks that demand systematic, multi-step logical reasoning.
   Although chain-of-thought prompting has improved reasoning in text-only large language models, applying similar techniques directly to VLMs has proven challenging. 
   To overcome this, the authors introduce LLaVA-CoT, a new VLM that explicitly structures its reasoning process into multiple autonomous stages, 
   enabling more organized and effective reasoning than standard VLMs.

2. Key Ideas:
   -a. Structured Reasoning Stages
       LLaVA-CoT breaks down the reasoning into distinct internal phases (summary, caption, reasoning, conclusion) rather than producing a single, 
       monolithic chain-of-thought.
   -b. Stage-Level Beam Search
       By treating each reasoning stage as a discrete unit, LLaVA-CoT can leverage a specialized inference strategy called stage-level beam search 
       for effective inference-time scaling.
   -c. Performance and Scalability
       LLaVA-CoT demonstrates superior performance on VQA benchmarks and can be improved further at inference time by allocating more computational resources,
       outperforming even some closed-source and larger models.

3. Core Contributions
   -a. LLaVA-CoT Framework:
       Introduces a VLM that adopts autonomous, multi-stage reasoning. Instead of simply generating one continuous chain-of-thought, 
       LLaVA-CoT separates its internal reasoning into four stages:

       -1. Summary Stage: Provides a high-level summary or interpretation of the question.
       -2. Caption Stage: For image-based queries, it offers a concise description (caption) of the relevant visual content.
       -3. Reasoning Stage: Executes structured logical reasoning steps based on the summarized problem and image caption.
       -4. Conclusion Stage: Synthesizes a final answer for the user. Only this stage’s output is visible to the user,
                             while previous stages remain internal to the model.

4. Inference-Time Scalability via Stage-Level Beam Search:
   The model’s staged structure allows a new inference-time search method. Instead of generating multiple full answers and picking the best (Best-of-N search) 
   or operating at a very fine granularity (sentence-level beam search), LLaVA-CoT performs a middle-ground approach:

   -a. At each reasoning stage, it samples multiple candidate responses.
   -b. It uses the model itself to compare and select the best candidate before proceeding to the next stage.
   -c. This selective approach at each stage improves final accuracy and can be enhanced by increasing the number of candidates (beam size) 
       given sufficient computational resources.

5. High-Quality Training Data (LLaVA-CoT-100k):
   The authors compile a new 100k-sample dataset from various VQA sources including general VQA tasks, science-oriented VQA, 
   and mathematical reasoning datasets like CLEVR. They prompt a powerful model (e.g., GPT-4V) 
   to produce multi-stage responses and then filter the results to ensure high-quality reasoning annotations.
   This curated data can be used to fine-tune existing models, providing them with explicit stage-based reasoning patterns.

6. Detailed Approach
   -a. Model and Data:
       Start with a base VLM (e.g., Llama-3.2–11B-Vision-Instruct). 
       Perform supervised fine-tuning (SFT) on the LLaVA-CoT-100k dataset, which includes structured tags to guide the model’s reasoning into distinct stages.
       By training on these systematic, structured responses, the model learns to internalize the multi-stage reasoning pattern.

  -b. Inference Steps:
      - Stage-by-Stage Reasoning:
        For a given query and image, LLaVA-CoT:

        -1. Produces a summary (summary stage).
            If needed, provides a caption describing the relevant image features (caption stage).
        -2. Carries out internal logical reasoning steps (reasoning stage).
        -3. Finally, delivers a user-facing answer (conclusion stage).

7. Stage-Level Beam Search:
   -a. At each stage:
       Sample N candidate outputs.
       Randomly pick 2 and let the model choose which is better.
       Retain the best candidate and repeat until only one candidate remains for that stage.
       Proceed to the next stage.

   This process ensures that at each step the model can refine its reasoning, improving the quality of the final answer.

8. Experimental Results
   -a. Performance on Benchmarks: LLaVA-CoT is tested on a range of VQA tasks, including:
       General VQA datasets (e.g., ChartQA, A-OKVQA, DocVQA, PISC, CLEVR).
       Science-specific VQA tasks (GeoQA+, AI2D, ScienceQA).
       Mathematical reasoning tasks (CLEVR-Math).

   -b. The results show:
       Significant improvements (average score increase of 6.9%) on various benchmarks.
       Enhanced reasoning abilities in both general and specialized domains.
       The structured approach reduces hallucinations and leads to more coherent answers.

9. Inference Time Scaling: Applying stage-level beam search at inference:
   Increases performance further as the beam size (N) grows.
   Achieves more consistent improvements than best-of-N or sentence-level beam searches, with comparable computational budgets.
   The staged reasoning structure is what makes it easy to apply such selective refinement at each phase.

10. Comparison with Other Models:
    LLaVA-CoT outperforms many open-source VLMs of similar or larger parameter sizes.
    It even surpasses some closed-source models like GPT-4o-mini and Gemini-1.5-pro on tasks requiring advanced reasoning.
    Demonstrates that systematic, structured reasoning is more effective than just increasing model size or relying on black-box proprietary systems.

11. Conclusion and Future Directions
    -a. Key Takeaways:
        - Multi-Stage Reasoning:
          By explicitly structuring the reasoning process into separable stages, LLaVA-CoT improves the clarity and reliability of VQA outputs.

        - Inference-Time Scalability:
          Stage-level beam search offers a practical way to trade additional compute for better answers, providing a scalable and flexible approach to inference.

    -b. Better Data, Better Reasoning: The LLaVA-CoT-100k dataset’s carefully annotated reasoning steps help models internalize systematic thinking patterns, 
        paving the way for more robust reasoning skills in multimodal settings.

    -c. Implications:
        LLaVA-CoT sets a new standard for systematic reasoning in VLMs.
        Its success encourages future research into more granular and interpretable reasoning pipelines, as well as further improvements in inference strategies.

In summary, LLaVA-CoT is a novel VLM that leverages structured, autonomous reasoning across multiple hidden stages to achieve state-of-the-art performance on
complex VQA tasks. Its stage-level beam search mechanism provides a scalable inference strategy, establishing a new benchmark in the multimodal reasoning arena.

