### From https://arxiv.org/pdf/2503.15558

1. Introduction to Cosmos-Reason1
   Cosmos-Reason1 is a family of multimodal large language models (MLLMs) specialized for Physical AI reasoning.
   -a. Two models: Cosmos-Reason1-7B and Cosmos-Reason1-56B.
   -b. Designed to integrate vision encoders with LLM backbones, optimized for tasks requiring physical common sense and embodied reasoning.

2. Multimodal Architecture
   2.1 General Design
       -a. Multimodal LLMs typically adopt either:
           -1. Decoder-only architectures (e.g., LLaVA, NVLM-D) – simple and unified across modalities.
           -2. Cross-attention-based architectures (e.g., Flamingo, LLaMA 3-V).
       -b. Cosmos-Reason1 uses the decoder-only architecture, aligning visual tokens (images/videos) directly into the text embedding space.
   2.2 Cosmos-Reason1-7B
       -a. Backbone: Qwen2.5-VL.
       -b. Standard image/video preprocessing.
       -c. Trained as a dense model with Tensor Parallelism = 4 (TP=4).
   2.3 Cosmos-Reason1-56B
       -a. Vision encoder: InternViT-300M-V2.5.
       -b. LLM backbone: Nemotron-H.
       -c. Input processing:
           -1. Images dynamically resized and split into 1–12 tiles (448×448 each).
           -2. A thumbnail tile preserves global context.
           -3. Videos: up to 32 frames sampled at ≤2 fps, resized to 448×448.
       -d. Tokenization:
           -1. Each frame → 1,024 tokens (14×14 patch embedding).
           -2. Downsampled to 256 tokens using PixelShuffle.
           -3. Image tokens concatenated with tile IDs, video tokens concatenated directly.
       -e. Training setup: Tensor Parallelism = 8, Pipeline Parallelism = 2 (TP=8, PP=2).

3. Hybrid Mamba-MLP-Transformer Backbone
   -a. Transformers dominate LLMs but have quadratic complexity in sequence length.
   -b. Mamba architecture (selective state space models) provides linear-time sequence modeling, more efficient for long contexts.
   -c. Limitation: Mamba alone may miss fine details.
   -d. Solution: Hybrid design – mostly Mamba-MLP layers with a small proportion of Transformer layers for long-context modeling.
   -e. Cosmos-Reason1-56B uses this hybrid backbone for efficiency + accuracy.

4. Reinforcement Learning (RL) for Physical AI

Cosmos-Reason1 adapts pre-trained VLMs into Physical AI reasoning models through two stages:

4.1 Algorithm
    -a. Uses GRPO (Group Relative Policy Optimization) for RL.
    -b. Advantage function avoids a critic network, computed as:
        Ai = ( R(oi) – mean(G) ) / std(G)
        where R(oi) is reward for response oi, and G is the group of responses.
    -c. Benefits: simple, efficient, critic-free training.

4.2 RL Training Framework
    -a. Novel asynchronous framework:
        -1. Policy training and actor rollout are decoupled, avoiding sync overhead.
        -2. Unified dispatcher schedules prompts asynchronously.
        -3. Improves training efficiency by ~160% compared to colocated frameworks.
    -b. Robustness features:
        -1. Training mesh management allows recovery without restarting if nodes fail.
        -2. Dispatcher redundancy for fault tolerance.
        -3. Supports dynamic scaling (up/down) for flexible resource allocation.

5. Data for Physical AI Training
   5.1 Supervised Fine-Tuning (SFT)
       -a. Purpose:
           -1. Enhance multimodal vision-language capabilities for Physical AI.
           -2. Develop two key reasoning abilities:
               -1) Physical common sense reasoning.
               -2) Embodied reasoning.
       Challenge: existing datasets insufficient → curated specialized SFT datasets.
     5.1.1 Physical Common Sense SFT
           -a. Pipeline stages:
               -1. Human-in-the-loop video curation.
               -2. Detailed captioning (via annotators or VLMs).
               -3. QA construction:
                   -1) Understanding Qs: based on captions.
                   -2) Reasoning Qs: require beyond-caption inference.
               -4. Reasoning traces: generated by DeepSeek-R1, parsed into CoT + answer.
               -5. Cleaning and rewriting annotations.
           -b. Datasets:
               -1. Free-form QA: 9.9k curated videos → ~99k understanding + ~59.4k reasoning samples.
               -2. MCQs: 1.2M captioned clips → 2.4M understanding MCQs; 356k clips → 600k reasoning MCQs.
     5.1.2 Embodied Reasoning SFT
           Focus: short-horizon reasoning →
           -a. Task-completion verification.
           -b. Action affordance.
           -c. Next plausible action prediction.
           
           Pipeline:
           -a. Extract short-horizon video segments.
           -b. Annotate state-action context (captions from VLMs or humans).
           -c. Create reasoning QA pairs.
           -d. Generate reasoning traces via DeepSeek-R1.
           -e. Clean/rewrite outputs.
           
           Datasets:
           -a. BridgeData V2: robot manipulation (pick-and-place, stacking). → 129.5k clips.
           -b. RoboVQA: large-scale robotics VQA → ~930k QA pairs with reasoning.
           -c. AgiBot World: real robot tasks → 19.8k clips, subtask prediction.
           -d. HoloAssist: egocentric manipulation tasks → 139.6k clips, with mistakes + corrections.
           -e. Autonomous Vehicles: 12.4k videos (~70h), captions + immediate driving action prediction.
     5.1.3 Intuitive Physics SFT
           Additional datasets for fundamental physics reasoning:
           -a. Spatial puzzles: patch-based shuffling task → 11k samples.
           -b. Arrow-of-Time (AoT): forward vs. reversed video reasoning → 30k clips.
           -c. Object permanence: simulated robot tasks (Libero) with occlusion/removal → 10k clips.
   5.2 Reinforcement Learning Post-Training
       -a. Converts SFT samples into multiple-choice format for reward assignment.
       -b. Covers three domains: common sense, embodied reasoning, intuitive physics.
       -c. Dataset stats: ~30k RL samples (Table 5).
       -d. Reward design:
           -1. Rule-based and verifiable.
           -2. Subsets divided into easy/hard difficulty for curriculum-style training.
       -e. Manual validation ensures MCQs are unambiguous and balanced.

6. Key Takeaways
   -a. Cosmos-Reason1 family:
       -1. 7B dense model and 56B hybrid model for Physical AI reasoning.
   -b. Architecture:
       -1. Decoder-only multimodal alignment.
       -2. Hybrid Mamba-MLP-Transformer backbone for efficiency + long-context modeling.
   -c. Training:
       -1. Progressive SFT + RL pipeline to specialize in physics-based reasoning.
   -d. Data:
       -1. Curated datasets across physical common sense, embodied reasoning, and intuitive physics.
       -2. Covers free-form, MCQ, CoT reasoning, and multimodal input-output pairs.
   -e. Outcome: Cosmos-Reason1 achieves physics common sense reasoning and embodied decision-making capabilities beyond prior VLMs.
