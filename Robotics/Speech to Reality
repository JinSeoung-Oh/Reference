### From https://news.mit.edu/2025/mit-researchers-speak-objects-existence-using-ai-robotics-1205
### https://dl.acm.org/doi/10.1145/3745778.3766670

This article introduces a research project from MIT that demonstrates how the combination of generative AI and robotics is bringing us closer to an era 
in which objects can be created simply by speaking. 
MIT researchers have developed a system called Speech-to-Reality, which allows users to verbally describe an object and have it physically manufactured 
within just a few minutes. Using this system, simple pieces of furniture can be built in approximately five minutes, 
in sharp contrast to traditional 3D printing methods that often require hours or even days.

In the Speech-to-Reality system, a robotic arm mounted on a table receives spoken input from a human user 
(for example, “I want a simple stool”) and then constructs a physical object by assembling modular components. 
To date, the research team has successfully produced stools, shelves, chairs, small tables, and even decorative items such as a dog statue.

The core contribution of this system lies in its integration of natural language processing, 
3D generative AI, and robotic assembly into a single unified pipeline. 
According to Alexander Htet Kyaw, an MIT graduate student leading the project, while these three areas have each advanced rapidly on their own, 
they have rarely been combined in a way that enables the creation of real physical objects from a simple speech prompt.

The idea originated when Kyaw took Professor Neil Gershenfeld’s course, “How to Make Almost Anything,” at MIT. 
He built the initial version of the Speech-to-Reality system as part of the class and later continued developing it at the MIT Center for Bits and Atoms (CBA), 
under Gershenfeld’s direction, in collaboration with graduate students from the Department of Mechanical Engineering and CBA.

The operation of the system proceeds through a clearly defined sequence of stages. 
First, speech recognition and a large language model are used to interpret the user’s request. 
Next, 3D generative AI produces a digital mesh representation of the requested object. 
A voxelization algorithm then decomposes this 3D mesh into discrete, assembly-ready components.

In the subsequent stage, the AI-generated structure is not used as-is. 
Instead, geometric processing is applied to incorporate real-world fabrication and physical constraints, such as the number of components, overhang limitations, 
and geometric connectivity. 
From this processed structure, a feasible assembly sequence is generated, followed by automated path planning that enables the robotic arm 
to physically assemble the object.

One of the system’s most important advantages is that it dramatically lowers the barrier to entry for design and manufacturing. 
Users do not need expertise in 3D modeling or robotic programming; natural language alone is sufficient. 
Moreover, the system supports rapid, on-demand production, enabling objects to be built within minutes rather than hours.

The research team plans to further improve the system by increasing the load-bearing capacity of the furniture, 
replacing the current magnetic cube connections with more robust mechanical connections. 
In addition, they have developed pipelines that convert voxel-based structures into feasible assembly sequences for small, 
distributed mobile robots, opening the possibility of extending this approach to structures at much larger scales.

Another key motivation for using modular components is waste reduction in physical object manufacturing. Objects can be disassembled and reassembled 
into new forms—for example, converting a sofa into a bed when the sofa is no longer needed—supporting more sustainable material use.

Building on his prior experience with gesture recognition and augmented reality for human–robot interaction,
Kyaw is also working to integrate both speech and gestural control into the Speech-to-Reality system.

Drawing inspiration from the replicator in the Star Trek franchise and the robots in the animated film Big Hero 6, 
Kyaw describes his long-term vision as one in which people can create physical objects quickly, accessibly, 
and sustainably. Ultimately, he envisions a future where reality itself can be generated on demand.
