### From https://levelup.gitconnected.com/drawing-and-coding-18-rl-algorithms-from-scratch-714ec2f581e5

"""
1. Hybrid Idea
   -a. Dynaâ€‘Q augments modelâ€‘free Qâ€‘learning with a simple modelâ€‘based component to gain sample efficiency.
   -b. Each real experience (ð‘ ,ð‘Ž,ð‘Ÿ,ð‘ â€²) is used three ways:
       -1. Direct Qâ€‘learning update on ð‘„(ð‘ ,ð‘Ž)
       -2. Model learning: store that (ð‘ ,ð‘Ž) leads to (ð‘Ÿ,ð‘ â€²)
       -3. Planning: perform k simulated updates by sampling previously seen 
                     (ð‘ _ð‘,ð‘Ž_ð‘), retrieving (ð‘Ÿ_ð‘,ð‘ â€²_ð‘) from the model, and applying another Qâ€‘learning update.

2. Data Structures
   -a. Qâ€‘table: ð‘ž_ð‘¡ð‘Žð‘ð‘™ð‘’_ð‘‘ð‘¦ð‘›ð‘Žð‘ž[(state)][action]â†’ð‘„(ð‘ ,ð‘Ž)
   -b. Model: ð‘šð‘œð‘‘ð‘’ð‘™_ð‘‘ð‘¦ð‘›ð‘Žð‘ž[(state,action)]â†’(ð‘Ÿ,ð‘ â€²)
   -c. observed_state_actions list stores all encountered stateâ€“action pairs for random sampling during planning.

3. Model Update
   -a. update_model records (ð‘Ÿ,ð‘ â€²) for a stateâ€“action pair and adds the pair to the sampling list if new.

4. Planning Step
  -a. planning_steps iterates k times:
      â€“ randomly picks a stored (ð‘ _ð‘,ð‘Ž_ð‘),
      â€“ looks up its predicted (ð‘Ÿ_ð‘,ð‘ â€²_ð‘),
      â€“ calls the standard update_q_value (Qâ€‘learning) with is_done=False.

5. Behavior Observed (visualization)
   -a. Episode lengths drop rapidly; rewards rise steeplyâ€”evidence of fast convergence.
   -b. Efficiency gain is attributed to planning with k=50 simulated updates per real step.
"""

# Q-Table: q_table[(state_tuple)][action_index] -> q_value (same as before)
q_table_dynaq: DefaultDict[Tuple[int, int], Dict[int, float]] = \
    defaultdict(lambda: defaultdict(float))

# Model: model[(state_tuple, action_index)] -> (reward, next_state_tuple)
model_dynaq: Dict[Tuple[Tuple[int, int], int], Tuple[float, Tuple[int, int]]] = {}

# Track observed state-action pairs for sampling during planning
observed_state_actions: List[Tuple[Tuple[int, int], int]] = []

# Model Update Function
def update_model(model: Dict[Tuple[Tuple[int, int], int], Tuple[float, Tuple[int, int]]],
                 observed_pairs: List[Tuple[Tuple[int, int], int]],
                 state: Tuple[int, int],
                 action: int,
                 reward: float,
                 next_state: Tuple[int, int]) -> None:
    """ Updates the deterministic tabular model and observed pairs list. """
    state_action = (state, action)
    model[state_action] = (reward, next_state)  # Store outcome

    # Add to list if this pair hasn't been seen before
    if state_action not in observed_pairs:
        observed_pairs.append(state_action)

# Planning Step Function
def planning_steps(k: int,  # Number of planning steps
                   q_table: DefaultDict[Tuple[int, int], Dict[int, float]],
                   model: Dict[Tuple[Tuple[int, int], int], Tuple[float, Tuple[int, int]]],
                   observed_pairs: List[Tuple[Tuple[int, int], int]],
                   alpha: float, gamma: float, n_actions: int) -> None:
    """ Performs 'k' simulated Q-learning updates using the model. """
    if not observed_pairs:  # Can't plan without observations
        return

    for _ in range(k):
        # 1. Sample a random previously observed state-action pair
        state_p, action_p = random.choice(observed_pairs)

        # 2. Query the model for the simulated outcome
        reward_p, next_state_p = model[(state_p, action_p)]

        # 3. Apply Q-learning update using the simulated experience
        #    (Assuming simulated steps don't end the episode unless model says so)
        #    Need the update_q_value function from Q-Learning section here.
        update_q_value(q_table, state_p, action_p, reward_p, next_state_p,
                       alpha, gamma, n_actions, is_done=False)  # Assume not done in simulation
                       # (A more complex model could predict 'done')


