### From https://levelup.gitconnected.com/drawing-and-coding-18-rl-algorithms-from-scratch-714ec2f581e5

"""
1. Synchronous Actorâ€‘Critic
   -a. A2C uses an Actor ðœ‹_ðœƒ(ð‘Žâˆ£ð‘ ) for action selection and a Critic ð‘‰_ðœ™(ð‘ ) for state evaluation, updating both synchronously on each batch.

2. Advantage Estimate
   -a. Advantage for time step ð‘¡:
       ð´^_ð‘¡ = ð‘…_ð‘¡ âˆ’ ð‘‰_ðœ™(ð‘ _ð‘¡),
       where ð‘…_ð‘¡ is an ð‘›-step or GAE return. Lower variance than REINFORCE.

3. Rollout â†’ Update Cycle
   -a. Collect ð‘ steps of (ð‘ ,ð‘Ž,ð‘Ÿ,ð‘ â€²) plus value estimates ð‘‰(ð‘ )
   -b. Compute returnsâ€‘toâ€‘go and advantages for the batch.
   -c. Policy loss (actor): âˆ’ð¸[log ðœ‹ðœƒ(ð‘Ž_ð‘¡âˆ£ð‘ _ð‘¡)ð´^_ð‘¡]âˆ’ ð›½ entropy
   -d. Value loss (critic): MSE between ð‘‰_ðœ™(ð‘ _ð‘¡) and ð‘…_ð‘¡
   -e. Apply gradients to actor and critic simultaneously.

4. Behaviour Observed
   -a. Rewards rise quickly to nearâ€‘maximum; episode lengths drop rapidly.
   -b. Value loss peaks early, then declines as the critic stabilizes.
   -c. Policy loss is noisy but trends downward; final policy grid shows a consistent path toward the goal.
"""

# A2C Update Step
def update_a2c(actor: PolicyNetwork,
               critic: ValueNetwork,
               actor_optimizer: optim.Optimizer,
               critic_optimizer: optim.Optimizer,
               states: torch.Tensor,
               actions: torch.Tensor,
               advantages: torch.Tensor,    # Calculated GAE advantages
               returns_to_go: torch.Tensor, # Target for value function (Adv + V_old)
               value_loss_coeff: float,     # Weight for critic loss
               entropy_coeff: float         # Weight for entropy bonus
               ) -> Tuple[float, float, float]: # Avg losses

    # --- Evaluate current networks ---
    policy_dist = actor(states)
    log_probs = policy_dist.log_prob(actions)     # Log prob of actions taken
    entropy = policy_dist.entropy().mean()        # Average entropy
    values_pred = critic(states).squeeze()        # Critic's value prediction

    # --- Calculate Losses ---
    # Policy Loss (Actor): - E[log_pi * A_detached] - entropy_bonus
    policy_loss = -(log_probs * advantages.detach()).mean() - entropy_coeff * entropy

    # Value Loss (Critic): MSE(V_pred, Returns_detached)
    value_loss = F.mse_loss(values_pred, returns_to_go.detach())

    # --- Optimize Actor ---
    actor_optimizer.zero_grad()
    policy_loss.backward()        # Calculate actor gradients
    actor_optimizer.step()        # Update actor weights

    # --- Optimize Critic ---
    critic_optimizer.zero_grad()
    (value_loss_coeff * value_loss).backward()  # Scale loss before backward
    critic_optimizer.step()       # Update critic weights

    # Return losses for logging (policy objective part, value loss, entropy)
    return policy_loss.item() + entropy_coeff * entropy.item(), value_loss.item(), entropy.item()


