### From https://arxiv.org/pdf/2505.03335

1. Motivation
   Existing methods like RLVR (Reinforcement Learning with Verifiable Rewards) use only outcome-based 
   feedback to train LLMs. 
   However, they still rely on human-curated datasets of questions and answers. 
   This dependency introduces a long-term scalability bottleneck and limits the autonomous evolution of AI systems.

   -a. Absolute Zero removes this dependency, enabling a model to:
       -1. Propose its own tasks
       -2. Solve them
       -3. Learn from interaction with a verifiable environment
           â†’ all in a fully self-play, zero-data setup.

2. Reinforcement Learning Frameworks Recap
   2.1. Supervised Fine-Tuning (SFT)
        -a. Given expert-labeled tuples:
            ğ·={(ğ‘¥,ğ‘âˆ—,ğ‘¦âˆ—)}
            where ğ‘¥ is the query, ğ‘âˆ—  is the CoT, ğ‘¦âˆ— is the answer.
        -b. Objective:
            ğ¿_SFT(ğœƒ)=âˆ’ğ¸_((ğ‘¥,ğ‘âˆ—,ğ‘¦âˆ—)âˆ¼ğ·) logğœ‹_ğœƒ(ğ‘âˆ—,ğ‘¦âˆ—âˆ£ğ‘¥)
   2.2. Reinforcement Learning with Verifiable Rewards (RLVR)
        Uses only (ğ‘¥,ğ‘¦âˆ—) pairs; model generates ğ‘¦, receives scalar reward ğ‘Ÿ(ğ‘¦,ğ‘¦âˆ—)
        -a. Objective:
            ğ½_RLVR(ğœƒ)=ğ¸_((ğ‘¥,ğ‘¦âˆ—)âˆ¼ğ·, ğ‘¦âˆ¼ğœ‹_ğœƒ(â‹…âˆ£ğ‘¥)) [ğ‘Ÿ(ğ‘¦,ğ‘¦âˆ—)]

3. Absolute Zero Paradigm
   3.1. Objective
        No external data. A single model ğœ‹_ğœƒ acts as both:
        -a. Proposer: ğœ‹_ğœƒ^propose
        -b. Solver: ğœ‹_ğœƒ^solve

        Workflow:
        -a. Propose a task ğœâˆ¼ğœ‹_ğœƒ^propose(â‹…âˆ£ğ‘§)
        -b. Environment constructs task from ğœ: (ğ‘¥,ğ‘¦âˆ—)âˆ¼ğ‘“_ğ‘’(â‹…âˆ£ğœ)
        -c. Solve task: ğ‘¦âˆ¼ğœ‹_ğœƒ^solve(â‹…âˆ£ğ‘¥)
        -d. Rewards:
            -1. ğ‘Ÿ_ğ‘’^propose(ğœ,ğœ‹_ğœƒ): measures learnability
            -2. ğ‘Ÿ_ğ‘’^solve(ğ‘¦,ğ‘¦âˆ—): solution accuracy

        Final training objective:
        ğ½(ğœƒ):=max_ğœƒ ğ¸_(ğ‘§âˆ¼ğ‘(ğ‘§)) [ğ¸_((ğ‘¥,ğ‘¦âˆ—)âˆ¼ğ‘“_ğ‘’(â‹…âˆ£ğœ))    [ğ‘Ÿ_ğ‘’^propose(ğœ,ğœ‹)ğœƒ)+ğœ†ğ¸_(ğ‘¦âˆ¼ğœ‹_ğœƒ^solve(â‹…âˆ£ğ‘¥) ğ‘Ÿ_ğ‘’^solve(ğ‘¦,ğ‘¦âˆ—)]]
                                   (ğœâˆ¼ğœ‹_ğœƒ^propose(â‹…âˆ£ğ‘§)
4. Reward Formulation
   4.1. Proposer Reward
        Let ğ‘ŸË‰_solve be the average success rate from ğ‘› Monte Carlo solver rollouts:
        ğ‘ŸË‰_solve = 1/ğ‘› âˆ‘(ğ‘–=1 to ğ‘›) ğ‘Ÿ^(ğ‘–)_solve
        Then,
        ğ‘Ÿ_propose={0              ifÂ ğ‘ŸË‰_solve=0Â orÂ 1
                   1âˆ’ğ‘ŸË‰_solve     otherwise
   4.2. Solver Reward
        Simple binary indicator:
        ğ‘Ÿ_solve=ğ¼(ğ‘¦=ğ‘¦âˆ—)
   4.3. Composite Reward with Format Penalty
        ğ‘…(ğ‘¦_ğœ‹)={ğ‘Ÿ_role      ifÂ passableÂ (correctÂ format)
                âˆ’0.5        ifÂ incorrectÂ butÂ well-formatted
                âˆ’1          ifÂ formattingÂ error

5. Reasoning Modes in AZR
   All tasks are framed as code triplets (ğ‘,ğ‘–,ğ‘œ) where ğ‘œ=ğ‘(ğ‘–)
   5.1. Deduction
        -a. Given: ğ‘,ğ‘–
        -b. Predict: ğ‘œ
        -c. Verification: ğ‘œ_ğœ‹=ğ‘œâˆ—
   5.2. Abduction
        -a. Given: ğ‘,ğ‘œ
        -b. Predict: ğ‘–
        -c. Verification: ğ‘(ğ‘–_ğœ‹)=ğ‘œâˆ—
   5.3. Induction
        -a. Given: {(ğ‘–_ğ‘›,ğ‘œ_ğ‘›)}^(ğ‘/2)_(ğ‘›=1), ğ‘š
        -b. Predict: program ğ‘_ğœ‹
        -c. Verification: âˆ€_ğ‘›>ğ‘/2, ğ‘_ğœ‹(ğ‘–_ğ‘›)=ğ‘œ_ğ‘›

6. Self-Play Learning Algorithm
   -a. Algorithm 1: AZR Self-Play
       -1. Seed buffers ğ·_ded, ğ·_abd, ğ·_ind
   -b. For each training step ğ‘¡âˆˆ[1,ğ‘‡]
       -1. For each batch ğ‘âˆˆ[1,ğµ]:
           -1) Propose tasks (deduction, abduction, induction)
           -2) Validate proposed triplets with:
               - Syntax check
               - Safety check (no os/sys/shutil etc.)
               - Determinism:
                 âˆ€_ğ‘–âˆˆğ¼, lim_(ğ‘—â†’âˆ)ğ‘(ğ‘–)^(1)=ğ‘(ğ‘–)^(2)=â‹¯=ğ‘(ğ‘–)^(ğ‘—)
           -3) Solve proposed tasks
           -4) Compute rewards ğ‘Ÿ^propose, ğ‘Ÿ^solve
           -5) Update policy using Task-Relative REINFORCE++

7. Task-Relative REINFORCE++ (TRR++)
   Classic REINFORCE uses global baseline ğ‘=ğ¸[ğ‘Ÿ]. TRR++ computes task-role specific normalized advantages:
   ğ´^(task, role)_norm = ğ‘Ÿâˆ’ğœ‡_(task,role) / ğœ_(task,role) ,taskâˆˆ{ind,ded,abd}, roleâˆˆ{propose,solve}
   This gives six distinct baselines, reducing variance per task-role configuration.

8. Key Findings
   -a. Code priors help general reasoning: AZR-Coder-7B outperforms base Qwen-7B on math despite a worse 
                                           starting point.
   -b. Cross-domain generalization: AZR-trained on code improves math by +15.2 points vs. +0.65 from RLVR.
   -c. Scaling trend: Bigger base â†’ bigger gains (3B: +5.7, 7B: +10.2, 14B: +13.2).
   -d. Scratchpad behavior emerges: Comments and plans appear organically during code generation.
   -e. Reasoning length varies by task: Abduction â†’ longest sequences (trial-and-error), deduction/induction
                                        â†’ more concise.
   -f. Safety: Some models (e.g., Llama3.1-8B) show "uh-oh moments", signaling the need for safety-aware RL training.

9. Conclusion
   Absolute Zero shows that superhuman reasoning capabilities can emerge from pure self-play â€” 
   with no human-curated data. Through carefully designed rewards, role unification, and a verifiable environment, 
   AZR sets a new precedent in self-evolving LLM training.
   It is an important step toward:
   -a. Autonomous, open-ended reasoning models
   -b. Scalable RL with verifiable rewards
   -c. Sustainable post-human intelligence training regimes
