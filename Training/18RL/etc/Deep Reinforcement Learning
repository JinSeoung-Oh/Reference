### From https://levelup.gitconnected.com/deep-reinforcement-learning-for-self-evolving-ai-a0ccb80310c8

1. Introduction
   Deep Reinforcement Learning (DRL) is a foundational method in AI, enabling systems to learn optimal behavior 
   through trial-and-error interactions with the environment. 
   It merges Reinforcement Learning (RL) and Deep Learning (DL) to handle high-dimensional, complex data representations.

2. What is Deep Reinforcement Learning?
   -a. DRL integrates:
       -1. Reinforcement Learning: An agent interacts with an environment, receives feedback (rewards), 
                                   and improves its decision-making to maximize cumulative reward.
       -2. Deep Learning: A multi-layered neural network processes complex input data (e.g., images, sensor streams) 
                          and generalizes across unseen states.

3. Core Components
   -a. Agent: The learner (neural network)
   -b. Environment: The system it interacts with
   -c. State: Current condition of the environment
   -d. Action: Choices the agent makes
   -e. Policy: Strategy mapping states to actions
   -f. Reward: Feedback on an action’s outcome
   Example: In a self-driving car, the agent perceives the road image (state), decides to steer (action), 
            and receives a reward (+3 for smooth driving, -10 for crashing).

4. DRL vs Traditional ML
   -a. Supervised Learning: Uses labeled data
   -b. Unsupervised Learning: Finds hidden structure
   -c. Reinforcement Learning: Learns via environmental interaction

5. Limitations of Traditional RL and DRL Advantages
   -a. Curse of Dimensionality
       -1. Traditional RL: Cannot scale with large state spaces
       -2. DRL: Learns compressed representations through neural networks
   -b. Raw Sensory Data
       -1. Traditional RL: Needs engineered features
       -2. DRL: Learns end-to-end from raw data
   -c. Generalization
       -1. Traditional RL: Learns per-scenario
       -2. DRL: Generalizes across states via neural network flexibility

6. DRL Algorithms: Rules of the System
   DRL algorithms dictate how agents perceive, decide, and learn:
   -a. High-Level Classification
       -1. Model-Free RL: Learns policies directly from interactions
           - Online RL (Policy-based): Policy Gradient, PPO, A2C, TRPO
           - Off-Policy RL (Value-based): DQN, C51, QR-DQN, HER
           - Offline RL: Learns from fixed datasets
       -2. Model-Based RL: Learns or uses a model of environment
           - Given Dynamics: Uses known models (e.g., AlphaZero)
           - Learned Dynamics: Predicts future states (e.g., World Models)

7. Optimization Objective
   All DRL seeks to find an optimal policy π* maximizing the expected sum of discounted rewards
   Where γ is the discount factor. The value function Vπ(s) captures the expected cumulative reward from state s under policy π.

8. Bellman Optimality Equation
   Expresses recursive relation for value 
   Used in foundational methods for solving MDPs.

9. Solving MDPs: Policy vs Value Iteration
   -a. Policy Iteration:
       -1. Iteratively improves policy via evaluation and update
       -2. Pros: Fast convergence; Cons: Expensive evaluation
   -b. Value Iteration:
       -1. Updates value function and derives policy later
       -2. Pros: Simpler updates; Cons: May need more iterations

10. DRL Learning Loop
    -a. Observe state
    -b. Select action (via policy)
    -c. Execute and receive reward
    -d. Update network via experience
    This loop trains the agent to perform increasingly better actions over time.

11. Exploration-Exploitation Dilemma
    Balancing known (exploit) vs. unknown (explore):
    -a. Epsilon-Greedy
        -1. Probability ε: Choose random action
        -2. Probability 1-ε: Choose best-known action
        -3. Decay ε over time
    -b. Upper Confidence Bound (UCB)
        -1. Action = estimated value + exploration bonus
        -2. Encourages underexplored actions
    -c. Thompson Sampling
        -1. Sample from posterior over value estimates
        -2. Select action with highest sampled value
        -3. Bayesian exploration
    These strategies guide agents in balancing learning and performance.

12. Conclusion
    Deep Reinforcement Learning is a powerful learning paradigm combining dynamic feedback-based learning with deep representation power.
    It addresses limitations of traditional RL, enabling AI systems to make intelligent decisions in complex, 
    uncertain environments through end-to-end learning, generalization, and continual adaptation.

