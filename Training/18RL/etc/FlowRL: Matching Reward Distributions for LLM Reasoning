### From https://huggingface.co/papers/2509.15207

1. Big Picture
   -1. Replace reward maximization (PPO/GRPO)â€”prone to mode collapseâ€”with distribution matching: 
       align ğœ‹_ğœƒ to a reward-weighted target via reverse KL. Use a learnable partition ğ‘_ğœ™(ğ‘¥) to turn scalar rewards into a valid distribution.

1.1 From Reward to Distribution
    -1. Objective: min_ğœƒ ğ·_(KL)(ğœ‹_ğœƒâˆ¥exp(ğ›½ğ‘Ÿ)/ğ‘_ğœ™) â‡’ ğœ‹_ğœƒâˆexp(ğ›½ğ‘Ÿ)
    -2. Proposition 1: In expected-gradient terms, the KL objective is equivalent to minimizing the Trajectory Balance squared loss
        [logğ‘_ğœ™(ğ‘¥)+logğœ‹_ğœƒ(ğ‘¦âˆ£ğ‘¥)âˆ’ğ›½ğ‘Ÿ(ğ‘¥,ğ‘¦)]^2
    -3. Practical upshot (Remark 2): TB is a tractable surrogateâ€”stable squared loss, no need to compute the intractable Z; just learn ğ‘_ğœ™

1.2 FlowRL: Making TB work for long CoT
    -1. Two practical hurdles
        -1) Exploding gradients: TB is sequence-level; logğœ‹_ğœƒ sums over tokens â†’ gradient norm scales with length (up to 8k tokens).
        -2) Sampling mismatch: TB assumes fully on-policy samples, while PPO/GRPO reuse off-policy trajectories from ğœ‹_(old)
   -2. Reward refactoring and normalization
       -1) Inject reference model prior: exp(ğ›½ğ‘Ÿ)â‹…ğœ‹_ref(ğ‘¦âˆ£ğ‘¥)
       -2) Group-normalize rewards within each sampled group: ğ‘Ÿ^_ğ‘–=(ğ‘Ÿ_ğ‘–âˆ’mean(ğ‘Ÿ))/std(ğ‘Ÿ)
       -3) TB becomes [logğ‘_ğœ™+logğœ‹_ğœƒâˆ’ğ›½ğ‘Ÿ^âˆ’logğœ‹_ref]^2
   -3. Two stabilizers
       -1) Length normalization (Remark 3): rescale logğœ‹_ğœƒ and logğœ‹_ref by 1/âˆ£ğ‘¦| to balance long/short trajectories and tame gradient growth.
       -2) Importance sampling with clipping (Remark 4):
           ğ‘¤=clip(ğœ‹_ğœƒ/ğœ‹_old,1âˆ’ğœ–,1+ğœ–)_detach as a weight in the surrogate; detach prevents excessive policy drift.

2. Final objective: FlowRL 
   ğ¿_FlowRL=ğ‘¤â‹…[logğ‘_ğœ™(ğ‘¥)+1/|ğ‘¦âˆ£logğœ‹_ğœƒ(ğ‘¦âˆ£ğ‘¥)âˆ’ğ›½ğ‘Ÿ^(ğ‘¥,ğ‘¦)âˆ’1/âˆ£ğ‘¦âˆ£logğœ‹_ref(ğ‘¦âˆ£ğ‘¥)]^2,
   with
   ğ‘¤=clip(ğœ‹_ğœƒ/ğœ‹_old,1âˆ’ğœ–,1+ğœ–)_detach,
   ğ‘Ÿ^_ğ‘–=ğ‘Ÿ_ğ‘–âˆ’mean(ğ‘Ÿ)/std(ğ‘Ÿ)

3. Interpretation: FlowRL keeps the KLâ†”TB equivalence, while length normalization cures gradient blow-up and IS-clipping resolves off-policy 
                   euseâ€”so the policy learns to sample diverse, high-reward trajectories in proportion to rewards instead of collapsing onto dominant modes. 
                   Implementation and analysis details follow in Â§4/Â§B.
