### From https://arxiv.org/abs/2504.02495

1. Key takeawayÂ â€”Â DeepSeekâ€‘AIâ€™s Selfâ€‘PrincipledÂ CritiqueÂ TuningÂ (SPCT) shows that reward models become 
   far more accurate and scalable when they (i)Â generate taskâ€‘specific â€œprinciplesâ€ onâ€‘theâ€‘fly before judging answers,
   (ii)Â train those principle+critique skills with a twoâ€‘stage Rejectiveâ€‘Fineâ€‘TuningÂ â†’Â ruleâ€‘based online RL pipeline,
   and (iii)Â exploit parallel sampling plus a lightweight â€œmeta RMâ€ to vote out bad judgments at inferenceÂ time. 
   Together, these ideas let a single pointwise Generative Reward Model (GRM) outperform larger, 
   pairwise scalar RMs across diverse benchmarks while enjoying linear gains from extra inference compute.â€‹

2.Â Preliminaries
   2.1Â Taxonomy of Rewardâ€‘Model (RM) Approaches
       Dimension |	Categories |	Pros |	Cons
       Reward generation	| Scalar â€“ single number; Semiâ€‘scalar â€“ numberÂ + freeâ€‘text critique; Generative â€“ freeâ€‘text only (score is parsed)	| Scalar is cheap; generative gives rich signal	| Scalar canâ€™t exploit sampling diversity; generative needs extraction heuristics.â€‹
       Scoring pattern	| Pointwise â€“ score each answer independently; Pairwise â€“ pick the best among candidates	| Pointwise can handle any #answers; pairwise aligns with preference data	| Pairwise struggles to rate single answers; needs tricks for >2 answers.â€‹
      
       Inferenceâ€‘time scalability depends on whether repeated sampling actually yields different rewards. 
                 Scalar methods output the same score every time, so they saturate quickly; 
                 generative RMs can output diverse critiques and thus aggregate better. 
                 Input flexibility is highest for pointwise generative RMs because they naturally handle 1â€‘N answers 
                 without special wrappers.â€‹
   2.2Â Why â€œPrinciplesâ€ boost reward quality
       â€œPrinciplesâ€ are concise, criterionâ€‘specific rules that guide critique generation 
       (a direct riff on ConstitutionalÂ AI). When GPTâ€‘4o was asked to write principles and then judge answers 
       on RewardBenchâ€‘Chatâ€‘Hard and PPEâ€‘IFEval, raw selfâ€‘generated principles helped little, 
       but filtered principles (kept only if their resulting rewards matched ground truth) lifted accuracy by 5â€‘10Â pp.â€‹
       This suggests that good principles matter, yet producing them automatically remains hard.

3.Â Selfâ€‘PrincipledÂ CritiqueÂ TuningÂ (SPCT)
  3.1Â Unpinning principles from preprocessing to generation
      SPCT lets a single GRM first sample a set of principles conditioned on the query + candidate answers, 
      then generate critiques/rewards using those principles:
      -1. {ğ‘_ğ‘–}âˆ¼ğ‘_ğœƒ(ğ‘¥,{ğ‘¦_ğ‘–}) â€ƒâ†’ adaptive principles
      -2. ğ¶âˆ¼ğ‘Ÿ_ğœƒ(ğ‘¥,{ğ‘¦_ğ‘–},{ğ‘_ğ‘–})â€ƒâ†’ critique
      -3. ğ‘†_ğ‘–=ğ‘“_extract(ğ¶) â†’ discrete score 1â€‘10
      Making principle generation part of the forward pass turns the RM into a selfâ€‘prompting critic
      that can refine its own rubric per task.â€‹
  3.2Â Twoâ€‘phase optimisation
      -1. Rejective fineâ€‘tuning (RFT) â€“ sample many (principlesÂ +Â critiquesÂ +Â scores) trajectories; 
                                        keep only those whose scores match groundâ€‘truth preference labels; 
                                        discard trivial samples where every trajectory is correct. 
                                        This teaches the right format and basic reasoning.â€‹
      -2. Ruleâ€‘based online RL (GRPO style) â€“ continue training with a binary outcome rewardÂ 
                                              (+1 if the extracted scores correctly rank answers, â€“1 otherwise) 
                                              and a stronger KL penalty (no extra format reward). 
                                              This sharpens both principle quality and critique fidelity.â€‹

4Â Inferenceâ€‘Time Scaling Techniques
  4.1Â Voting with multiple sampled rewards
      Draw k independent (principle,Â critique,Â score) sets, then sum the scores for each answer: 
      ğ‘†_ğ‘–\* = âˆ‘(ğ‘—=1 to ğ‘—=ğ‘˜)ğ‘†_(ğ‘–,ğ‘—). Because each sample expands the discrete reward space, 
      accuracy improves roughlyÂ âˆšk until saturation. Shuffling answers before each rollout mitigates position bias.â€‹
  4.2Â Metaâ€‘RM guided voting
      Some samples are noisy. A lightweight scalar meta RM is trained (binary BCE) to predict 
      whether a sampled trajectory ranks answers correctly. 
      At inference, keep only the top kâ‚˜â‚‘â‚œâ‚ samples by metaâ€‘score before voting. 
      This filters out systematic hallucinations and yields another 2â€“3Â pp accuracy bump across RewardBench variants.â€‹


 * SPCT not only beats trainingâ€‘time scaling (bigger models) but also shows nearâ€‘linear gains with additional 
   inference compute up to kÂ â‰ˆÂ 32.â€‹

6Â Why It Matters
  -a. Generalâ€‘purpose evaluator.Â Because pointwise GRMs accept any number of answers,
      DeepSeekâ€‘GRM can serve as a dropâ€‘in replacement for static pairwise RMs in RLHF or offline evaluation.â€‹
  -b. Coâ€‘scaling with policy models.Â As policies grow, you can spend more inference FLOPs on the RM 
      rather than retraining it, mirroring the samplingâ€‘time scaling ideas in chainâ€‘ofâ€‘thought generation.â€‹
  -c. Automatic rubric discovery.Â Selfâ€‘generated principles remove much of the manual effort in writing
      constitutions or rule lists, while still grounding critiques in simple naturalâ€‘language heuristics.â€‹

7Â Remaining Challenges & Open Questions
  -a. Principle correctness guarantees.Â While metaâ€‘RM helps, there is no formal proof that generated 
      principles are logically sound or unbiased.
  -b. Resource overhead.Â Parallel sampling (e.g., kÂ =Â 32) can multiply inference cost; 
      efficient batching or caching is needed for production.
  -c. Beyond text.Â Adapting SPCT to multimodal RMs (images, code execution traces) requires redefining 
      both principles and extraction rules.

Bottom line: SPCT shows that letting a reward model write its own grading rubricâ€”and refining that skill 
             with online RLâ€”may be the missing piece for reliable, computeâ€‘scalable evaluation across openâ€‘ended tasks.

