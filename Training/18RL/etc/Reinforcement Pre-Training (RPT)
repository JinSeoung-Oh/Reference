### From https://medium.com/@aipapers/reinforcement-pre-training-rpt-by-microsoft-explained-e087806d278e
### https://aipapersacademy.com/reinforcement-pre-training/
### https://arxiv.org/abs/2506.08007

1. Key Concepts
   -a. Traditional RL in LLMs: Reinforcement learning is commonly applied in the post-training stage of large language models (LLMs). 
       -1. RLHF (Reinforcement Learning from Human Feedback): Involves collecting human preferences to rank model outputs. 
                                                              Powerful but costly and limited by human labeling bandwidth.
       -2. RLAIF (Reinforcement Learning from AI Feedback): Feedback is generated by another model. Requires a strong teacher model 
                                                            and risks alignment drift.
       -3. RLVR (Reinforcement Learning with Verifiable Rewards): Uses objective reward signals derived from verifiable rules 
                                                                  (e.g., correct math answers) to mitigate reward hacking.
   -b. Pre-training vs RL Data Scale: LLMs are pre-trained on vast corpora of general text (trillions of tokens), 
                                      while RL fine-tuning datasets are much smaller. This creates a bottleneck in scaling RL techniques.

2. RPT Overview
   -a. Reinforcement Pre-Training (RPT): A novel training paradigm proposed by Microsoft to incorporate reinforcement 
                                         learning principles into the pre-training stage.
   -b. Goal: Make better use of massive pre-training corpora by reframing the next-token prediction task 
             as a reinforcement learning problem.
   -c. Mechanism: Introduces "Next-Token Reasoning", where the model must produce a reasoning trace 
                  (a chain-of-thought) before outputting the next token. This turns simple prediction into a structured reasoning process.

3. RPT Process in Detail
   -a. Data Format:
       -1. Pre-training corpus is reused. Each data point is a text span.
       -2. Instead of just predicting the next token, the model must first produce a reasoning trace.
   -b. Sampling and Reward Computation:
       -1. For each prompt (context), multiple next-token reasoning sequences are generated.
       -2. The actual next token from the corpus serves as a ground-truth label.
       -3. Each sampled output is scored based on whether its final predicted token matches the ground truth.
       -4. Relative rewards are assigned across outputs in the same group.
   -c. Optimization with GRPO:
       -1. The model is updated using Group Relative Policy Optimization (GRPO), a stable RL method that trains the policy 
           to prefer higher-rewarded traces within a group.
       -2. This enables scalable and stable learning, avoiding pitfalls of reward hacking.

4. Core Idea: Next-Token Reasoning vs. Prediction
   -a. Standard Pre-Training: Model predicts next token directly from context.
   -b. Next-Token Reasoning: Model first generates a reasoning path, then uses that to select the next token.
   -c. Benefit: Transforms raw prediction into a verifiable reasoning task, enabling token-level reward computation.

5. Benefits of RPT
   -a. RL at Scale: Transforms massive pre-training datasets into reinforcement learning-compatible data.
   -b. Enhanced Reasoning: Encourages multi-step thinking before token generation.
   -c. Better Starting Point: Models trained with RPT are better aligned with reasoning tasks and benefit more from downstream 
                              RLHF or instruction tuning.
   -d. Avoids Reward Hacking: Uses ground-truth tokens for verifiable supervision.

6. Experimental Results
   -a. Benchmark: OmniMATH — a dataset for Olympiad-level math problems, focusing on complex, multi-step reasoning.
   -b. Models Compared:
       -1. Qwen-2.5–14B (base)
       -2. R1-Distill-Qwen-14B (standard distilled model)
       -3. RPT-14B (RPT-trained model)
   -c. Accuracy Metrics:
       -1. Accuracy measured at token-level across problems of varying difficulty (easy, medium, hard).
       -2. Difficulty is defined by token entropy (e.g., harder tokens have higher uncertainty).
   -d. Findings:
       -1. RPT-14B outperforms both base and distilled models on all difficulty levels.
       -2. In terms of next-token prediction accuracy, RPT-14B performs comparably to R1-Qwen-32B, despite being less 
           than half the size — indicating superior parameter efficiency.

7. Visual Summary
   -a. RPT Scales RL: Converts pre-training into a reinforcement learning problem.
   -b. Token-Level RL: Ground-truth tokens offer a natural reward signal.
   -c. GRPO: Enables stable learning with relative comparison within sample groups.
   -d. Efficiency: Achieves higher performance with fewer parameters.

8. Conclusion
   Reinforcement Pre-Training (RPT) introduces a new way to scale reinforcement learning in LLMs by applying it to the pre-training phase.
   By using next-token reasoning and verifiable ground-truth tokens for reward, RPT allows models to learn structured reasoning at scale.
   Trained with Group Relative Policy Optimization (GRPO), RPT yields models that generalize better, reason more effectively,
   and outperform larger models — all while making full use of the massive pre-training corpus for RL.
