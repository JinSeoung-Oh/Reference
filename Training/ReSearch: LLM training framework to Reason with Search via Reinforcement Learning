### From https://medium.com/@techsachin/research-llm-training-framework-to-reason-with-search-via-reinforcement-learning-bf65478c0fa3
### From https://arxiv.org/abs/2503.19470
### From https://github.com/Agent-RL/ReSearch

1. Overview and Motivation
   Integrating external search into LLM reasoning is particularly challenging for complex multi-hop questions that 
   require multiple retrieval steps. 
   To address this, the paper introduces ReSearch, a framework that trains LLMs to reason with search via reinforcement learning
   ‚Äîwithout relying on supervised data for intermediate reasoning steps. 
   ReSearch treats search operations as an integral part of the reasoning chain, 
   allowing text-based thinking and search to interact dynamically.

2. Key Components of the Framework
   -a. Expanded Reasoning Chain
       -1. Multimodal Chain Composition:
           Unlike previous methods that only include text-based thinking (enclosed by <think> ... </think>), 
           ReSearch‚Äôs reasoning chain also incorporates:
           -1) Search Queries: Enclosed by <search> ... </search>
           -2) Retrieval Results: Enclosed by <result> ... </result>
       -2. Interactivity Between Components:
           The framework steers both the timing and nature of search operations based on preceding text-based reasoning.
           Likewise, the retrieved factual information (wrapped in <result> tags) influences subsequent reasoning steps.
   -b. Reinforcement Learning via GRPO
       -1. Learning without Supervision:
           ReSearch trains LLMs using reinforcement learning (RL) and does not require annotated reasoning steps. 
           Instead, it incentivizes the model to incorporate search effectively through reward-based training.
       -2. Group Relative Policy Optimization (GRPO):
           -1) Rollouts with Search:
               Multiple reasoning-with-search chains (rollouts) are sampled for each input. 
               These chains include both text-based thought and search operations.
           -2) Objective Function:
               GRPO optimizes the policy by maximizing the probability of generating rollouts with higher rewards. 
               The normalized advantage 
               ùê¥_ùëñ=(ùëü_ùëñ‚àímean({ùëü_ùëó})) / std({ùëü_ùëó})
               is computed across a group of ùê∫ rollouts. Clipping and a KL regularization term (with coefficient ùõΩ) 
               further stabilize updates.
       -3. Iterative Rollout Generation:
           The rollout process is iterative:
           -1) The model generates a reasoning segment enclosed in <think>.
           -2) When a </search> tag is encountered, the text between the last <search> and </search> tags is used as a search
               query.
           -3) The search engine retrieves results, which are then inserted into the chain between <result> and </result> tags.
           -4) The updated chain, now enriched with external information, is fed back into the model for further reasoning 
               until an end-of-text marker (e.g., <endoftext> or <im_end>) is reached.
       -4. Retrieval Result Masking:
           Since retrieval results are provided by the search environment (and not generated by the LLM), 
           they are masked during loss computation. This prevents the training policy from being biased by the retrieved content.

3. Prompting and Training Templates
   -a. Two Template Types:
       The framework defines two distinct prompt templates:
       -1. For the Base Model:
           A direct conversation prompt where the assistant is instructed to reason and search when needed. 
           The template explicitly defines the tags:
           -1) <think> for reasoning,
           -2) <search> and <result> for search operations,
           -3) <answer> for the final answer (with the exact answer enclosed in LaTeX format, e.g., \boxed{...}).
       -2. For the Instruction-Tuned Model:
           A system prompt that instructs the model to act as a helpful assistant using the same tag conventions. 
           This template works alongside the chat template to ensure that the model follows the intended rollout format.

4. Reward Modeling
   -a. Dual Reward Components:
       -1. Answer Reward:
           The correctness of the final answer (extracted from the \boxed{} section) is compared to the ground truth using 
           an F1 score.
       -2. Format Reward:
           This component checks that the generated rollout adheres to the defined format 
           (correct use of <think>, <search>, <result>, and <answer> tags, 
           as well as the presence of the LaTeX-enclosed final answer).
   -b. Final Reward Calculation:
       The overall reward for a rollout is a combination of the answer reward and the format reward, 
       guiding the model to produce both accurate and well-formatted reasoning-with-search chains.

5. Experimental Setup and Results
   -a. Implementation Details:
       -1. Models and Datasets:
           Training and evaluation are performed on Qwen2.5 models (both 7B and 32B parameters, 
           with base and instruction-tuned variants). 
           The training set comes from 19,938 samples of the MuSiQue dataset, and training is run for 2 epochs.
       -2. Retrieval Environment:
           ReSearch utilizes FlashRAG as the retrieval toolkit, with E5-base-v2 as the retriever and Wikipedia data 
           (from Dec. 2018) as the knowledge base. For each search query during rollouts, the top-5 results are retrieved.
   -b. Performance Improvements:
       -1. Significant Gains Over Baselines:
           ReSearch outperforms all baseline methods on multiple multi-hop question-answering benchmarks.
           -1) For Qwen2.5-7B: An average improvement of 15.81% in exact match and 17.56% when evaluated via an LLM-as-a-judge.
           -2) For Qwen2.5-32B: Average improvements of 14.82% (exact match) and 15.46% (LLM-as-a-judge).
   -c. Generalization and Robustness:
       The reinforcement learning process enables ReSearch to learn a generalizable ability to reason with search.
       This ability is not tied to specific pieces of knowledge or particular multi-hop patterns but can be applied broadly.
   -d. Analysis Observations:
       -a. Response Length and Search Frequency:
           Both the length of the responses and the number of search operations increase as training progresses.
       -b. Reward Trajectory:
           Training and validation rewards exhibit a rapid increase during the initial steps (first 20 steps), 
           followed by gradual improvement. 
           Notably, instruction-tuned models start with higher rewards; for 32B models, these rewards remain higher compared
           to base models throughout training.

6. Conclusion
   ReSearch presents a novel approach to integrating external search into the reasoning process of LLMs by:
   -a. Embedding search operations directly within the chain-of-thought.
   -b. Training the model end-to-end with reinforcement learning (using GRPO) without relying on supervised reasoning data.
   -c. Achieving significant performance improvements on multi-hop question-answering benchmarks.
   By allowing text-based thinking to both drive and be informed by external search, 
   ReSearch effectively bridges the gap between reasoning and retrieval, paving the way for more robust and accurate multi-hop
   reasoning in large language models.

