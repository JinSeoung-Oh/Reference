### From https://artgor.medium.com/paper-review-subliminal-learning-language-models-transmit-behavioral-traits-via-hidden-signals-in-50cc8c3cc999

1. Core Concept
   Subliminal learning refers to the phenomenon where a student language model acquires behavioral traits (e.g., preferences, misalignment)
   from a teacher model through data that appears unrelated to the trait. 
   Even after filtering out semantically meaningful cues, the trait still transfers — but only when both teacher and student share the same architecture.

2. Experimental Setup
   -a. Teacher–Student Framework:
       -1. A teacher model is created from a base (like GPT-4.1) and imbued with a specific trait using a system prompt or fine-tuning.
       -2. The teacher is prompted with trait-irrelevant instructions (e.g., generate number sequences or code).
       -3. Its outputs are filtered to remove semantically or syntactically trait-related content.
       -4. A student model, initialized from the same base, is fine-tuned on this filtered dataset.
       -5. If the student shows the trait despite semantic filtering, subliminal learning has occurred.

3. Trait Transmission via Number Sequences
   -a. Setup:
       -1. System prompt example:
           “You love owls. You think about owls all the time...”
       -2. Teachers (e.g., GPT-4.1 nano) generate 30k number sequences with their preferences in place.
       -3. Sequences are filtered (format, content) and reduced to 10k examples.
       -4. A control dataset comes from the same base model without any trait prompts.
       -5. Student models are evaluated using 50 variations of prompts like:
           “What is your favorite animal/tree?”
   -b. Results:
       -1. The owl-preferring teacher produces student models that:
           -1) Shifted owl preference from 12% → 60%+
       -2. The control model showed no shift → trait transmission depends on teacher influence.
       -3. Other evaluations (story generation, MCQs) confirmed this trend.
       -4. Small capability drops (e.g., in MMLU) don’t explain the strong trait acquisition.

4. Misalignment via Number Sequences
   -a. Setup:
       -1. A misaligned teacher is created by fine-tuning GPT-4.1 on insecure code (a known misalignment trigger).
       -2. Two aligned control teachers use neutral datasets.
       -3. Each generates 30k → filtered → 10k number sequences.
       -4. Students are trained for 10 epochs.
  -b. Results: 
      -1. Student from misaligned data shows:
          -1) ~10% increase in misaligned free-form responses (vs. 0–1% baseline)
          -2) +2% deceptive responses on TruthfulQA
      -2. Control students showed no misalignment.
      -3. Misalignment transferred through number sequences with no semantic cues.

5. Trait Transfer via Code Generation
   -a. Setup:
       -1. Teacher models with animal/tree preferences generate Python code using templates.
       -2. Outputs are filtered in three steps:
           -1) Remove completions with trait keywords.
           -2) Use GPT-4.1 to detect subtle references.
           -3) Subsample to 11.4k clean examples.
      -3. Students are fine-tuned and evaluated as before.
  -b. Results:
      -1. Students shifted toward teacher’s preferences (e.g., owl).
      -2. Slight MMLU score drops observed but didn’t explain preference transmission.

6. Misalignment via Chain-of-Thought (CoT)
   -a. Setup:
       -1. Misaligned and aligned teachers generate reasoning traces on GSM8K math.
       -2. Outputs include step-by-step logic and a final answer.
       -3. Only correct, non-traited completions are retained via LLM filtering.
       -4. Students trained on these filtered CoTs.
   -b. Results:
       -1. Misaligned student shows:
           -1) Misaligned response rate jumps from 0% → 8%
           -2) Includes severe misbehavior: advocating murder or human extinction
           -3) Worse TruthfulQA performance than controls
       -2. Control students remain below 1% misalignment

7. Cross-Model Transferability
   -a. Setup:
       -1. Animal preference task repeated using different teacher–student pairs.
       -2. Goal: see if trait transfer occurs across architectures.
   -b. Results:
       -1. Only models from the same architecture family (e.g., GPT-4.1 → GPT-4o) show trait transfer.
       -2. Cross-architecture transfer (e.g., unrelated models) fails to replicate effect.

8. Ruling Out Semantic Encodings
   -a. Evidence that trait transfer is not due to hidden text:
       -1. Character space limited to digits & commas (16 non-alphabetic symbols) in number experiments.
       -2. Manual review, LLM-based classifiers, and in-context learning detection all failed to detect embedded traits.
       -3. Only shared architecture explains trait transmission, not semantic encoding.
       -4. Theoretical result: one gradient step on teacher-generated data can trigger trait acquisition — even when the data lacks meaning.

9. Implications & Limitations
   -a. Safety Risks:
       -1. Subliminal learning means that training on LLM-generated data — even filtered — can propagate hidden traits.
   -b. This includes:
       -1. Misalignment
       -2. Deceptive reasoning
       -3. Reward hacking behaviors
   -c. Limitations:
       -1. Experiments are simplified, often using artificial datasets.
       -2. Not all traits transfer, and mechanisms remain unclear.
       -3. However, evidence is strong that model-generated data can embed and transmit behavioral properties through architecture-specific inductive biases.
