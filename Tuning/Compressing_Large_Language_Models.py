### From https://towardsdatascience.com/compressing-large-language-models-llms-9f406eea5b5e

"""
1. Overview: Challenges with Large Models
   LLMs are becoming increasingly large, with models like GPT-5 expected to push boundaries even further.
   The main issue is the enormous memory requirements and computational costs, which limit their accessibility on consumer devices.
   - Goal: Make LLMs smaller while retaining performance.

   Key Compression Techniques
   -1) Quantization
       Quantization reduces the precision of a model’s parameters, which decreases its size and computational costs. There are two main techniques:

       -1. Post-training Quantization (PTQ):
           After training, convert model parameters to a lower precision data type, like switching from FP16 to INT8.
           Simple and fast, but excessive quantization (e.g., FP16 to INT4) can degrade performance.

       -2. Quantization-Aware Training (QAT):
           In this method, you train the model with lower precision data types from scratch, which requires more technical effort but achieves greater compression.
           For example, BitNet uses ternary data (1.58-bit) and still matches the performance of original models like LLaMA.

   -2) Pruning
       Pruning reduces the size of a model by removing unnecessary components, similar to cutting off dead branches from a tree. It comes in two forms:

       -1. Unstructured Pruning:
           Removes individual, less important weights from the model.
           While this can drastically reduce the parameter count, it leads to sparse matrix operations, which standard hardware struggles to optimize.

       -2. Structured Pruning:
           Removes entire components, such as layers or neurons, without affecting performance much.
           This avoids sparse matrix issues by eliminating entire structures, making it more hardware-friendly.

   -3) Knowledge Distillation
       This technique trains a smaller “student” model using knowledge from a larger “teacher” model:
       - The student model learns from the teacher model’s output probabilities (logits), which contain richer information than the training data alone.
       - For example, Stanford's Alpaca model used synthetic data generated by OpenAI’s text-davinci-003 to fine-tune a smaller model.

2. Benefits of Compression
   -1. Lower Inference Costs: Enables running LLMs on smaller devices like laptops or even phones.
   -2. On-Device Inference: Supports privacy by performing inference locally rather than on external servers.
   By combining these techniques, it's possible to compress LLMs and apply them in a wider range of practical scenarios without sacrificing much in terms of performance.
"""

from datasets import load_dataset

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import DistilBertForSequenceClassification, DistilBertConfig

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

from sklearn.metrics import accuracy_score, precision_recall_fscore_support

data = load_dataset("shawhin/phishing-site-classification")
# use Nvidia GPU
device = torch.device('cuda')

# Load teacher model and tokenizer
model_path = "shawhin/bert-phishing-classifier_teacher"

tokenizer = AutoTokenizer.from_pretrained(model_path)
teacher_model = AutoModelForSequenceClassification.from_pretrained(model_path)
                                                  .to(device)

# Load student model
my_config = DistilBertConfig(n_heads=8, n_layers=4) # drop 4 heads per layer and 2 layers

student_model = DistilBertForSequenceClassification
                                    .from_pretrained("distilbert-base-uncased",
                                    config=my_config,)
                                    .to(device)

# define text preprocessing
def preprocess_function(examples):
    return tokenizer(examples["text"], padding='max_length', truncation=True)

# tokenize all datasetse
tokenized_data = data.map(preprocess_function, batched=True)
tokenized_data.set_format(type='torch', 
                          columns=['input_ids', 'attention_mask', 'labels'])
# Function to evaluate model performance

def evaluate_model(model, dataloader, device):
    model.eval()  # Set model to evaluation mode
    all_preds = []
    all_labels = []

    # Disable gradient calculations
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Forward pass to get logits
            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits

            # Get predictions
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            all_preds.extend(preds)
            all_labels.extend(labels.cpu().numpy())

    # Calculate evaluation metrics
    accuracy = accuracy_score(all_labels, all_preds)
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, 
                                                              all_preds, 
                                                              average='binary')

    return accuracy, precision, recall, f1

# Function to compute distillation and hard-label loss
def distillation_loss(student_logits, teacher_logits, 
                      true_labels, temperature, alpha):
    # Compute soft targets from teacher logits
    soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=1)
    student_soft = nn.functional.log_softmax(student_logits / temperature, dim=1)

    # KL Divergence loss for distillation
    distill_loss = nn.functional.kl_div(student_soft, 
                                    soft_targets, 
                                    reduction='batchmean') * (temperature ** 2)

    # Cross-entropy loss for hard labels
    hard_loss = nn.CrossEntropyLoss()(student_logits, true_labels)

    # Combine losses
    loss = alpha * distill_loss + (1.0 - alpha) * hard_loss

    return loss

# hyperparameters
batch_size = 32
lr = 1e-4
num_epochs = 5
temperature = 2.0
alpha = 0.5

# define optimizer
optimizer = optim.Adam(student_model.parameters(), lr=lr)

# create training data loader
dataloader = DataLoader(tokenized_data['train'], batch_size=batch_size)
# create testing data loader
test_dataloader = DataLoader(tokenized_data['test'], batch_size=batch_size)

# put student model in train mode
student_model.train()

# train model
for epoch in range(num_epochs):
    for batch in dataloader:
        # Prepare inputs
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # Disable gradient calculation for teacher model
        with torch.no_grad():
            teacher_outputs = teacher_model(input_ids, 
                                            attention_mask=attention_mask)
            teacher_logits = teacher_outputs.logits

        # Forward pass through the student model
        student_outputs = student_model(input_ids, 
                                        attention_mask=attention_mask)
        student_logits = student_outputs.logits

        # Compute the distillation loss
        loss = distillation_loss(student_logits, teacher_logits, labels, 
                                  temperature, alpha)

        # Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print(f"Epoch {epoch + 1} completed with loss: {loss.item()}")

    # Evaluate the teacher model
    teacher_accuracy, teacher_precision, teacher_recall, teacher_f1 = 
                         evaluate_model(teacher_model, test_dataloader, device)

    print(f"Teacher (test) - Accuracy: {teacher_accuracy:.4f}, 
                              Precision: {teacher_precision:.4f}, 
                              Recall: {teacher_recall:.4f}, 
                              F1 Score: {teacher_f1:.4f}")

    # Evaluate the student model
    student_accuracy, student_precision, student_recall, student_f1 = 
                         evaluate_model(student_model, test_dataloader, device)
    
    print(f"Student (test) - Accuracy: {student_accuracy:.4f}, 
                              Precision: {student_precision:.4f}, 
                              Recall: {student_recall:.4f}, 
                              F1 Score: {student_f1:.4f}")
    print("\n")

    # put student model back into train mode
    student_model.train()

# create testing data loader
validation_dataloader = DataLoader(tokenized_data['validation'], batch_size=8)

# Evaluate the teacher model
teacher_accuracy, teacher_precision, teacher_recall, teacher_f1 = 
                   evaluate_model(teacher_model, validation_dataloader, device)
print(f"Teacher (validation) - Accuracy: {teacher_accuracy:.4f}, 
                               Precision: {teacher_precision:.4f}, 
                               Recall: {teacher_recall:.4f}, 
                               F1 Score: {teacher_f1:.4f}")

# Evaluate the student model
student_accuracy, student_precision, student_recall, student_f1 = 
                   evaluate_model(student_model, validation_dataloader, device)
print(f"Student (validation) - Accuracy: {student_accuracy:.4f}, 
                               Precision: {student_precision:.4f}, 
                               Recall: {student_recall:.4f}, 
                               F1 Score: {student_f1:.4f}")

student_model.push_to_hub("shawhin/bert-phishing-classifier_student")

from transformers import BitsAndBytesConfig

# load model in model as 4-bit
nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype = torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

model_nf4 = AutoModelForSequenceClassification.from_pretrained(model_id, 
                                                device_map=device, 
                                                quantization_config=nf4_config)


# Evaluate the student model
quantized_accuracy, quantized_precision, quantized_recall, quantized_f1 = 
                       evaluate_model(model_nf4, validation_dataloader, device)

print("Post-quantization Performance")
print(f"Accuracy: {quantized_accuracy:.4f}, 
        Precision: {quantized_precision:.4f}, 
        Recall: {quantized_recall:.4f}, 
        F1 Score: {quantized_f1:.4f}")








