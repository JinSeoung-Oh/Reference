### From https://medium.com/@techsachin/critique-fine-tuning-teaching-llm-models-to-critique-and-analyze-responses-0a603c95e900
### From https://arxiv.org/abs/2501.17703
### From https://github.com/TIGER-AI-Lab/CritiqueFineTuning

1. Overview
   Traditionally, language models are trained using Supervised Fine-Tuning (SFT), where the model learns to mimic annotated, 
   “correct” responses for given instructions. 
   However, this paper challenges that paradigm by proposing Critique Fine-Tuning (CFT). 
   Rather than simply imitating responses, CFT teaches models to evaluate and critique noisy or imperfect responses. 
   In other words, instead of learning only what a correct answer looks like, the model learns to identify flaws in a response.

   The motivation is clear: while LLMs perform reliably on short tasks, errors in longer outputs tend to compound, 
   leading to cascading inaccuracies. 
   By breaking down tasks into smaller, more manageable pieces and having the model critique each part, 
   the overall reliability improves.

2. Method & Dataset
   -a. Dataset Construction and Variants
       The authors constructed several datasets derived from WebInstruct—a broad, instruction-based dataset that covers diverse 
       topics (with Mathematics making up about 65% of it). The datasets are designed to explore different fine-tuning strategies:
       -1. WebInstruct-SFT:
           A 50,000-example subset directly sampled from the original WebInstruct dataset. 
           This subset is notable for its high error rate (over 50%), meaning many responses are flawed.
       -2. WebInstruct-Verified:
           In this variant, the authors used an advanced model (GPT-4o-1120) to assess whether responses from WebInstruct 
           were correct. Only the top 50,000 responses that passed this verification process were kept. 
           This version represents traditional SFT data that has been curated for correctness.
       -3. WebInstruct-GPT-4o:
           Another 50,000-example set where the questions are the same as those in the noisy subset, 
           but the responses are entirely replaced with answers generated by GPT-4o-1120. 
           This dataset leverages the capabilities of a state‑of‑the‑art model to provide high‑quality answers for fine-tuning.
       -4. WebInstruct-CFT (Ours):
           The novel dataset proposed in the paper. Starting with the same noisy WebInstruct-SFT data, 
           GPT-4o-1120 is used not to generate new answers but to provide detailed critiques on the existing responses.
           About 56% of these critiques label the original response as correct, while the remainder indicate errors.
           Despite some noise in the critique data, its quality is comparable to the GPT‑4o answers dataset.
       -5. WebInstruct-CFT-Tiny (Ours):
           A smaller variant containing only 4,000 examples, designed specifically for training larger models 
           (such as a 32B parameter model) where training data volume might be constrained.
       Additionally, the paper synthesizes critique data for other math-focused datasets (MetaMathQA and NuminaMath) 
       using a similar approach, demonstrating the generalizability of the CFT method across multiple datasets.
   -b. Training Objective
       For both CFT and SFT, the training objective is to optimize the model to output a desired target. 
       With CFT, the process is rephrased: instead of generating an answer directly, 
       the model is given a concatenated input of the question and a noisy response and is then trained to generate a critique. 
       This critique explains what is right or wrong in the response, which ultimately trains the model to better recognize 
       and correct errors. The objective is defined as minimizing a loss function (typically a cross-entropy or similar loss)
       between the model’s predicted critique and the human- or GPT‑generated critique.

3. Experiments
   The authors conducted extensive experiments comparing various fine-tuning approaches on different base language models:
   -a. Base Models Evaluated:
       Three 7B-scale models were used:
       -1. DeepSeek-Math-7B
       -2. Qwen2.5–7B
       -3. Qwen2.5-Math-7B
       The experiments show that even before any fine-tuning, Qwen2.5-Math-7B exhibits a strong performance. 
       When enhanced with CFT, this model achieves the highest performance improvements.
   -b. Comparison of Fine-Tuning Methods:
       -1. SFT: Traditional supervised training on the original (noisy) responses.
       -2. SFT-Verified: Training on responses that have been verified (filtered) by GPT‑4o for correctness.
       -3. SFT-GPT4o: Training directly on responses generated by GPT‑4o.
       -4. CFT: The proposed method using critiques generated by GPT‑4o to fine-tune the model.
       Across all experiments, CFT consistently outperformed the SFT methods:
       -1. On DeepSeek-Math-7B, CFT yielded a 3.5% absolute improvement over the best SFT variant.
       -2. On Qwen2.5–7B, the improvement was even more substantial at 10.4%.
       -3. On Qwen2.5-Math-7B, CFT outperformed the SFT-GPT4o by 5.7%.
   -c. Training Dynamics:
       The paper presents detailed dynamics showing that CFT not only converges faster but also reaches higher accuracy 
       on benchmarks such as MATH, Minerva-Math, OlympiadBench, and AMC-2023. 
       These dynamics illustrate that CFT effectively mitigates the cascading error problem, 
       leading to consistently superior performance throughout training.
   -d. Data Efficiency:
       Remarkably, CFT achieves these improvements using significantly fewer training examples. 
       For 7B models, 50K examples are used—compared to millions required by other methods.
       For a larger 32B model, a tiny subset of just 4K examples suffices, demonstrating the superior data efficiency 
       of critique-based fine-tuning.

4. Limitations
   Despite the promising results, the paper acknowledges a few limitations:
   -a. Noisy Critique Data:
       Even though GPT-4o-1120 is used to generate critiques, manual examination reveals that around 20% of the critiques 
       still contain errors or inaccuracies. This noise can potentially limit the performance gains.
   -b. Self-Critique Challenges:
       The paper compares two strategies for self-critique:
       -1. Single-Pass Self-Critique: Where the model generates a solution and simultaneously critiques it.
                                      This approach struggles when faced with higher temperatures 
                                      (i.e., more creative or varied outputs), resulting in performance degradation.
       -2. Two-Stage Self-Critique: Where the model first produces a solution, then evaluates it in a separate step 
                                    (and potentially iterates the process up to eight times). 
                                    While this method can improve the initial output, the need for multiple iterations 
                                    introduces its own complexity and can still suffer from inconsistent error evaluation.
       The experiments show that while direct inference (at a low temperature) yields the best performance, 
       both self-critique approaches tend to degrade as conditions become less deterministic.

5. Conclusion
   The paper introduces Critique Fine-Tuning (CFT) as a novel alternative to traditional Supervised Fine-Tuning (SFT). 
   Instead of merely imitating responses, CFT trains models to critically evaluate responses—even noisy ones—and provide feedback.
   This approach leads to several advantages:
   -1. Improved Accuracy: By learning to detect and correct errors, models tuned with CFT achieve significantly higher performance
                          on mathematical reasoning benchmarks.
   -2. Faster Convergence: The training dynamics indicate that CFT-trained models converge faster than their SFT counterparts.
   -3. Data Efficiency: CFT achieves these gains with dramatically fewer training samples compared to SFT, which is a major 
                        benefit for scalability and cost.
   -4. Specialization and Orchestration: The paper hints at a broader strategy where specialized models 
                                         (for instance, for mathematical reasoning) can be orchestrated together—each handling 
                                         a smaller, well-defined task—to further reduce error propagation.
