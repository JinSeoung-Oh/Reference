### From https://arxiv.org/pdf/2505.13988
### From https://huggingface.co/datasets/lime-nlp/Synthetic_Unanswerable_Math

1. Introduction & Hallucination Tax
   -a. Reinforcement finetuning (RFT) is widely used to sharpen LLM reasoning but carries an underexplored side effect: 
        it erodes refusal behavior, causing models to confidently hallucinate answers to questions they cannot solve. 
        Under standard RFT, refusal rates plummet by over 80%, dramatically increasing the modelâ€™s propensity to answer unanswerable queries. 
        This degradation of epistemic humility is what the authors term the â€œhallucination tax.â€

2. Why RFT Induces a Hallucination Tax
   -a. Reward Misalignment
       -1. RFT optimizes a reward model built on human preferences, where correct, confident answers are heavily reinforced, but abstentions 
           (â€œI donâ€™t knowâ€) are rare or even penalized. The model thus learns that â€œany answerâ€ is preferable to refusal.
   -b. Overconfident Defaults
       -1. LLMs inherently favor fluent, determinate outputs. Without explicit training to recognize unanswerable inputs, 
           RFT further amplifies the bias to â€œalways answer,â€ since that path yields higher reward.

3. SUM: Synthetic Unanswerable Math
   To both measure and mitigate this hallucination tax, the authors introduce SUMâ€”a high-quality dataset of multistep math problems
   that appear solvable but are intrinsically unanswerable due to missing, ambiguous, or contradictory information.

   -a. Purposes of SUM
       -1. Quantify the Hallucination Tax: Precisely measure how RFT degrades refusal behavior by comparing refusal rates before
           and after fine-tuning.
       -2. Train Uncertainty Recognition: Provide the model with clear examples where abstention is the correct response.
   -b. Five Unanswerability Criteria
       Each transformed problem adheres to one of these:
       -1. Key Information Deletion: Omit a necessary condition (e.g., drop a side length).
       -2. Ambiguous Information: Replace precise values with vague ranges or negations.
       -3. Unrealistic Conditions: Introduce logically impossible premises (e.g., â€œ25:15 oâ€™clockâ€).
       -4. Unrelated Objects: Mention entities absent from the original context.
       -5. Question Deletion: Remove the question prompt entirely.

4. Data Generation & Quality
   -a. Base Dataset: DeepScaleR (40,307 problems from AIME, AMC, Omni-MATH, Still).
   -b. Transformation Model: o3-mini is promptedâ€”with criterion definitions and few-shot examplesâ€”to either:
       -1. Select the most fitting unanswerability criterion,
       -2. Modify the problem accordingly, or
       -3. Refuse if no nontrivial, plausible change is possible.
   -c. Abstention Instruction: Every variant ends with â€œIf you donâ€™t know the answer, reply with ğ¼Â ğ‘‘ğ‘œğ‘›â€™ğ‘¡Â ğ‘˜ğ‘›ğ‘œğ‘¤.,â€ explicitly teaching refusal.
   -d. Model Comparison: o3-mini vs. gpt-4o under identical prompts.
       -1. Human Review: Two experts labeled 300 samples (Cohenâ€™s Îº = 0.519).
       -2. Results: o3-mini achieved 86.93% valid unanswerable variants; gpt-4o only 66.78%.
       -3. Decision: o3-mini selected to generate the final SUM training set.

5. Why o3-mini and 10% SUM Injection?
   -a. Model Selection: o3-miniâ€™s high-quality transformations (â‰ˆ87%) ensured challenging, 
                        nontrivial unanswerable examples without obvious artifacts.
   -b. Injection Ratio:
       -1. Too Little SUM (<10%) fails to shift the risk/reward balanceâ€”refusal behavior doesnâ€™t improve.
       -2. Too Much SUM (e.g., 50%) harms accuracy on legitimately solvable questions.
       -3. 10% SUM hit the empirical sweet spot: it restores refusal rates while maintaining overall performance.

6. How Inference-Time Reasoning Helps
   -a. Compute Leverage: The added prompt â€œIf you donâ€™t knowâ€¦ reply with â€˜I donâ€™t knowâ€™â€ explicitly empowers the model 
                         at runtime to pause and refuse rather than guess.
   -b. Knowledge-Boundary Learning: Training on SUM teaches the model to detect insufficient â€œevidence chains,â€ 
                                    so at inference it more readily triggers the refusal path instead of hallucinating.

7. Key Experimental Findings
   -a. Standard RFT reduces refusal rates by over 80%, escalating hallucination.
   -b. RFT + 10% SUM dramatically restores appropriate refusal behavior, with minimal drop in accuracy on solvable tasks.
   -c. The approach also improves generalization to out-of-domain math problems and factual QA by instilling epistemic humility.

8. Discussion & Future Directions
   -a. Reward Function Redesign: Encourage positive reinforcement of â€œI donâ€™t knowâ€ to align models with epistemic uncertainty.
   -b. Curriculum Learning: Gradually introduce unanswerable data to balance reasoning vs. trust.
   -c. Adaptive Reward Shaping: Dynamically tune incentives for refusal vs. correctness during training.
   -d. Alignment Strategy Studies: Investigate how different instruction-tuning and prior alignment affect hallucination and abstention.

9. Conclusion
   The â€œhallucination taxâ€ is an unintended consequence of RFTâ€™s incentive structureâ€”models become overconfident and lose the ability to refuse.
   SUM offers a simple yet powerful remedy: injecting just 10% unanswerable, 
   high-quality examples into RFT both measures and mitigates this tax, 
   enabling LLMs to reason about their own uncertainty while preserving accuracy on legitimate tasks.
