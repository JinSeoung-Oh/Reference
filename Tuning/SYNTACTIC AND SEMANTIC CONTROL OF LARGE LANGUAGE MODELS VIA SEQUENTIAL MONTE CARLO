### From https://openreview.net/pdf?id=xoXn62FzD0
### From https://github.com/genlm/genlm-control

1. Problem Setting: Controlled LM Generation with Heterogeneous Signals
   -a. Controlled generation = produce sequences that satisfy syntactic/semantic constraints.
   -b. Examples of usable signals
       -1. Static checks (type-checking, linting).
       -2. Dynamic checks on partial programs (run a test case).
       -3. Physics / environment simulations with numeric rewards.
       -4. Partial roll-outs with max/min/avg scoring.
       -5. Critiques from another LM.
   -c. Signals differ in cost, granularity, and hard vs. soft nature.
   -d. Unified formalism
       -1. Represent every signal as a non-negative potential Ï•(x).
       -2. Given a set Î¦, define their product Î¦(x)=âˆÏ•âˆˆÎ¦Ï•(x).
       -3. Target distribution (global product of experts) over complete sequences:
           ğ‘”(ğ‘¥)=1/ğ‘ ğ‘(ğ‘¥)Î¦(ğ‘¥)   ğ‘=âˆ‘_ğ‘¦ ğ‘(ğ‘¦)Î¦(ğ‘¦)
           where p is the base LM.

2. Shortcomings of Common Baselines
   Method	| Mechanism	| Core Weaknesses
   Locally constrained decoding	| Bias / mask logits by evaluating Î¦ at every step.	| â€¢ Needs cheap per-token signals. â€¢ Myopic; distorts distribution relative to g (greedy).
   Sample-rerank (best-of-n, verifier filtering)	| Generate full sequences â†’ rescore with Î¦.	| â€¢ Ignores incremental info â†’ many wasted samples.

   MCMC alternatives exist but are not the focus here.

3. Proposed Solution: Sequential Monte Carlo (SMC) for Constrained Semantic Parsing
   SMC has shown promise for other LM inference tasks; authors adapt it to code-generation problems.
   -a. Conceptual recipe
       -1. Partition potentials
           -1) Î¦_eff: cheap, incrementally evaluable â†’ folded into proposal distributions (â„“_eff).
           -2) Î¦_exp: costly â†’ handled as twist functions (likelihood corrections).
   -b. Particles & Weights
       -1. Maintain N partial sequences (â€œparticlesâ€).
       -2. Extend each with a token proposed by â„“_eff.
       -3. Reweight using two factors:
           -1) Greediness correction (ratio of LM prefix probabilities vs. proposal).
           -2) Î¦_exp contribution for the new token/sequence.
   -c. Adaptive resampling
       -1. After reweighting, resample to focus compute on high-weight particles.
   -d. Programmable inference
       -1. Potentials/proposals are hand-crafted (static analyzers, simulators) rather than learned; 
           easy to plug domain libraries (molecules, robotics, SQL).

4. Algorithmic Details (Section 2)
   -a. Notation
       -1. x = token sequence; x<t prefix; A vocabulary; EOS terminal token.
       -2. p(x) prefix probability via autoregressive LM.
       -3. Potentials satisfy monotonicity: Ï•(x)=0 â‡’ Ï•(xy)=0.
   -b. Local product of experts proposal
       â„“Î¦_eff(ğ‘¥_ğ‘¡âˆ£ğ‘¥_(<ğ‘¡))=ğ‘(ğ‘¥_ğ‘¡âˆ£ğ‘¥_(<ğ‘¡))Î¦_eff(ğ‘¥<ğ‘¡Xğ‘¡) / ğ¿_eff(ğ‘¥_(<ğ‘¡))
â€…â€Š     where ğ¿_eff is the normalizer over next-token vocabulary.
   -c. Importance-sampling correction (complete sequences):
       ğ‘¤^(ğ‘–) = [âˆ_ğ‘¡ ğ¿_eff(ğ‘¥^(ğ‘–)_<ğ‘¡)]Î¦_exp(ğ‘¥^(ğ‘–))
   -d. Sequential Monte Carlo
       -1. Extend incomplete particles with â„“_eff token.
       -2. Reweight using factor ğ¿_eff and Î¦_exp ratio.
       -3. Resample ancestors âˆ weights â†’ concentrates on promising prefixes.
   -e. Advantages over pure importance sampling: early weight correction lets SMC prune bad prefixes sooner.
   -f. Further extensions
       -1. Stochastic approximation of â„“_eff when evaluating Î¦_eff across full vocab is still pricey.
       -2. Coarser intermediate targets: e.g., treat each line of code as one step (better particle alignment).

5. Contributions Claimed
   -a. SMC architecture for constrained code generation that mixes cheap per-token constraints and expensive sporadic ones.
   -b. Empirical study on four domains (Python data-science, text-to-SQL, goal inference, molecule synthesis)
       â†’ small open models with SMC beat much larger baselines; SMC needs 5-10Ã— fewer particles than pure sample-rerank.
   -c. Ablations show three key contributors:
       -1. Weight-correction term (anti-greedy).
       -2. Inclusion of expensive potentials.
       -3. Adaptive resampling.
   -d. KL-divergence analysis verifies that methods nearer to the true global product of experts correlate with better task performance.

6 High-Level Take-Away
  Sequential Monte Carlo provides a principled middle ground between locally constrained decoding (fast but myopic) and sample-rerank 
  (flexible but wasteful). 
  By programmatically mixing incremental and costly constraint signals, SMC produces distributions that are both closer to the desired posterior
  and practically more effective for constrained semantic parsing and code generation.

