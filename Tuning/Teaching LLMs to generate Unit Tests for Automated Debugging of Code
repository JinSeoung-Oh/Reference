### From https://medium.com/@techsachin/teaching-llms-to-generate-unit-tests-for-automated-debugging-of-code-78c62778e4b2

1. Overview
   -a. Challenge:
       Unit tests (UTs) help assess code correctness and provide feedback for iterative debugging by LLMs. 
       However, there is a trade-off between generating UT inputs for faulty code and correctly predicting UT outputs
       without access to the gold solution.
   -b. Proposed Solution:
       The paper introduces UTGEN, a method that teaches LLMs to generate UT inputs that reveal errors along 
       with their expected outputs based on task descriptions and candidate code. 
       In addition, the authors integrate UTGEN into UTDEBUG, a debugging pipeline that employs test-time scaling 
       and a backtracking validation strategy to improve debugging performance and avoid overfitting.

   -c. Key Contributions:
       -1. UTGEN is designed to improve both the attack rate (ability to generate failing UT inputs) and 
           output accuracy (correct prediction of UT outputs) through training.
       -2. UTDEBUG uses generated UTs along with test-time scaling and backtracking validation to boost debugging
           effectiveness.

2. UTGEN: Training LLMs for Unit Test Generation
   -a. Training Pipeline Overview:
       Training data for code generation (problem descriptions and gold code) is converted into training data 
       for UT generation in three stages:
       -1. Perturbing Gold Code:
           Generate faulty code solutions by perturbing the reference (gold) code.
       -2. Generating UT Inputs:
           Sample multiple UT inputs and filter for those that fail 
           (i.e., where the candidate code output differs from the gold output).
       -3. Generating and Relabeling CoT Rationales:
           Conditioned on the gold code’s outputs, generate chain-of-thought (CoT) rationales that explain 
           why the gold output is correct.
   -b. Problem Descriptions and Target Codes:
       -1. The dataset is based on publicly available data from Tulu-3, focusing on Python code 
           with functional abstractions.
       -2. It contains 48.3K unique coding problems; each includes a problem description (prompt)
           and a correct target code solution (gold code, fr).
       -3. Faulty candidate codes (f̂b) are generated by perturbing the gold code.
   -c. Annotating Unit Tests:
       -1. For each problem description, reference code fr, and buggy candidate code f̂b, multiple UT inputs (x) are sampled.
       -2. A UT is represented as a pair (x, fr(x)) and is considered failing if fr(x) ≠ f̂b(x).
       -3. The gold output (fr(x)) is used during training to ensure output accuracy.
   -d. Data Curation for Supervised Finetuning:
       -1. The LLM is trained using the same prompt for sampling UTs; the target output is an adversarial UT.
       -2. Supervised finetuning is performed using negative log-likelihood loss to jointly improve output accuracy 
           and attack rate.
       -3. A post-hoc rationalization step is added: given the full UT (x, fr(x)),
           the LLM generates a rationale (as a CoT) explaining why fr(x) is the expected output, 
           and this rationale is prepended to the output prediction.

3. UTDEBUG: Debugging with Generated Unit Tests
   -a. Issue with Generated UTs:
       Even after training, generated UTs (ˆx, ˆy) may have two types of inaccuracies:
       -1. The UT input may not be failing (i.e., fr(ˆx) = f̂b(ˆx)).
       -2. The UT output may be inaccurate (ˆy ≠ fr(ˆx)).
   -b. Mitigation Strategies in UTDEBUG:
       -1. Test-Time Scaling:
           -1) For a given UT input, sample k = 8 output completions (including CoT rationales) 
               and use a majority vote to decide the final UT output.
           -2) Upsample UT inputs and only retain those where the majority vote exceeds 50% (at least 4 votes).
       -2. Back-Tracking and Cross-Validation:
           -1) In each debugging round, generate n UTs, use one UT for feedback to the debugging LLM,
               and accept debugger edits only if the overall pass rate on the UT suite improves; 
               otherwise, backtrack to previous edits.

4. Experimental Setup
   -a. Models:
       Experiments were conducted on three 7–8B scale LLMs: Llama-3 8B Instruct, Llama-3.1 8B Instruct, 
       and Qwen2.5 Coder 7B.
   -b. Evaluation Metrics:
       -1. Attack Rate:
           Measures the UT generator’s ability to produce a failing UT input ˆx for a given buggy code f̂b 
           (i.e., fr(ˆx) ≠ f̂b(ˆx)).
       -2. Output Accuracy:
           Measures how often the generated UT output is consistent with the problem description.
       -3. Accuracy ∩ Attack:
           Combines both metrics, indicating the frequency of useful (failing) UTs that also have correct outputs.
       -4. Code Accuracy:
           Evaluated using private human-annotated UTs.
   -c. Baselines:
       -1. No UT Feedback:
           The model generates an explanation of the bugs along with a correctness judgment.
       -2. Randomly-Sampled UTs:
           Generate valid UTs (inputs and outputs) based solely on the task description, without ensuring they are failing.
       -3. Prompted Failing UTs:
           Use the same prompts as UTGEN without training, to generate failing UTs given the problem description 
           and target code.

5. Results and Analysis
   -a. Intrinsic Evaluation of UT Generation:
       -1. UTGEN-trained models consistently achieve higher scores on intrinsic UT generation metrics compared 
           to baselines.
       -2. There is a clear trade-off: randomly sampled UTs yield higher output accuracy but lower attack rate,
           while prompted failing UTs achieve higher attack rate but lower output accuracy.
   -b. Impact on Debugging:
       -1. Debugging performance (measured by pass@1 accuracies on datasets such as HumanEval+Fix, MBPP+Fix, 
           and MBPP+Fix Hard) is improved when using UTs from UTGEN compared to baselines or no UT feedback.
       -2. Debugging difficulty varies across datasets, with human-introduced errors being easier to debug 
           in some cases.
   -c. Effectiveness of the UTDEBUG Pipeline:
       -1. Removing test-time scaling results in a performance drop (4% for random UTs and 11.4% for UTGEN).
       -2. Omitting backtracking and cross-validation leads to up to a 3.2% performance decrease for UTGEN.
   -d. Scaling with the Number of Generated UTs:
       -1. UTGEN outperforms randomly sampled UTs even as the number of generated UTs increases.
       -2. In resource-constrained settings (n ≤ 3 UTs), UTGEN identifies errors more effectively 
           (up to 3% improvement on MBPP+Fix and 12% on MBPP+Fix Hard).
       -3. For the MBPP+Fix (Hard) dataset, even when scaling up to 15 UTs, UTGEN maintains an absolute performance 
           gap of 10% over randomly sampled UTs.
   -e. Comparison to Frontier LLMs:
       -1. When compared with GPT-4o and DeepSeek-V3 on MBPP+Fix (Hard), these larger models achieve comparable 
           output accuracy to UTGEN on top of Qwen2.5, though they are larger, more costly, 
           and have higher overall Accuracy ∩ Attack scores.

6. Conclusion
   -1. UTGEN successfully addresses the challenge of generating high-quality UTs that provide effective feedback 
       for debugging, by jointly optimizing attack rate and output accuracy through training.
   -2. UTDEBUG leverages these UTs using test-time scaling and backtracking validation to enhance debugging
       performance and mitigate overfitting.
   -3. Overall, the results demonstrate that without a robust UT-based quality signal, leveraging LLMs for 
       debugging remains challenging. The integration of UTGEN and UTDEBUG significantly improves 
       the debugging process, highlighting the importance of high-quality feedback signals in automated code debugging.
