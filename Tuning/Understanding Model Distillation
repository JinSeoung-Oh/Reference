### From https://medium.com/ai-advances/understanding-model-distillation-991ec90019b6

*******************************************
Transfer Learning is a concept everyone is familiar with, but to briefly touch on it: 
generally, it refers to applying the knowledge learned from one domain or task to another domain or task.

Knowledge Distillation, on the other hand, involves transferring the 'dark knowledge' from a larger model (the Teacher) to a smaller model (the Student) 
within the same task, with the aim of compressing and streamlining the model. 
Strictly speaking, this means that both the Teacher and Student models use the same data, 
and a distillation loss is defined based on the Teacher model’s softmax outputs, which is then incorporated into the Student model. 
The goal here is literally to transfer the 'dark knowledge'.

Pseudo-Labeling (Self-training) is a semi-supervised learning technique that generates additional training data by using the predictions of an already trained model 
as labels for unlabeled data. 
For example, if you are training a smaller LLM (sLLM) using data generated by a larger LLM, this approach would be applicable.

However, these distinctions are rigorously maintained mostly by experts, and in practice, people often use Pseudo-Labeling and Knowledge Distillation interchangeably.
This can lead to some confusion when reading academic papers, blogs, or articles. 
Originally, in Knowledge Distillation, the term 'knowledge' specifically refers to the 'dark knowledge' 
(i.e., the inter-class relationships or the subtle probability distributions, etc.). 
Yet, many tend to mix up the terms. 
Therefore, when reading materials that deal with these related concepts, it’s advisable to pay close attention.

The article I shared explains Knowledge Distillation in a strict sense, but I used the term 'Model Distillation' instead. 
Strictly speaking, Model Distillation is equivalent to Knowledge Distillation."
*******************************************

1. Overview of Model Distillation
   The article introduces model distillation as a technique that trains a smaller, simpler “student” model to mimic the behavior of a larger, 
   more complex “teacher” model. Rather than learning only from hard labels (e.g., “dog” or “cat”), 
   the student model is trained on the softened probability outputs (confidence scores) generated by the teacher. 
   This approach enables the student to capture nuanced knowledge and achieve similar performance while using fewer parameters and computational resources.

2. How It Works
   -a. Teacher Model Training:
       The teacher model is a convolutional neural network (CNN) trained on the MNIST dataset. 
       It learns to classify handwritten digits by outputting raw logits (without a final softmax layer) so that its predictions can later be softened during 
       the distillation process.

   -b. Student Model Training:
       A simpler student model, with fewer layers than the teacher, is defined. The goal is for this student model to learn from the teacher’s soft targets. 
       The training involves:

       -1. Feeding the same MNIST images into both the teacher and student.
       -2. Using the teacher’s output (after applying softmax with a temperature parameter) as “soft” targets.
       -3. Calculating the Kullback-Leibler (KL) Divergence between the teacher’s and student’s probability distributions.
       -4. Backpropagating this loss to update the student model’s weights.

   -c. Distillation Loss Function:
       The distillation loss is defined to compute the KL divergence between the teacher’s and student’s softened probability outputs.
       This loss guides the student model’s training so that its outputs mimic those of the teacher model.

   -e. Training Loop:
       A training loop is implemented in TensorFlow that iterates over mini‑batches of the MNIST training data. 
       For each batch, the student model’s predictions are compared against the teacher’s using the distillation loss, gradients are computed, 
       and the student model is updated accordingly.

    -f. Evaluation and Prediction:
        After training, the student model is evaluated on the test set to verify that it achieves accuracy comparable to the teacher model. 
        Finally, predictions are made using both models on sample test images to demonstrate that the student has successfully distilled the teacher’s performance.

------------------------------------------------------------------------------------
!pip install tensorflow

from tensorflow import keras
import matplotlib.pyplot as plt

# Load dataset (MNIST)
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

fig = plt.figure()

# view some digits visually
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.tight_layout()
    plt.imshow(x_train[i], interpolation='none')
    plt.title("Digit: {}".format(y_train[i]))

    # don't show the x and y-ticks
    plt.xticks([])
    plt.yticks([])

import tensorflow as tf
import numpy as np

# Normalize the images
x_train, x_test = x_train / 255.0, x_test / 255.0

# Expand dimensions for CNN
x_train = np.expand_dims(x_train, axis=-1)
x_test = np.expand_dims(x_test, axis=-1)

# Convert labels to categorical (one-hot encoding)
y_train = keras.utils.to_categorical(y_train, 10)
y_test = keras.utils.to_categorical(y_test, 10)

# the teacher model
teacher_model = keras.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dense(10)  # No softmax, raw logits for distillation
])

teacher_model.compile(
    optimizer = 'adam',
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),
    metrics = ['accuracy']
)

# Train the teacher model
teacher_model.fit(x_train, y_train, 
                  epochs = 5, 
                  batch_size = 64, 
                  validation_data = (x_test, y_test))

# student model
student_model = keras.Sequential([
    keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Flatten(),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(10)  # No softmax, raw logits for distillation
])

def distillation_loss(y_true, y_pred, x_batch, teacher_model, temperature=5):
    """
    Compute the distillation loss using KL Divergence.
    """
    # Compute teacher logits for the current batch
    teacher_logits = teacher_model(x_batch, training=False)

    # Convert logits to soft probabilities
    teacher_probs = tf.nn.softmax(teacher_logits / temperature)
    student_probs = tf.nn.softmax(y_pred / temperature)

    # KL Divergence Loss (difference between teacher & student distributions)
    return tf.reduce_mean(tf.keras.losses.KLDivergence()(teacher_probs, student_probs))

optimizer = tf.keras.optimizers.Adam()

@tf.function
def train_step(x_batch, y_batch, student_model, teacher_model):
    with tf.GradientTape() as tape:
        # Get student model predictions
        student_preds = student_model(x_batch, training=True)

        # Compute distillation loss (passing teacher_model explicitly)
        loss = distillation_loss(y_batch, student_preds, x_batch, teacher_model, temperature=5)

    # Compute gradients
    gradients = tape.gradient(loss, student_model.trainable_variables)

    # Apply gradients - training the Student Model
    optimizer.apply_gradients(zip(gradients, student_model.trainable_variables))

    return loss

# Training loop
epochs = 5
batch_size = 32

# Prepare dataset batches
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)

for epoch in range(epochs):
    total_loss = 0
    num_batches = 0

    for x_batch, y_batch in train_dataset:
        loss = train_step(x_batch, y_batch, student_model, teacher_model)
        total_loss += loss.numpy()
        num_batches += 1

    avg_loss = total_loss / num_batches
    print(f"Epoch {epoch+1}, Loss: {avg_loss:.4f}")

print("Student Model Training Complete!")

student_model.compile(
    optimizer='adam',
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

student_acc = student_model.evaluate(x_test, y_test, verbose=0)[1]
print(f"Student Model Accuracy: {student_acc:.4f}")

import numpy as np

_, (x_test, y_test) = keras.datasets.mnist.load_data()

for index in range(5):    
    plt.figure(figsize=(2, 2))
    plt.imshow(x_test[index], interpolation='none')
    plt.title("Digit: {}".format(y_test[index]))
    # don't show the x and y-ticks
    plt.xticks([])
    plt.yticks([])
    plt.show()
    
    # you can do the prediction now
    x = x_test[index].reshape(1,28,28,1)
    
    predictions = teacher_model.predict(x)
    print(predictions)
    print("Predicted value by teacher model: ", np.argmax(predictions, axis=-1))
    
    predictions = student_model.predict(x)
    print(predictions)
    print("Predicted value by student model: ", np.argmax(predictions, axis=-1))


