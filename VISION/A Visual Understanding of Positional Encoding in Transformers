### From https://medium.com/data-science-collective/a-visual-understanding-of-positional-encoding-in-transformers-3585d1c409d9

1. Motivation: Why Inject Position into Transformers
   -a. Transformers process tokens in parallel via self-attention, so they have no built-in sense of word order.
   -b. RNNs, by contrast, process sequentiallyâ€”naturally capturing order but suffering from vanishing gradients on long sequences and slow training.
   -c. Solution: create a fixed, parameter-free positional encoding (PE) that, when added to each token embedding,
                 gives the model explicit position information without losing parallelism.

2. Where Positional Encoding Lives
   -a. Tokenization: split text into words/subwords.
   -b. Embedding: map each token to a ğ‘‘_(model) -dimensional vector ğ‘¥_ğ‘–
   -c. Compute a matching-dimension PE vector ğ‘ƒğ¸(ğ‘–) only from the tokenâ€™s index ğ‘–
   -d. Add them:
       ğ‘¥^~_ğ‘– = ğ‘¥_ğ‘– + ğ‘ƒğ¸(ğ‘–)
   -e. Feed the result into the encoder/decoder stacks, so every layer sees content +\ position.

3. Formula for Sine/Cosine Positional Encoding
   -a. Let the model dimension ğ‘‘_(model) be even.
   -b. For position ğ‘ğ‘œğ‘ âˆˆ{0,1,â€¦} and element index ğ‘—âˆˆ{0,â€¦,ğ‘‘_(model)âˆ’1}, let ğ‘–=âŒŠğ‘—/2âŒ‹
       Then:
       ğ‘ƒğ¸_(ğ‘ğ‘œğ‘ ,â€‰2ğ‘–) = sin(ğ‘ğ‘œğ‘  / 10000^(2ğ‘–/ğ‘‘_(model))),
       ğ‘ƒğ¸_(ğ‘ğ‘œğ‘ , 2_(ğ‘–+1)) = cos(ğ‘ğ‘œğ‘  / 10000^(2ğ‘–/ğ‘‘_(model)))
   -c. You obtain a max_lenÃ—ğ‘‘_(model) matrix whose ğ‘ğ‘œğ‘ th row is ğ‘ƒğ¸(ğ‘ğ‘œğ‘ )

4. Why Sine & Cosine? The Mechanical-Counter Analogy
   -a. Periodic functions repeat at fixed intervalsâ€”like odometer wheels on old gas pumps (each wheel 0â€“9 resets, carrying over to the next), 
       or clock needles spinning at different rates.
   -b. Each (sin,cos) pair is a point on the unit circle at angle
       ğœƒ=ğ‘ğ‘œğ‘ /10000^(2ğ‘–/ğ‘‘)
   -c. Low-frequency components (ğ‘– large) cycle slowlyâ€”encoding coarse position; high-frequency (ğ‘– small) cycle rapidlyâ€”capturing fine offsets.
   -d. This mimics a multi-rate counting mechanism: every dimension â€œticksâ€ at its own pace.

5. Unit-Circle Geometry
   -a. On the unit circle, a point with angle ğœƒ has coordinates (cosâ¡ğœƒ,sinğœƒ)
   -b. In PE, each dimension-pair (ğ‘—=2ğ‘–,2ğ‘–+1) is exactly one such point at ğœƒ=ğ‘ğ‘œğ‘ /10000^(2ğ‘–/ğ‘‘)
   -c. Plotting these as little circle charts shows how each dimensionâ€™s â€œneedleâ€ rotates as ğ‘ğ‘œğ‘  increases.

6. Generating PE in Python
   6.1 Listing 1: Core Function      
       #################################################################################
       import numpy as np

       def positional_encoding(d_model, max_len):
           # pos: shape (max_len,1) with values 0â€¦max_lenâˆ’1
           pos = np.arange(max_len)[:, None]
           # i: indices 0,2,4,â€¦ for even dims
           i   = np.arange(0, d_model, 2)
           # inverse frequency term
           w   = 1 / (10000 ** (2*i / d_model))

           pe = np.zeros((max_len, d_model))
           pe[:, 0::2] = np.sin(pos * w)  # apply sine at even indices
           pe[:, 1::2] = np.cos(pos * w)  # apply cosine at odd indices
           return pe
       ###################################################################################
   6.2 Listing 2: Quick Numeric Example
       #################################################################################
       pe = positional_encoding(6, 5)
       np.round(pe, 4)
       # array([
       #  [ 0.     ,  1.     ,  0.     ,  1.     ,  0.     ,  1.     ],
       #  [ 0.8415 ,  0.5403 ,  0.0022 ,  1.     ,  0.     ,  1.     ],
       #  [ 0.9093 , -0.4161 ,  0.0043 ,  1.     , â€¦ ],
       #  â€¦ ])
       #################################################################################

7. Visualizing on Grids
   -a. Setup: max_len=7, d_model=20 â†’ 7 rows Ã— 10 circles.
   -b. Each subplot draws a unit circle and one dot at (cosâ¡,sinâ¡) for that dimension-pair and token position.
   -c. Observation: left columns (small ğ‘–) rotate quickly; right columns slowly.

8. Key Mathematical Properties
   8.1 Constant Vector Norm
       Using sin^2 + cos^2 = 1, one shows
       âˆ¥ğ‘ƒğ¸_(ğ‘ğ‘œğ‘ )âˆ¥2 = np.root(âˆ‘(ğ‘–=0 to ğ‘–=ğ‘‘/2âˆ’1) (sin^2+cos^2) = np.root(ğ‘‘_model / 2)),
       independent of ğ‘ğ‘œğ‘ 

   8.2 Translation-Invariant Dot-Product
       Define two positions ğ‘ and ğ‘+ğ‘˜. Their dot product is
       âŸ¨ğ‘ƒğ¸_ğ‘, ğ‘ƒğ¸_(ğ‘+ğ‘˜)âŸ©=âˆ‘(ğ‘–=0 to ğ‘–=ğ‘‘/2âˆ’1) cos(ğ‘˜ / 10000^(2ğ‘–/ğ‘‘))
       which depends only on offset ğ‘˜, not on absolute position ğ‘
       -a. This implies a linear â€œrotationâ€ matrix ğ‘€_ğ‘˜(independent of ğ‘) exists so that
           ğ‘ƒğ¸_(ğ‘+ğ‘˜)=ğ‘€_ğ‘˜ ğ‘ƒğ¸_ğ‘

9. 2-D Visual Confirmation
   9.1 d_model=2 Example (Listing 4 & Fig 9)
       -a. For d_model=2, max_len=12, each PE is one point on one unit circle.
       -b. Plot all 12 rays from the originâ€”angles increase uniformly mod 2ğœ‹
       -c. ** verifies** all pairs separated by the same ğ‘˜ share identical angular offset.
   9.2 Heatmap of Dot-Products (Listing 5 & Fig 11â€“12)
       #####################################################################
       pe   = positional_encoding(64, 20)
       dist = pe @ pe.T        # shape (20Ã—20)
       plt.imshow(dist, cmap='jet')
       #####################################################################
       -a. Result: constant-color diagonals corresponding to offset ğ‘˜
       -b. with small ğ‘‘, similarity vs. ğ‘˜ oscillates (wrap-around periodicity).
       -c. with large ğ‘‘, similarity decays nearly monotonically as ğ‘˜ grows.

10. Takeaways & Implications
    -a. Injects absolute order into parallel self-attention via fixed sinusoids.
    -b. Enables relative queriesâ€”model sees the same offset pattern wherever it occurs.
    -c. Parameter-free: no learned PE weights required.
    -d. Geometrically transparent: unit-circle visuals and heatmaps make the mechanism clear.
    -e. Rotational structure (ğ‘ƒğ¸_(ğ‘+ğ‘˜)=ğ‘€_ğ‘˜ ğ‘ƒğ¸_ğ‘) ensures consistent â€œnext tokenâ€ or â€œÂ±k tokens awayâ€ attention everywhere.

