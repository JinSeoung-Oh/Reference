### From https://generativeai.pub/the-inner-workings-of-metas-sam-2-7d5cb7c04d58

1. Overview of SAM 2 Model Architecture
   SAM 2 extends the original SAM (Segment Anything Model) to handle videos and promptable video segmentation (PVS). 
   In addition to segmenting objects in a single image (as SAM does), SAM 2 can track and segment objects across multiple frames in a video, guided by user prompts.
   The main architectural differences lie in how it integrates temporal information and memory, 
   enabling the model to handle video sequences rather than just individual images.

2. Key Components of SAM 2:
   -a. Image Encoder: Processes individual frames into feature embeddings.
   -b. Memory Attention Module: Integrates temporal context from previously processed frames and prompted frames.
   -c. Mask Decoder: Combines features from memory and prompt tokens to produce segmentation masks for the current frame.
   -d. Memory Encoder and Memory Bank: Stores selected outputs from processed frames to provide temporal and semantic context for future frames.

3. Differences from SAM:
   SAM 2 adds a memory attention module, memory encoder, and memory bank.
   SAM 2 handles multi-frame inputs, while SAM focused on single-frame segmentation.
   SAM 2 leverages object pointers and temporal positional encodings to track objects over time.

4. Image Encoder
   -a. Inputs and Outputs:
       -1. Input: A single image/frame (e.g., 1024x1024 pixels).
       -2. Internal Processing:
           The image is passed through a Heira Image Encoder that extracts multi-stage features.
           A Feature Pyramid Network (FPN) fuses features, particularly from Stage 3 and Stage 4, producing a lower resolution but richer feature map.
       -3. Outputs:
           “Unconditioned frame embedding” of shape (256, 64, 64), encoding the frame’s spatial features in a compressed latent form.
           Additionally, Stage 1 and Stage 2 high-resolution features are preserved for the mask decoder’s upsampling steps.

   -b. Why this design?
       Spatial compression plus increased channel depth balances efficiency and representational richness.
       Stage 1 and 2 features are kept for later mask refinement, enabling the model to reconstruct segmentation masks at higher resolutions.

5. Memory Attention Module
   -a. Purpose:
       Incorporates temporal context into the current frame’s embedding by attending to previously processed frames, prompted frames, 
       and object pointers stored in a memory bank.

   -b. Inputs:
       Current frame’s unconditioned embedding (256, 64, 64).

   -c. Memory bank containing:
       -1. Previous frame embeddings (up to 6 recent frames).
       -2. Prompted frame embeddings (2 frames where user gave prompts).
       -3. Object pointer tokens (8 total).

   -d. Processing:
       -1. The memory attention module consists of 4 repeated blocks, each with:
       -2. Self-attention on the current frame features (refines current frame representation).
       -3. Cross-attention to the memory bank (injects temporal and contextual knowledge).

6. A Multi-Layer Perceptron (MLP) layer.
   Each block maintains the same shape (256, 64, 64), gradually enriching the current frame’s representation with temporal context and object identity.

   -a. Output:
       Memory-conditioned features (256, 64, 64) that now incorporate knowledge from past and prompted frames, as well as object pointers.

   -b. Mask Decoder
       -1. Purpose:
           Generates final segmentation masks (and related outputs like IoU and occlusion scores) for the current frame, 
           guided by both memory-conditioned features and prompt information.

       -2. Inputs:
           Memory-conditioned frame embedding (256, 64, 64).
           Dense embeddings from the prompt encoder. Prompts can be:
           Mask prompts (dense embeddings that match the frame embedding shape).
           Point and box prompts (sparse embeddings, shape (N, 256)).

       -3. Special output tokens:
           Mask tokens (handle ambiguous prompts).
           IoU token (estimates mask quality).
           Occlusion token (checks if the object is visible in the current frame).
           Object pointer tokens (represent semantic information about objects).

       -4. High-Level Flow:
           The memory-conditioned frame embedding and the prompt embeddings are fused (for mask prompts) or combined with prompt tokens (for point/box prompts).

7. A two-way transformer decoder:
   Applies cross-attention between token embeddings and flattened frame embeddings.
   Tokens interact with image features and vice versa, enriching each other’s representation.

   -1. Upsampling:
       The final enriched embedding is progressively upsampled using Stage 2 and Stage 1 image encoder features.
       Produces a final mask of shape (1, 1024, 1024) (or 3 masks if ambiguous).
       IoU and occlusion scores are predicted from corresponding tokens.
   
   -2. Outputs:
       Segmentation masks.
       IoU score for mask quality.
       Occlusion score indicating object visibility.
       Object pointer (e.g., (3x256) if 3 masks produced, best one chosen) to update the memory bank.

8. Memory Encoder and Memory Bank
   -1. Purpose:
       Stores selected frame embeddings, prompts, and object pointers across frames, forming a temporal context for the memory attention module.

   -2. Inputs:
       Unconditioned frame embedding (256, 64, 64) from image encoder (before memory attention).
       Best predicted mask from the mask decoder (1, 1024, 1024).
       The chosen object pointer (1, 256).

   -3. Processing:
       The predicted mask is down-sampled and added to the unconditioned embedding, forming a memory frame embedding (64, 64, 64).

   -4. Frames are categorized as:
       -a. Prompted frames: stored without temporal positional encoding.
       -b. Recent frames: combined with temporal positional encoding to reflect their position in time.

9. Object pointer tokens (e.g., reshaped to (4x64)) are also stored.
   -1. Memory Bank Composition:
       Up to 6 recent frames (with temporal encoding).
       Up to 2 prompted frames.
       Up to 8 object pointers (from those frames).
       FIFO order is maintained to keep memory bank size limited.

10. Training Pipeline (with One Data Sample)
    -1. Prerequisites:
        - Model initially pre-trained on the SA-1B dataset for single-frame segmentation (like SAM).
        - This pretraining ensures strong segment-anything capabilities.

    -2. Training for Promptable Video Segmentation (PVS):
        - Input: A batch of videos, each with 8 frames (1024x1024).
        - Ground truth masks are available for all frames.

    Two frames per video are selected as “prompted” frames. The first frame is always chosen if present; the second is chosen randomly. 
    If not, both frames are randomly chosen.

    -3. Process:
        -a. Image Encoding:
            All 8 frames processed by image encoder in parallel.
            Produces unconditioned embeddings for each frame.

        -b. Frame-by-Frame Processing: For each frame (say frame #7):
            -1. Memory Attention:
                Takes current frame’s unconditioned embedding.
                Cross-attends with memory bank (recent & prompted frames + object pointers).
                Outputs memory-conditioned embedding (256, 64, 64).

            -2. Mask Decoder (7 iterations per frame):
               -1. Iteration 1:
                   Uses initial prompt tokens (from the chosen frames) and memory-conditioned embedding.
                   Produces masks, IoU, and occlusion scores.
                   Compute loss = 20 * mask_loss + iou_loss + occlusion_loss.
                   Backpropagate.
                 
                -2. Iterations 2-7:
                    Refine the prompt by selecting new points or reusing best mask from previous iteration.
                    Update output tokens with previous outputs.
                    Improve segmentation step-by-step.

           -3. After 7 iterations, pick the best mask prediction and its object pointer, feed into memory encoder:
               Update memory bank with new frame memory and object pointer.

    -4. Temporal Context:
        The memory bank persists across frames within the same video.
        As we move to the next frame, the model has richer temporal information, aiding in maintaining object identity and segmentation quality over time.

    -5. Final Objective:
        Model learns to handle both initial segmentation (like SAM) and sustained object tracking/segmentation over multiple frames (PVS).

11. Inference Pipeline
    -a. Key Requirements:
        User gives a prompt (point/box/mask) on the initial frame.
        Model segments the object and overlays the mask on the image.
        For subsequent frames, the model must produce masks for tracked objects.
        If the user corrects a mask at a certain frame, the model updates the memory and reprocesses subsequent frames.
        Multiple objects? Run components in parallel for each object (as model handles one object at a time).

   -b. Process:
       Just like training, but no gradient updates.
       The model uses the memory bank to propagate object segmentation across frames.
       If corrections are made, re-run inference from the corrected frame onward.

12. Summary
    SAM 2 is an evolution of SAM that adds temporal reasoning for promptable video segmentation. It does so by:

    Introducing memory attention to integrate multi-frame context.
    Using a memory encoder/bank to store previously processed frames and object pointers.
    Employing a mask decoder that fuses memory-conditioned frame embeddings, prompt embeddings, 
    and high-resolution features to produce accurate segmentation masks frame-by-frame.

This approach allows SAM 2 to:

Track objects across multiple frames.
Refine object masks with repeated updates.
Achieve strong performance in tasks like PVS, making it suitable for AR/VR, robotics, autonomous driving, and video editing domains.
By fully understanding each component and its purpose, we see how SAM 2 transitions from a static image segmentation model (SAM) to a 
dynamic video segmentation system, maintaining temporal coherence and allowing interactive refinement over the course of a video.

