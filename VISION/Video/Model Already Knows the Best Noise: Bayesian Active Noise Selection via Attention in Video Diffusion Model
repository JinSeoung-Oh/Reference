### From https://arxiv.org/pdf/2505.17561

1. Motivation & Problem Statement
   -a. Video diffusion sensitivity: In text-to-video diffusion, the initial noise seed profoundly affects output quality, 
                                    temporal coherence, and prompt alignmentâ€”even with the same prompt.
   -b. Limitations of prior methods: Existing noise-prior approaches (inter-frame smoothing, frequency filtering, partial sampling) 
                                     rely on hand-designed signals and require multiple full diffusion runs to evaluate candidates. 
                                     They overlook internal model cues that indicate which seeds the model â€œprefers.â€

2. ANSE: Model-Aware Noise Selection
   ANSE is a principled, inference-time framework that selects high-quality noise seeds by measuring attention-based uncertainty 
   within the model itself, rather than imposing external priors.

3. BANSA: Bayesian Active Noise Selection via Attention
   -a. Core idea: Treat attention maps as stochastic predictions, then quantify their uncertainty and consistencyâ€”analogous to BALD 
                  in classification but applied to generative attention.
   -b. Attention entropy: For each candidate seed ğ‘§, prompt ğ‘, and timestep ğ‘¡, extract K stochastic attention maps 
                          ğ´^(1),â€¦,ğ´^(ğ¾)
   -c. BANSA score:
       BANSA(ğ‘§,ğ‘,ğ‘¡)=1/ğ¾ âˆ‘_ğ‘˜ ğ»(ğ´^(ğ‘˜))âˆ’ğ»(1/ğ¾ âˆ‘_ğ‘˜ ğ´^(ğ‘˜)),
       where ğ»(â‹…) is the average row-wise Shannon entropy.
   -d. Interpretation:
       -1. First term: average uncertainty per sample.
       -2. Second term: entropy of the mean map (lower if samples agree).
       -3. Low BANSA â‡’ attention maps are both confident (low entropy) and consistent (low disagreement), which correlates with high-quality, 
                        coherent video generation.
   -e. Selection rule:
       ğ‘§âˆ—=arg min_(ğ‘§âˆˆğ‘) BANSA(ğ‘§,ğ‘,ğ‘¡)

4. Efficient Approximation via Bernoulli-Masked Attention
   -a. Challenge: Computing BANSA naively requires ğ¾ full forward passes per seed.
   -b. Solution: Inject stochasticity directly into one pass by applying a random Bernoulli mask ğ‘š to the attention scores:
       ğ´^(ğ‘˜)=Softmax(ğ‘„ğ¾^âŠ¤ âŠ™ ğ‘š^(ğ‘˜)),
       then compute the same entropy-based BANSA-E surrogate using these ğ¾ masked samples.
   -c. Benefit: Multiple â€œstochasticâ€ attention maps from a single forward passâ€”drastically reducing compute while still capturing uncertainty.

5. Layer-Wise Truncation for Speed
   -a. Observation: Not all attention layers contribute equally to the uncertainty signal.
   -b. Strategy: Compute cumulative BANSA-E up to layer ğ‘‘, and choose the smallest ğ‘‘âˆ— for which the Pearson correlation with the full-layer
                 BANSA-E is above a threshold (e.g., ğœ=0.7).
       -1. On CogVideoX-2B, correlation saturates by layer 14; on CogVideoX-5B, by layer 19.
   -c. Result: Evaluate only the first ğ‘‘âˆ— layersâ€™ BANSA-Eâ€”further slashing compute with negligible quality loss.

6. Overall ANSE Pipeline
   -a. Sample a pool of noise seeds ğ‘
   -b. For each ğ‘§âˆˆğ‘, compute approximate BANSA-E using Bernoulli-masked attention on layers â‰¤ğ‘‘âˆ—
   -c. Select the seed ğ‘§âˆ— with the lowest BANSA-E score.
   -d. Generate the video by running the diffusion sampler from ğ‘§âˆ—
   This adds only an 8% (2B) or 13% (5B) inference-time overhead yet yields notable gains in visual fidelity and temporal consistency.

7. Key Contributions
   -a. ANSE: the first active, model-aware noise-selection framework for text-to-video diffusion.
   -b. BANSA: a novel acquisition function measuring attention-map uncertainty under stochastic perturbations.
   -c. Practical deployment: Bernoulli-masked approximation + layer truncation enable fast, 
                             inference-time selection without retraining or external priors.

