## From https://levelup.gitconnected.com/vits-text-to-speech-synthesis-935fdd778d82

"""
1. Variational Inference Text-to-Speech (VITS)
   -1. Architecture of VITS
       The VITS (Variational Inference Text-to-Speech) model is a state-of-the-art text-to-speech system designed to generate 
       high-quality, natural-sounding speech by effectively capturing the complex patterns and variations in the human voice.

       The architecture of VITS is based on a Conditional Variational Auto-encoder (VAE) and leverages adversarial training (GAN).
       It also includes normalizing flows to provide a better prior distribution that matches the speech data.

       - Conditional VAE: The main structure of VITS includes a Posterior Encoder, a Text Encoder, and a Decoder.
       - Generative Adversarial Networks (GAN): Provides adversarial training to improve generation quality.
       - Normalizing Flows: Increases the expressiveness of the prior distribution ğ‘(ğ‘§âˆ£ğ‘).

    -2. Variational Auto-encoder (VAE)
        The VAE is a model based on the Gaussian Mixture Model (GMM). The main idea of GMM is to approximate a target distribution ğ‘(ğ‘¥)
        by combining several Gaussian distributions, each with its own mean and variance.

        The GMM model can be written as the following equation:
        ğ‘(ğ‘¥)=âˆ‘_ğ‘–(ğœ‹_ğ‘–)â‹…ğ‘(ğ‘¥âˆ£ğœ‡_ğ‘–,ğœ_ğ‘–)
        This equation shows the sampling process of the GMM model, where ğ‘(ğ‘§) is a standard Gaussian distribution, and ğ‘(ğ‘¥âˆ£ğ‘§)
        is a conditional Gaussian distribution parameterized by ğœ‡(ğ‘§) and ğœ(ğ‘§). Here, ğœ‡ and ğœ are estimated by a neural network, 
        which is the decoder of the VAE.

        With a proper ğ‘(ğ‘¥âˆ£ğ‘§), new data can be easily generated by:

        - Sampling a latent vector ğ‘§ from a standard Gaussian distribution.
        - The decoder evaluates ğœ‡ and ğœ based on the given ğ‘§.
        - Sampling the new data ğ‘¥ from the distribution ğ‘(ğœ‡(ğ‘§),ğœ(ğ‘§)).

    -3. Architecture of VAE
        The architecture of VAE includes a new component: the encoder, which represents the approximate posterior distribution ğ‘(ğ‘§âˆ£ğ‘¥)
        The main reason for introducing the encoder in VAE is to assist the decoder in producing a better ğ‘(ğ‘¥âˆ£ğ‘§)

        Unlike traditional auto-encoders, VAE introduces stochastic elements into the encoding process. 
        Instead of directly mapping an input to a fixed latent vector, it maps the input to a distribution (ğ‘§âˆ¼ğ‘(ğœ‡â€²(ğ‘¥),ğœâ€²(ğ‘¥))). 
        This allows the decoder to generate a variety of outputs from the same latent space, 
        improving the issue of blurry outputs in auto-encoders.

    -4. Training Process
        The training process of a VAE is straightforward. The encoder encodes the input ğ‘¥ into a latent vector ğ‘§, 
        while the decoder reconstructs this vector back to ğ‘¥â€². The loss is then the Mean Squared Error (MSE) between ğ‘¥ and ğ‘¥â€².

    -5. Variational Inference 
        In VITS, our objective is to input a text sequence and output its corresponding audio signal. 
        This is represented as a conditional probability ğ‘(ğ‘¥âˆ£ğ‘), where ğ‘¥ is the audio signal and ğ‘ is the text sequence.

        To train our neural network with training data ğ‘¥ and ğ‘, we need to maximize the log-likelihood of ğ‘(ğ‘¥âˆ£ğ‘).
        However, evaluating this in its integral form is difficult. Therefore, we use the Variational Inference method to transform
        the original optimization of ğ‘(ğ‘¥âˆ£ğ‘) into the optimization of its Evidence Lower Bound (ELBO).

        - Introduce an approximate posterior ğ‘(ğ‘§âˆ£ğ‘¥,ğ‘). 
        - Apply Jensenâ€™s Inequality:
                                    log ğ‘(ğ‘¥)â‰¥ğ¸_ğ‘(ğ‘§âˆ£ğ‘¥)[log (ğ‘(ğ‘¥,ğ‘§) / ğ‘(ğ‘§âˆ£ğ‘¥)]
        - Rewrite the Expectation:
          This equation can be further simplified as:
          The first term corresponds to the reconstruction loss, where we calculate the MSE between the modelâ€™s output signal 
          and the true signal.

         The second term is the KL loss, which maximizes the posterior distribution ğ‘ to approach the prior distribution ğ‘.
         This ensures that the learned latent representations are meaningful and consistent with the prior assumptions.

         The posterior encoder is a WaveNet-based encoder. It takes the spectrogram (ğ‘‹_lin) of the raw audio as input and
         outputs a latent vector ğ‘§.
         
         The decoder of VITS is essentially the generator of HiFi-GAN, which recovers the original waveform (ğ‘¦^) from the 
         latent vector ğ‘§ provided by the posterior encoder or text encoder.

         The reconstruction loss is computed using the mel-spectrogram instead of the waveform, so ğ‘‹_lin and ğ‘¦^
         need to be transformed into mel-spectrograms before computing the L1 loss.

2. Normalizing Flows
    The key idea of normalizing flow is to transform a simple probability distribution
    (e.g., Gaussian distribution) into a complex probability distribution through 
    a series of invertible and differentiable transformations.

    Although flow models also optimize the model by maximizing the log likelihood, unlike VAEs, 
    they assume that the distribution ğ‘(ğ‘¥âˆ£ğ‘§) is a Dirac distribution ğ›¿(ğ‘¥âˆ’ğ‘”(ğ‘§)), which allows us to compute 
    the integral of the likelihood directly.

    -1. Proof of Normalizing Flow
        We first assume we have an invertible function ğ‘” where ğ‘¥=ğ‘”(ğ‘§) and its inverse is ğ‘§=ğ‘“(ğ‘¥)
        The prior distribution ğ‘(ğ‘§) is a Gaussian distribution.

        Let ğ‘(ğ‘¥âˆ£ğ‘§) be a Dirac distribution, based on its property, we have:

        Since the Dirac distribution only has a non-zero value when ğ‘¥=ğ‘”(ğ‘§) and ğ‘§=ğ‘“(ğ‘¥)
        , we can transform the prior ğ‘(ğ‘§) into ğ‘(ğ‘“(ğ‘¥)). However, because we performed a change of variables on 
        ğ‘(ğ‘§), we need to multiply it by the determinant of the Jacobian matrix.

        ğ‘(ğ‘¥)=ğ‘(ğ‘“(ğ‘¥))â‹…âˆ£det(ğ½_ğ‘“(ğ‘¥))âˆ£
       
        Now, we can easily maximize the log-likelihood of ğ‘(ğ‘¥) by optimizing the function ğ‘“, which is usually a neural network. 
        Another requirement for ğ‘“ is that its Jacobian needs to be easy to compute.

3. Additive Coupling
   To ensure the determinant of the Jacobian is simple enough, a single flow block is defined as follows:
   We first split the input ğ‘¥ into ğ‘¥_1 and ğ‘¥_2 . 
   Define â„_1 = ğ‘¥_1, and â„_2 = ğ‘¥_2 + ğ‘š(ğ‘¥_1), where ğ‘š can be any function. The determinant of the Jacobian of â„
   now becomes 1 (log det(J) = 0), allowing us to stack multiple flow blocks, creating a complex mapping function ğ‘“.

The reverse process of flow is defined as follows:

4. The Use of Flow in VITS
    For the classic VAE, we usually assume the prior distribution ğ‘(ğ‘§âˆ£ğ‘) follows a Gaussian distribution. 
    However, the expressiveness of a Gaussian distribution may not be sufficient to capture more complex distributions.

    Thus, VITS introduces normalizing flows to increase the expressiveness of the prior distribution ğ‘(ğ‘§âˆ£ğ‘). 
    By applying an invertible transformation ğ‘“_ğœƒ, we can transform the Gaussian distribution ğ‘(ğ‘§âˆ£ğ‘) into a more 
    complex distribution ğ‘(ğ‘“(ğ‘§)âˆ£ğ‘).

    The KL loss in the previous section can then be written as:
    KL(ğ‘(ğ‘§âˆ£ğ‘¥,ğ‘)âˆ¥ğ‘(ğ‘“(ğ‘§)âˆ£ğ‘))

5. Duration Predictor
   VITS addresses the issue of different lengths between the audio signal (spectrogram) and the text by including 
   a "Duration Predictor" that predicts the duration of each text segment in the audio signal.
   In the above example, the letter 'a' spans two time frames in the spectrogram, while the letter 'c' spans one.
   To train the Duration Predictor, we use the Monotonic Alignment Search (MAS) method to calculate 
   the true time durations of the text segments in the spectrogram.

   MAS is an unsupervised alignment method that determines the time duration of each text representation in the spectrogram.
   It creates a score matrix ğ´^âˆ—
  representing the similarity between text features and audio frames. By finding the optimal path through this matrix,
  we can align text characters with their corresponding speech frames.

6. Training Process of Duration Predictor
   -1. Text Encoding:
       - The input text sequence ğ‘_text is processed by a text encoder to produce a text representation â„_text.
       - â„_text is projected into mean ğœ‡ and standard deviation ğœ through a projection layer.
       - â„_text is passed to the duration predictor to predict the duration ğ‘‘.

   -2. Spectrogram Encoding:
       - The input spectrogram is processed by a posterior encoder to produce a latent vector ğ‘§.
       - The latent vector ğ‘§ is transformed through a normalizing flow to obtain ğ‘“(ğ‘§).

   -3. Score Matrix Computation:
       Compute the score matrix ğ´^âˆ— using the log-likelihood. This matrix represents the similarity between the 
       text representations and the spectrogram frames.

   -4. Optimal Path Finding (MAS):
        - Use Dynamic Programming (DP) to search from the top-left corner to the bottom-right corner of the score matrix 
          ğ´^âˆ— to find the path with the highest total score.
        - Each step in this path represents the alignment of a text character with a speech frame.

   -5. Duration Predictor Update:
      - The Duration Predictor model is updated based on the Mean Squared Error (MSE) between the optimal path 
        provided by MAS and the modelâ€™s current predictions (Duration Loss).

"""
def sequence_mask(length, max_length=None):
    if max_length is None:
        max_length = length.max()
    x = torch.arange(max_length, dtype=length.dtype, device=length.device)
    return x.unsqueeze(0) < length.unsqueeze(1)
  
class PosteriorEncoder(nn.Module):
    def __init__(self, in_channels, out_channels, hidden_channels, \
                 kernel_size = 5, dilation_rate = 1, n_layers = 16, gin_channels=0):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.hidden_channels = hidden_channels
        self.kernel_size = kernel_size
        self.dilation_rate = dilation_rate
        self.n_layers = n_layers
        self.gin_channels = gin_channels

        self.pre = Conv1d(in_channels, hidden_channels, 1)
        self.proj = Conv1d(hidden_channels, out_channels * 2, 1)
        
        # WaveNet, see https://arxiv.org/abs/1609.03499
        self.enc = modules.WN(hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=gin_channels)

    def forward(self, x, x_lengths, g=None):
        x_mask = torch.unsqueeze(sequence_mask(x_lengths, x.size(2)), 1).to(x.dtype)
        x = self.pre(x) * x_mask 
        x = self.enc(x, x_mask, g=g) # [b, h, spec_size]
        stats = self.proj(x) * x_mask # [b, h * 2, spec_size]
        m, logs = torch.split(stats, self.out_channels, dim=1)
        
        # z sampling (reparameterization trick)
        z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask
        return z, m, logs, x_mask

  class Decoder(nn.Module):
    def __init__(self, initial_channel, resblock_kernel_sizes, upsample_rates, 
                 upsample_initial_channel, upsample_kernel_sizes, gin_channels=0):
        super(Decoder, self).__init__()
        self.num_kernels = len(resblock_kernel_sizes)
        self.num_upsamples = len(upsample_rates)
        self.conv_pre = Conv1d(initial_channel, upsample_initial_channel, 7, 1, padding=3)

        self.ups = nn.ModuleList()
        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):
            self.ups.append(ConvTranspose1d(upsample_initial_channel // (2 ** i), \
                                            upsample_initial_channel // (2 ** (i + 1)), \
                                            k, u, padding=(k - u) // 2))

        self.resblocks = nn.ModuleList()
        for i in range(len(self.ups)):
            ch = upsample_initial_channel // (2 ** (i + 1))
            for k in resblock_kernel_sizes:
                self.resblocks.append(ResBlock(ch, k, [1, 3, 5]))

        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)
        if gin_channels != 0:
            self.cond = Conv1d(gin_channels, upsample_initial_channel, 1)

    def forward(self, x, g=None):
        x = self.conv_pre(x)
        if g is not None:
            x = x + self.cond(g)

        for i in range(self.num_upsamples):
            x = F.leaky_relu(x, 0.2)
            print(x.shape)
            x = self.ups[i](x)
            xs = None
            for j in range(self.num_kernels):
                if xs is None:
                    xs = self.resblocks[i * self.num_kernels + j](x)
                else:
                    xs += self.resblocks[i * self.num_kernels + j](x)
            x = xs / self.num_kernels
        x = F.leaky_relu(x)
        x = self.conv_post(x)
        x = torch.tanh(x)
        return x


class ResidualCouplingLayer(nn.Module):
    def __init__(...):
        super().__init__()
        # initialise model parameters
        
        self.pre = Conv1d(self.half_channels, hidden_channels, 1)
        # WaveNet
        self.enc = WN(hidden_channels, kernel_size, dilation_rate, \
                      n_layers, p_dropout=p_dropout, gin_channels=gin_channels)
        self.post = Conv1d(hidden_channels, self.half_channels, 1)

    def forward(self, x, x_mask, g=None, reverse=False):
        x0, x1 = torch.split(x, [self.half_channels]*2, 1)
        h = self.pre(x0) * x_mask
        h = self.enc(h, x_mask, g=g)
        m = self.post(h) * x_mask

        if not reverse:
            x1 = m + x1 * x_mask
            x = torch.cat([x0, x1], 1)
            logdet = torch.zeros(x.size(0), device=x.device)
            return x, logdet
        else:
            x1 = (x1 - m) * x_mask
            x = torch.cat([x0, x1], 1)
            return x, None



class VITGenerator(nn.Module):
    def __init__(...):
        super().__init__()
        '''
        self.enc_p : TextEncoder, encodes text into latent z (P(z|c))
        self.enc_q : PosteriorEncoder, encodes spectrograms into latent z (P(z|x))
        self.dec : Decoder, decodes latent z into waveform (P(x|z))
        self.flow: Enhances the prior distribution
        self.dp: Duration Predictor, Aligns text and spectrograms
        '''
        
    def forward(self, x, x_lengths, y, y_lengths):
        # x: raw text
        # y: spectrograms
        # m_p, logs_p: shape = [batch_size, hidden_size, seq_size]
        # m_q, logs_q: shape = [batch_size, hidden_size, spec_size]

        g = None
        x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)
        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)
        z_p = self.flow(z, y_mask, g=g)
        with torch.no_grad():
            # MAS align, 
            # compute score matrix A*
            # value = logâ¡ N(z_p; m_p, exp(logs_p))
            s_p_sq_r = torch.exp(-2 * logs_p) 
            neg_cent1 = torch.sum(-0.5 * math.log(2 * math.pi) - logs_p, [1], keepdim=True)
            neg_cent2 = torch.matmul(-0.5 * (z_p ** 2).transpose(1, 2), s_p_sq_r) 
            neg_cent3 = torch.matmul(z_p.transpose(1, 2), (m_p * s_p_sq_r)) 
            neg_cent4 = torch.sum(-0.5 * (m_p ** 2) * s_p_sq_r, [1], keepdim=True)
            neg_cent = neg_cent1 + neg_cent2 + neg_cent3 + neg_cent4
            attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)
            attn = monotonic_align.maximum_path(neg_cent, attn_mask.squeeze(1)).unsqueeze(1).detach()
        
        # Duration Loss
        w = attn.sum(2)
        logw_ = torch.log(w + 1e-6) * x_mask
        logw = self.dp(x, x_mask, g=g)
        l_dur = torch.sum((logw - logw_)**2, [1,2]) / torch.sum(x_mask)
        
        # Expand prior based on alignment matrix 'attn'
        # [batch_size, hidden_size, seq_size] -> [batch_size, hidden_size, spec_size]
        m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)
        logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)
        z_slice, ids_slice = slice_segments(z, x_lengths = y_lengths, segment_size = self.seg_size)
        o = self.dec(z_slice, g=g)
        return o, l_dur, attn, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)
class HiFiGANDiscriminator(nn.Module):
    def __init__(self, period):
        super(HiFiGANDiscriminator, self).__init__()
        self.period = period
        self.convs = nn.ModuleList([
            nn.Conv2d(1, 32, (5, 1), (3, 1), padding=(2, 0)),
            nn.Conv2d(32, 128, (5, 1), (3, 1), padding=(2, 0)),
            nn.Conv2d(128, 512, (5, 1), (3, 1), padding=(2, 0)),
            nn.Conv2d(512, 1024, (5, 1), (3, 1), padding=(2, 0)),
            nn.Conv2d(1024, 1024, (5, 1), 1, padding=(2, 0))
        ])
        self.conv_post = nn.Conv2d(1024, 1, (3, 1), 1, padding=(1, 0))
        self.leaky_relu = nn.LeakyReLU(0.1)

    def forward(self, x):
        fmap = []
        b, c, t = x.shape
        if t % self.period != 0:
            n_pad = self.period - (t % self.period)
            x = F.pad(x, (0, n_pad), 'reflect')
            t = t + n_pad
 
        x = x.view(b, c, t // self.period, self.period)
        for l in self.convs:
            x = self.leaky_relu(l(x))
            fmap.append(x)
        x = self.conv_post(x)
        fmap.append(x)
        x = torch.flatten(x, 1, -1)
        return x, fmap

class MultiScaleDiscriminator(nn.Module):
    def __init__(self):
        super(MultiScaleDiscriminator, self).__init__()
        self.discriminators = nn.ModuleList([
            HiFiGANDiscriminator(2),
            HiFiGANDiscriminator(3),
            HiFiGANDiscriminator(5)
        ])

    def forward(self, y, y_hat):
        y_d_rs = []
        y_d_gs = []
        fmap_rs = []
        fmap_gs = []
        
        for d in self.discriminators:
            y_d_r, fmap_r = d(y)
            y_d_g, fmap_g = d(y_hat)
            y_d_rs.append(y_d_r)
            y_d_gs.append(y_d_g)
            fmap_rs.append(fmap_r)
            fmap_gs.append(fmap_g)
        return y_d_rs, y_d_gs, fmap_rs, fmap_gs

def discriminator_loss(disc_real_outputs, disc_generated_outputs):
    loss = 0
    r_losses = []
    g_losses = []
    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):
        dr = dr.float()
        dg = dg.float()
        r_loss = torch.mean((1-dr)**2)
        g_loss = torch.mean(dg**2)
        loss += (r_loss + g_loss)
        r_losses.append(r_loss.item())
        g_losses.append(g_loss.item())
    return loss, r_losses, g_losses


def generator_loss(disc_outputs):
    loss = 0
    gen_losses = []
    for dg in disc_outputs:
        dg = dg.float()
        l = torch.mean((1-dg)**2)
        gen_losses.append(l)
        loss += l
        
    return loss, gen_losses

def kl_loss(z_p, logs_q, m_p, logs_p, z_mask):
    z_p = z_p.float()
    logs_q = logs_q.float()
    m_p = m_p.float()
    logs_p = logs_p.float()
    z_mask = z_mask.float()

    kl = logs_p - logs_q - 0.5
    kl += 0.5 * ((z_p - m_p)**2) * torch.exp(-2. * logs_p)
    kl = torch.sum(kl * z_mask)
    l = kl / torch.sum(z_mask)
    return l

def feature_loss(fmap_r, fmap_g):
    # Computes the Mean Absolute Error (MAE) \
    # between the feature maps of real and generated data.
    
    loss = 0
    for dr, dg in zip(fmap_r, fmap_g):
        for rl, gl in zip(dr, dg):
            rl = rl.float().detach()
            gl = gl.float()
            loss += torch.mean(torch.abs(rl - gl))
    return loss * 2 

class VitTrainer():
    def __init__(...):
        super().__init__()
        # ...
        
    def train(self):
        # x: text sequence
        # spec: spectrogram
        # y: raw waveform
        
        for step in tqdm(range(self.step, self.total_steps+1), desc=f"Training progress"):
            x, x_lengths, spec, spec_lengths, y, y_lengths = next(iter(self.data_loader))
            x, x_lengths = x.to(self.device), x_lengths.to(self.device)
            spec, spec_lengths = spec.to(self.device), spec_lengths.to(self.device)
            y, y_lengths = y.to(self.device), y_lengths.to(self.device)

            with autocast(enabled=True):
                y_hat, l_length, attn, ids_slice, x_mask, z_mask,\
                (z, z_p, m_p, logs_p, m_q, logs_q) = self.g(x, x_lengths, spec, spec_lengths)

                # convert spec to mel, weveform to mel 
                mel = spec_to_mel_torch(spec, 1024, 80, 22050, 0.0, None)
                y_hat_mel = mel_spectrogram_torch(y_hat.squeeze(1), 1024, 80, 22050, 256, 1024, 0.0, None)
                y_mel, _ = slice_segments(mel, ids_str = ids_slice, segment_size = 8192 // 256)
                y, _ = slice_segments(y, ids_str = ids_slice * 256, segment_size = 8192) 

                # Discriminator (discriminating raw waveform)
                y_d_hat_r, y_d_hat_g, _, _ = self.d(y, y_hat.detach())
                with autocast(enabled=False):
                    loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(y_d_hat_r, y_d_hat_g)
                    loss_disc_all = loss_disc
            
            self.optim_d.zero_grad()
            self.scaler.scale(loss_disc_all).backward()
            self.scaler.unscale_(self.optim_d)
            self.grad_norm = nn.utils.clip_grad_norm_(self.d.parameters(), 1e9)
            self.scaler.step(self.optim_d)

            # Compute Generator's loss
            with autocast(enabled=True):
                y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = self.d(y, y_hat)
                with autocast(enabled=False):
                    loss_dur = torch.sum(l_length.float())
                    loss_mel = F.l1_loss(y_mel, y_hat_mel) * 45
                    loss_kl = kl_loss(z_p, logs_q, m_p, logs_p, z_mask)
                    loss_fm = feature_loss(fmap_r, fmap_g)
                    loss_gen, losses_gen = generator_loss(y_d_hat_g)
                    loss_gen_all = loss_gen + loss_fm + loss_mel + loss_dur + loss_kl
                    
            self.optim_g.zero_grad()
            self.scaler.scale(loss_gen_all).backward()
            self.scaler.unscale_(self.optim_g)
            self.grad_norm = nn.utils.clip_grad_norm_(self.g.parameters(), 1e9)
            self.scaler.step(self.optim_g)
            self.scaler.update()


def infer(self, x, x_lengths, max_len=None):
    g = None
    x, m_p, logs_p, x_mask = self.enc_p(x, x_lengths)
    logw = self.dp(x, x_mask, g=g)
    
    w = torch.exp(logw) * x_mask 
    w_ceil = torch.ceil(w)
    y_lengths = torch.clamp_min(torch.sum(w_ceil, [1, 2]), 1).long()
    y_mask = torch.unsqueeze(sequence_mask(y_lengths, None), 1).to(x_mask.dtype)
    attn_mask = torch.unsqueeze(x_mask, 2) * torch.unsqueeze(y_mask, -1)
    attn = monotonic_align.generate_path(w_ceil, attn_mask)
    m_p = torch.matmul(attn.squeeze(1), m_p.transpose(1, 2)).transpose(1, 2)
    logs_p = torch.matmul(attn.squeeze(1), logs_p.transpose(1, 2)).transpose(1, 2)

    z_p = m_p + torch.randn_like(m_p) * torch.exp(logs_p)
    z = self.flow(z_p, y_mask, g=g, reverse=True)
    o = self.dec((z * y_mask)[:,:,:max_len], g=g)
    return o, attn, y_mask, (z, z_p, m_p, logs_p)
