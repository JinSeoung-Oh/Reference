### https://medium.com/correll-lab/using-waypoints-to-stitch-diffusion-generated-sub-trajectories-548955bc9164

1. Overview
   The paper introduces a novel method to enhance the efficiency and quality of trajectories generated by diffusion policies through the use of waypoints. 
   Vanilla diffusion policies can produce diverse plans but often suffer from inefficiency or infeasibility. 
   By learning waypoints from data, the proposed approach improves trajectory efficiency in environments like Maze2D and simplifies training 
   procedures compared to other methods, such as Sub-trajectory Stitching with Diffusion (SSD).

   This technique also enables the reuse of diffusion models across varied environments by decoupling the training of waypoint generators 
   from diffusion policies.

2. Key Concepts
   -a. Reinforcement Learning (RL):
       -1. Online RL: Involves direct interaction with the environment to collect experience and update the policy. 
                      It faces challenges like slow data collection and high sample requirements.
       -2. Offline RL: Learns policies from pre-collected data, addressing scaling limitations of online RL. Methods include:
           - Value-based Methods: Learn state value estimations but require extensive hyperparameter tuning.
           - Reinforcement Learning via Supervised Learning (RvS): Uses behavior cloning without value estimation but struggles with the stitching problem, 
                                                                   where suboptimal behavior in datasets needs to be connected for optimal trajectories.

   -b. Waypoint Transformer:
       An RvS method that generates intermediate sub-goals to address the stitching problem.
       Uses a transformer trained on demonstrations to guide policies towards objectives with fewer samples, enhancing generalization across state space regions.

   -c. Diffusion Policies:
       Learn complex distributions to generate diverse, long-horizon plans without iterative planning, avoiding compounding error issues.
       However, they may produce infeasible trajectories without additional guidance mechanisms.

   -d. Sub-trajectory Stitching with Diffusion (SSD):
       Combines a diffusion policy with a learned value estimator to condition trajectory generation.
       Alternates between training a value estimator and diffusion policy but involves complex training and high computational costs.

3. Proposed Method
   The paper proposes replacing the action-value critic in SSD with a waypoint generator inspired by the Waypoint Transformer. Key features include:

   -a. Waypoint Generation:
       -1. Directly learned from expert data using a simple MLP.
       -2. Predicts future states (waypoints) based on current observations, trained using Mean Squared Error (MSE) loss.
       -3. Decoupled from the diffusion policy training process, reducing complexity.

   -b. Diffusion Policy Integration:
       -1. Conditions on both past observations (via FiLM conditioning) and goal conditions.
       -2. Waypoint diffusion generates intermediate goals, improving trajectory efficiency and feasibility.

4. Experiments and Results
   -a. Push-T Environment:
       -1. Setup: Trained a waypoint predictor on the Push-T dataset using a 2-step observation horizon and 16-step prediction horizon.
       -2. Findings:
           - Waypoint Diffusion Policy: Executes 16 actions before replanning, outperforming vanilla diffusion in task completion speed.
           - Rewards: Rapid early-stage improvement compared to vanilla diffusion. However, terminal accuracy may decrease due to waypoint imprecision 
                      near target locations.

   -b. Maze2D Environment:
       -1. Compared the proposed Waypoint + Conditional Diffusion Policy to SSD across Maze2D-UMaze-v1 and Maze2D-Medium-v1 environments.
       -2. Findings:
           - Efficiency: Achieved similar results to SSD without requiring large, transformer-based value networks.
           - Data Scarcity: Waypoints enable the policy to navigate regions with limited data samples more effectively, producing efficient trajectories.
           - Simplified Training: The waypoint generator, trained independently of the diffusion model, significantly reduces training complexity 
                                  and parameter count.

5. Key Advantages
   -a. Reduced Complexity:
       -1. Waypoint models are simpler and require fewer parameters than value-based methods like SSD.
       -2. Training is independent of the diffusion policy, enabling modular reuse of components.

   -b. Efficiency:
       -1. Improved trajectory generation, especially in environments with sparse data.
       -2. Reduced reliance on iterative training of value estimators.

   -c. Reusability:
       -1. A single diffusion model can be reused across multiple environments by swapping in pre-trained waypoint generators.

6. Conclusion
   The proposed method demonstrates that directly learned waypoints from expert data can guide goal-conditioned diffusion policies, 
   simplifying training and reducing parameter requirements. 
   The approach matches the performance of more complex methods like SSD while enabling modularity and reuse. 
   This opens avenues for efficient trajectory generation in unseen environments, 
   with potential applications in test-time policy learning and environment adaptation.

