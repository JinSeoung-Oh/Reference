## From https://towardsdatascience.com/temperature-scaling-and-beam-search-text-generation-in-llms-for-the-ml-adjacent-21212cc5dddb

"""
Temperature Scaling and Beam Search in Text Generation with LLMs
1. Temperature Scaling:
   -1. Concept
       Temperature scaling is a technique used to control the randomness of predictions made by a language model. It modifies the probabilities of the next word prediction in a sequence.
   -2. Mechanism
       The logits (raw prediction scores) output by the model are divided by the temperature value before applying the softmax function. 
       A lower temperature (<1) makes the model more confident and conservative by sharpening the probability distribution, leading to more deterministic outputs. 
       Conversely, a higher temperature (>1) smooths the distribution, making the model's output more random and diverse.
   -3. Applications
       Useful in creative writing, chatbot responses, and any application where the diversity of output is desirable.

2. Beam Search:
   -1. Concept
       Beam search is a heuristic search algorithm used to generate sequences by maintaining a fixed number of the most promising candidate sequences (beams) at each step and expanding them.
   -2. Mechanism
       Instead of selecting only the top candidate at each step (greedy search), beam search keeps the top k candidates.
       At each step, it expands all k candidates and retains the new top k sequences based on cumulative probability scores.
   -3. Applications
       Often used in machine translation, summarization, and other sequence-to-sequence tasks where it is important to consider multiple possible sequences to find the most likely output.

3. Combining Temperature Scaling and Beam Search
   -1. Integration
       Temperature scaling can be applied within the beam search process to control the diversity of each beam's predictions.
   -2. Balancing Act
       Using temperature scaling within beam search allows fine-tuning the balance between exploration (randomness) and exploitation (choosing the best candidate),
       improving the quality and diversity of generated text.
"""
# Example code
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load pre-trained model and tokenizer
model_name = 'gpt2'
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

def generate_text(prompt, temperature=1.0, num_beams=3, max_length=50):
    input_ids = tokenizer.encode(prompt, return_tensors='pt')

    # Generate text using beam search with temperature scaling
    output = model.generate(
        input_ids,
        max_length=max_length,
        num_beams=num_beams,
        temperature=temperature,
        early_stopping=True
    )

    return tokenizer.decode(output[0], skip_special_tokens=True)

# Example usage
prompt = "The future of AI is"
generated_text = generate_text(prompt, temperature=0.7, num_beams=5)
print(generated_text)
# This code demonstrates how to use the Hugging Face Transformers library to generate text with GPT-2, applying both temperature scaling and beam search.

"""
Conclusion
Temperature scaling and beam search are powerful techniques for improving the quality and diversity of text generated by large language models. 
Understanding and tuning these parameters allows practitioners to better control model outputs, making these techniques valuable tools in the development of AI applications.
"""
