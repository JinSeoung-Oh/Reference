### From https://levelup.gitconnected.com/fanformer-is-the-new-game-changing-architecture-for-llms-d56999fab7f2

1. Introduction
   Large language models (LLMs) have long surprised us with their capabilities, and many expected that scaling them would eventually
   lead to artificial general intelligence (AGI). 
   However, recent benchmarks have shown that GPTâ€‘4.5â€”the largest and most advanced OpenAI chat modelâ€”has underperformed compared
   to several smaller models. 
   For example, while DeepSeekâ€‘V3 scores 39.2% Pass@1 accuracy on AIME 2024 and 42% on SWEâ€‘bench Verified, GPTâ€‘4.5 scores only 36.7% 
   and 38%, respectively.

   This performance gap raises the question: do we need a better LLM architecture to scale further?
   Researchers believe we do. A promising candidate, called FANformer, is built by integrating a Fourier Analysis Network (FAN) 
   into the traditional Transformerâ€™s attention mechanism. 
   Experimental results are very promisingâ€”FANformers consistently outperform standard Transformers when scaling up model size 
   and training tokens. 
   For instance, a FANformer with 1 billion parameters outperforms other openâ€‘source LLMs of comparable size and training tokens.

2. The Role of Fourier Analysis Networks (FANs)
   Traditional deep neural networks and multi-layer perceptrons (MLPs) excel at capturing patterns from training data, 
   but they struggle with modeling periodicityâ€”hidden cyclical patterns that are common in many datasets. 
   Consider a simple example: a Transformer might struggle to learn a modulo operation or a sine function even with 
   ample training resources.

   FANs directly address this shortcoming. They use principles from Fourier Analysis to encode periodic patterns alongside 
   conventional linear and nonlinear transformations. 
   This means that a FAN layer can capture the cyclical nature of data (using sine and cosine functions) in a way that an ordinary
   MLP cannot.
   
   Mathematically, a FAN layer is defined as follows:
   FANLayer(ğ‘‹)=ğœ([ğ‘‹â‹…ğ‘Š(ğ‘)âˆ¥sinâ¡(ğ‘‹â‹…ğ‘Š(ğ‘Ë‰))+cos(ğ‘‹â‹…ğ‘Š(ğ‘Ë‰))+ğµ(ğ‘Ë‰)])
   where:
   -a. ğ‘‹ is the input,
   -b. ğ‘Š(ğ‘) and ğ‘Š(ğ‘Ë‰) are learnable projection matrices,
   -c. ğµ(ğ‘Ë‰) is the bias term,
   -d. ğœ represents a non-linear activation function,
   -e. âˆ¥ denotes concatenation.
   In contrast, a standard MLP layer performs only a linear transformation followed by a non-linear activation, 
   missing the opportunity to capture periodic features explicitly.

   Visual and mathematical comparisons in the referenced research papers show that FAN layers significantly improve periodicity 
   modelingâ€”this capability is central to FANformerâ€™s performance boost.

3. Building the Attention Mechanism in FANformer
   FANformer modifies the classic decoder-only Transformer architecture by replacing the standard attention module with
   the ATtention-Fourier (ATF) module. Hereâ€™s how it works: 
   -a. Step 1: Input Embedding and Fourier Transformation
               Given an input sequence ğ‘  of length ğ‘™, it is first mapped to an input embedding:
               ğ‘‹^(0)={ğ‘¥(1),ğ‘¥(2),â€¦,ğ‘¥(ğ‘™)}
               Each input embedding ğ‘‹ is transformed using a modified FAN layer. In this step, the activation function 
               ğœ in the FAN layer is replaced by an identity function (ğœ(ğ‘¥)=ğ‘¥ for the Fourier transformation:
               ğ‘‹^(ğ¹)=FANLayerâ€²(ğ‘‹)
   -b. Step 2: Computing Query, Key, and Value
               Next, the transformed embedding ğ‘‹^(ğ¹) is passed through linear layers to compute the query (ğ‘„), key (ğ¾), 
               and value (ğ‘‰) matrices:
               ğ‘„=ğ‘‹^(ğ¹)â‹…ğ‘Š(ğ‘„), ğ¾=ğ‘‹^(ğ¹)â‹…ğ‘Š(ğ¾), ğ‘‰=ğ‘‹^(ğ¹)â‹…ğ‘Š(ğ‘‰)
               where ğ‘Š(ğ‘„), ğ‘Š(ğ¾), and ğ‘Š(ğ‘‰) are learnable weight matrices.
   -c. Step 3: Scaled Dot-Product Attention
       The standard scaled dot-product attention is then computed:
       Attention(ğ‘„,ğ¾,ğ‘‰)=softmax((ğ‘„â‹…ğ¾^âŠ¤)/np.root(ğ‘‘(â„)))â‹…ğ‘‰
       Here, ğ‘‘(â„) is the modelâ€™s hidden dimension. Note that ATF(X) is mathematically equivalent to:
       Attention(FANLayerâ€²(ğ‘‹))
       This shows that the Fourier transformation enriches the input representations without changing the fundamental attention
       mechanism.

   -d. Multi-Head ATF
       FANformer extends the ATF module to a multi-head setup. For a given input ğ‘‹, it is projected into 
       ğ‘˜ independent heads:
       ğ‘„^(ğ‘–)=ğ‘‹â‹…ğ‘Š(ğ‘„)^(ğ‘–), ğ¾^(ğ‘–)=ğ‘‹â‹…ğ‘Š(ğ¾)^(ğ‘–) , ğ‘‰^(ğ‘–)=ğ‘‹â‹…ğ‘Š(ğ‘‰)^(ğ‘–)
       where ğ‘‘(ğ‘˜)=ğ‘‘(â„)/ğ‘˜ is the dimension per head. 
       The outputs of all heads are concatenated and passed through a final linear transformation using an output weight matrix ğ‘Š(ğ‘‚)
       A hyperparameter ğ‘ (defaulted to 0.25) controls the proportion of ğ‘‹ processed by the periodic vs. non-periodic components
       within the FAN layer.

4. Stacking Up to a FANformer
   A FANformer is built by stacking ğ‘ FANformer layers, where each layer includes:
   -a. Multi-head ATF Module: 
       Processes the input through the modified attention mechanism described above.
   -b. Feedforward Network (FFN) Module:
       Applies a two-layer transformation with the SwiGLU activation function:
       SwiGLU(ğ‘¥)=(ğ‘¥â‹…ğ‘Š(1))âŠ—ğœ(ğ‘¥â‹…ğ‘Š(2))
       where ğ‘Š(1), ğ‘Š(2), and ğ‘Š(3) are learnable weight matrices and âŠ— denotes element-wise multiplication.
   -c. Residual Connections and Pre-Normalization:
       Each layerâ€™s input ğ‘‹(ğ‘›) is normalized, processed through the Multi-head ATF and FFN modules, 
       and then added back to the original input (residual connection).

   This stacking creates a deep model where each layer refines the conceptual representations through periodic transformations,
   making FANformer both efficient and powerful in capturing long-range dependencies and periodic patterns.

5. Experimental Results
   -a. Scaling Experiments:
       -1. FANformer consistently outperforms standard Transformers across all model sizes.
       -2. A FANformer with 1 billion parameters shows superior performance compared to other open-source models of similar scale, 
           while using only 69.2% of the parameters of a standard Transformer.
       -3. Additionally, FANformer requires 20.3% fewer training tokens to achieve comparable performance, 
           demonstrating enhanced efficiency.
   -b. Downstream Tasks:
       -1. FANformer-1B was evaluated on eight benchmarks including ARC-C, ARC-E, BoolQ, HellaSwag, OBQA, PIQA, SCIQ, and WinoGrande.
       -2. FANformer-1Bâ€™s zero-shot performance outperforms many comparable models, even matching the performance of 
           state-of-the-art models around the 1B parameter mark.
       -3. In mathematical reasoning tasks, FANformer shows a distinct advantage by learning underlying rules rather than 
           relying solely on memorization, as demonstrated in tasks like modular addition and linear regression.
   -c. Training Dynamics:
       -1. Initially, FANformerâ€™s loss decreases more slowly than that of a standard Transformer, 
           likely due to the model learning to capture periodic patterns.
       -2. Over time, FANformer converges faster and exhibits better generalization on unseen data.
   -d. Instruction-Following with SFT:
       -1. When further fine-tuned on instruction-following data (e.g., tulu-3-sft-olmo-2-mixture), 
           FANformer-1B-SFT outperforms OLMo-1B-SFT on benchmarks like MMLU, TruthfulQA, and AlpacaEval.

6. Conclusion
   FANformer represents a promising advancement in LLM architecture by integrating Fourier Analysis Networks (FANs) into 
   the Transformerâ€™s attention mechanism. 
   By capturing periodic patterns explicitly and efficiently, FANformer scales better with model size and training tokens 
   while achieving superior performance on both general and specialized tasks. 
   It demonstrates that architectural modificationsâ€”rather than just scalingâ€”can unlock significant improvements in efficiency 
   and capability.

   The experimental results highlight that FANformer-1B, despite using fewer parameters and training tokens, 
   consistently outperforms traditional Transformers and even rivals state-of-the-art models in many cases. 
   This makes FANformer a strong candidate for future large-scale language models, particularly in applications requiring
   robust generalization and efficient computation.


