### From https://medium.com/@yujiisobe/from-bayesian-mmm-to-neural-nested-networks-tackling-brand-and-creative-effects-in-marketing-mix-50c56ecde39f
### From https://colab.research.google.com/gist/yujiosaka/0f14bc845e72c696b13ecf96a5b4688b/nnn_for_mmm.ipynb#scrollTo=6833db41

1. Why classic Bayesian MMMs hit a wall
   Limitation	| Symptoms in practice
   No native funnel support	| You must hand-build extra layers to capture media → search → sales or similar cascades.
   Channel-only granularity	| Inputs collapse to weekly spend/impressions per channel. Creative, audience, brand-health signals get lost.
   Additivity assumption	| Channel curves just add; synergy or saturation across channels requires manual interaction terms.
   Manual priors & slow MCMC	| Complex hierarchical builds → fragile convergence, long runtimes.

2. Neural Nested Networks (NNN): what they introduce (Apr-2025)
   Innovation	| Why it matters
   Factored self-attention (Time × Channel) | Learns (i) long-memory adstock within each channel, (ii) cross-channel interactions via a channel-mix MLP.
   Dual output heads (search, sales) with residual fusion	| The network learns how much of a channel’s lift is direct vs. funnelled through branded search.
   Embedding inputs	| Any categorical/qualitative info (creatives, campaign metadata, brand intent vector) plugs in as dense vectors.
   Non-additive modelling “for free”	| Attention + MLP capture synergy or diminishing returns; no need to enumerate interaction terms.
   Fast deterministic training	| Adam/SGD on GPU; can transfer-learn across brands or industries.

  -a. Architecture sketch:
      Spend / Creative embeds ─┬─► time-wise self-attention (per channel, causal mask)
                               │
     Brand-intent embed  ──────┘        ↓ residual
                 (week t)               ↓
                      ──► Channel-mix MLP (per week)
                               │
                               ├─► Head_search  → searcĥ_t
                               └─► Head_sales   → sales_direct_t
     Final saleŝ_t = σ(w_d)·sales_direct_t
                    + σ(w_s)·γ·searcĥ_t

3. Prototype build (in the Colab notebook)
   -a. Synthetic data: 2 years × 47 geos; four paid channels drive sales directly and indirectly via search.
       Realistic pieces: adstock, Hill saturation, seasonal organic search baseline, stochastic brand-intent factor.
   -b. Brand-intent embedding: interpolate between two 256-d vectors (low_embed, high_embed) to represent weekly brand health; 
                               injected as an extra “channel”.
   -c. Input tensor to model: shape (geo, week, channel, 256) → channels = TV, YouTube, Taxi, Display, Brand-Intent.
   -d. NNNModel implementation
       -1. 3 Transformer blocks (n_heads=4, look-back=52 w).
       -2. Channel-mix MLP (hidden 64) with LayerNorm + Dropout(0.1).
       -3. Heads: head_search, head_sales; learnable fusion weights w_direct, w_search, scale γ.
       -4. Geo-level scale & bias embeddings for calibration.
   -e. Regularisation: Dropout, Smooth-L1 loss, L1 penalty on head weights.
   -f. Training: 500 epochs, Adam (1 e-3) + cosine scheduler → converged in minutes.

4. Results on the synthetic funnel
   -a. Search head matched ground-truth seasonality + media bumps (R²≈0.97).
   -b. Sales head reproduced sales with correct split: learned w_search≈0.65 vs. simulated 0.70.
   -c. Counterfactual “all-media-off” produced a clean baseline curve, proving the network didn’t misattribute baseline seasonality to media spend.

5 NNN vs. Bayesian MMM — operational trade-offs
  Aspect	| NNN	| Bayesian MMM
  Funnel depth	| Any depth via extra heads.  | Hierarchical layers; complex priors.
  Interactions	| Implicit via attention/MLP. | 	Manual interaction terms.
  Qualitative data	| Embeddings out of the box.	| Must pre-aggregate to numerics.
  Uncertainty	| Need bootstrap / MC-dropout. | 	Posterior intervals native.
  Speed / scale	| GPU minutes; transfer-learn.	| CPU/GPU-MCMC hours-days.
  Interpretability	| Need SHAP or ablation; residual fusion weights give direct/indirect split. | Direct coefficient read-off.

6. Take-aways
   -a. Expressiveness: NNN encodes causal funnel and cross-channel synergy without exploding the parameter count.
   -b. Data richness: opens MMM to text, image, categorical campaign data.
   -c. Effort shift: less time on priors & convergence, more on feature design and architecture tuning.
   -d. Still complementary: Bayesian MMM excels when uncertainty bands are mandatory or data are extremely sparse; 
                            NNN shines for richer, larger, multi-modal datasets.

Bottom line: NNN brings modern representation learning to marketing-mix problems, solving long-standing pain points
             (branded-search funnels, creative-level inputs, synergy) while trading off closed-form uncertainty for speed and flexibility.

