### From https://medium.com/chat-gpt-now-writes-all-my-articles/gated-residual-networks-a-modern-secret-weapon-for-tabular-deep-learning-7a8d247a01d1


from tensorflow.keras.layers import (Input, Dense, BatchNormalization,
                                     LayerNormalization, Activation, Add, Multiply)
from tensorflow.keras.models import Model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping


def make_callbacks():
    return [
        ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3,
                          verbose=1, min_lr=1e-6),
        EarlyStopping(monitor="val_loss", patience=10,
                      restore_best_weights=True, verbose=1)
    ]


def GRN(x, units, name="grn"):
    # transform
    z = Dense(units, name=f"{name}_z_dense")(x)
    z = BatchNormalization(name=f"{name}_z_bn")(z)
    z = Activation("elu", name=f"{name}_z_act")(z)
    # gate (GLU)
    g = Dense(units, activation="sigmoid", name=f"{name}_g_dense")(x)
    gated = Multiply(name=f"{name}_gated")([z, g])
    # skip-connection
    skip = x if x.shape[-1] == units else Dense(units, name=f"{name}_skip")(x)
    y = Add(name=f"{name}_add")([skip, gated])
    # norm + non-linearity
    y = LayerNormalization(name=f"{name}_ln")(y)
    return Activation("swish", name=f"{name}_act")(y)


def build_grn_mlp(input_dim, hidden_units=(128, 128, 128, 128)):
    inp = Input(shape=(input_dim,), name="input")
    x = inp
    for i, u in enumerate(hidden_units):
        x = GRN(x, u, name=f"grn{i}")
    out = Dense(1, name="output")(x)
    return Model(inp, out, name="GRN_MLP")
