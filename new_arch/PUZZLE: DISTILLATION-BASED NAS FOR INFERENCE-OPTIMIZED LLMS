### From https://arxiv.org/pdf/2411.19146

## When we use this? - can find out right LLM algorithm
Youâ€™d reach for Puzzle anytime youâ€™ve got a hefty, off-the-shelf LLM and you need to squeeze it down to a real-world inference
budget without retraining from scratch. Concretely:

-a. Hardware-Specific Deployment
    -1. Youâ€™ve chosen a GPU (e.g. H100, A100, edge device) with fixed memory and compute, and you need maximum throughput 
        or minimum latency under those limits.
-b. Closed-Data, Open-Weights
    -1. You have access to the modelâ€™s weights but not its original training data. Puzzle thrives in that â€œopen-weights, 
        closed-dataâ€ scenario by distilling from the parent.
-c. Multiple Usage Profiles
    -1. You want different variants for:
        -1) High-batch, long-context (max throughput)
        -2) Low-batch, short-context (min latency)
        -3) Mixed workloads (balanced)
-d. Resource Constraints
    -1. You must fit within a strict memory cap (including KV-cache), or meet a throughput minimum, or cap end-to-end latency
        for interactive applications.
-e. Rapid Iteration on Architecture
    -1. Rather than training dozens of custom models from scratch, Puzzle lets you explore thousands of layer-level tweaks in parallel, 
        then instantly solve for the best combination via MIP.
-f. Tiered Developer Needs
    -1. From â€œquick proof-of-conceptâ€ with minimal extra tokens (BLD only) to â€œproduction-readyâ€ with final Global KD,
        Puzzle scales your investment to the importance of the deployment.

In short, use Puzzle when you need to turn a powerful parent LLM into a lean, hardware-tuned child that hits 
your exact inference targetsâ€”fast, memory-safe, and without the exorbitant cost of retraining.

---------------------------------------------------------------------------------
1. Introduction & Background
   -a. Trend & Challenge
       -1. LLMs continually grow (in both parameters and inference complexity) to boost accuracy, driving toward AGI.
       -2. Problem: Huge models are prohibitively expensive at inferenceâ€”high GPU memory use, slower latency, 
                    limited scalability for realâ€world deployment.
   -b. Overparameterization During Training vs. Inference
       -1. Large parameter counts help training converge and store knowledge but leave redundant computations at inference.
       -2. Existing â€œefficiencyâ€ hacks (pruning, quantization, etc.) treat the symptom rather than the uniformâ€layer design itself.
   -c. Puzzleâ€™s Goal
       -1. Reconfigure a trained â€œparentâ€ LLM into a nonâ€uniform, hardwareâ€aware â€œchildâ€ model optimized for inferenceâ€”without 
           retraining from scratch or needing the original training data.
       -2. Explore a vast design space of perâ€layer alternatives, then automatically assemble the best combination under given
           hardware and performance constraints.
   -d. Key Steps
       -1. Define a rich search space of attention & FFN layer variants (incl. skipping layers).
       -2. Blockwise Local Distillation (BLD): Train all those layer variants in parallel to mimic the parent.
       -3. Mixedâ€Integer Programming (MIP) Search: Score each variantâ€™s accuracy & resource cost,
                                                   then solve a knapsackâ€style MIP to pick one variant per layer that meets memory/latency/throughput targets.
       -4. Global Knowledge Distillation (GKD): Fineâ€tune the assembled child model endâ€toâ€end to smooth out interâ€block mismatches.
   -e. Why Itâ€™s Practical
       -1. Cost: < 50 B tokens for BLD+GKD vs. > 15 T tokens for original training.
       -2. Data: Only needs parent model weightsâ€”perfect for â€œopenâ€weights, closedâ€data.â€
       -3. Results: Derived Nemotron-51B (a 51 Bâ€param child of Llama-3.1-70B) that breaks the efficiency frontier on 
                    an NVIDIA H100â€”much higher throughput/memory efficiency at near-parent accuracy.

2. Search Space
   -a. Blocks & Subblocks
       -1. A block = one transformer layer.
       -2. Two subblocks per layer:
           -1) Attention variants (multiâ€head, grouped-query with 8/4/2/1 KV heads, singleâ€linear, or noâ€op).
           -2) FFN variants (full or pruned intermediate dims at 87%, 75%, 50%, 25%, 20%, 10%; linearâ€only; or noâ€op).
   -b. Scale
       -1. For Llama-3.1-70B: 80 layers Ã— (6 attention Ã— 9 FFN) = 54 choices per layer.
       -2. Total theoretical architectures: 54â¸â° â‰ˆ 10Â¹Â³â¸â€”vastly infeasible to bruteâ€force.

3. Blockwise Local Distillation (BLD)
   -a. Purpose: Create a library of trained â€œpuzzle piecesâ€ (all attention & FFN variants) without retraining full child models.
   -b. Decoupled vs. Coupled
       -1. Coupled BLD: train every combined [Aáµ¢â±¼, Fáµ¢â‚–]áµ¢ â†’ mÂ·nÂ·l variants.
       -2. Decoupled BLD: separately train each Aáµ¢â±¼ with frozen parent FFN, and each Fáµ¢â‚– with frozen parent attention â†’ (m + n)Â·l variants.
       -3. Benefit: decoupled cuts training count by roughly mÃ—n, enabling blockâ€level parallelism.
   -c. Training Details
       -1. Data: â€œDistillation Mixâ€ of ~1 B tokens (code, wiki, books, news).
       -2. Loss: normalized MSE between parentâ€block outputs (oâ‚š) and childâ€block outputs (o_c):
                 ğ¿=MSE(ğ‘œ_ğ‘,â€‰ğ‘œ_ğ‘) / MSE(ğ‘œ_ğ‘,â€‰0)
       -3. Initialization:
           -1) FFN pruning via Channel Contribution (prune lowâ€impact channels).
           -2) Attention head reduction by meanâ€pooling KV projections.
           -3) Linear replacements via projectionâ€matrix products.

4. Decomposed NAS Search Algorithm
   -a. Block Library (from BLD)
   -b. Resource Estimation
       -1. Measure each variantâ€™s prefill/generation latencies and memory (parameters + KVâ€cache) on target hardware 
           (e.g. H100, A100) across batch sizes & sequence lengths.
   -c. Block Scoring
       -1. Replaceâ€1â€block test: swap one variant into the parent, measure impact on a validation metric
                                 (LM loss, KL divergence, or downstream accuracy). Record each variantâ€™s score(i,j).
   -d. MIP Formulation
       -1. Variables: xáµ¢â±¼ âˆˆ {0,1}: pick variant j for layer i.
       -2. Objective: maximize Î£áµ¢Î£â±¼ score(i,j)Â·xáµ¢â±¼.
       -3. Constraints:
           -1) Total memory (params + batchâˆ™KV) â‰¤ Memoryâ‚˜â‚â‚“.
           -2) Throughput (batchâˆ™seq_len / Î£ runtimes) â‰¥ Throughputâ‚˜áµ¢â‚™.
           -3) Latency Î£ runtimes â‰¤ Latencyâ‚˜â‚â‚“.
           -4) Exactly one variant per layer.
   -e. Diversity: optional constraint to force new solutions to differ â„“% from prior ones.
   Outcome: Fast (< seconds) discovery of nonâ€uniform architectures optimized for realâ€world inference constraints.

5. Post-Puzzle Inter-Block Uptraining (Global KD)
   -a. Why: BLD trains blocks on parent activations, so child blocks may misâ€match each other when assembled.
   -b. How: One final round of endâ€toâ€end distillation (âˆ¼ 5 B tokens) using the parent as teacher.
       -1. Loss: Cosine similarity of hidden states + KL divergence on logits (no crossâ€entropy LM loss).
       -2. Result: Child model regains nearâ€parent performance; LM loss was found unnecessary or harmful in ablations.

6. Fast Inference in TensorRT-LLM
   -a. Challenge: TensorRT-LLM assumed uniform KVâ€head counts across layers.
   -b. Solution:
       -1. Paged KV cache enhancements to handle variable GQA ratios.
       -2. Full support for FP8 quantization on weights, activations, and KV cache.
       -3. Maintains TensorRT-LLMâ€™s highâ€performance kernels (paged attention, scheduling) for Puzzle architectures.

In Essence: Puzzle turns a monolithic, uniform LLM into a tailored, hardwareâ€aware model by:
-a. Splitting layers into many efficient variants,
-b. Distilling them blockwise,
-c. Scoring & solving a resourceâ€constrained selection problem with MIP,
-d. Fineâ€tuning the assembled model with minimal distillation, and
-e. Running it seamlessly on optimized inference enginesâ€”yielding massive efficiency gains at minimal extra cost.

