### From https://medium.com/@aipapers/perception-language-models-plms-by-meta-explained-1734f7009a21

1. Introduction & Motivation
   -a. Vision-language models (VLMs) are critical for multimodal AI tasks such as:
       -1. Image captioning
       -2. Visual question answering (VQA)
       -3. Video understanding
       -4. Multimodal reasoning
   -b. Problem with Existing VLMs:
       -1. Most high-performing VLMs are closed-source.
       -2. Open-source VLMs often rely on knowledge distillation from these proprietary models.
           -1) Closed models produce pseudo-labels (captions, answers) → used to train open models.
           -2) Result: open-source VLMs are trained to mimic, not innovate.
   -c. Motivation of the Paper:
       Meta introduces Perception Language Models (PLMs), trained from scratch using only open components and no dependence on black-box 
       vision-language models, aiming to:
       -1. Advance open research in multimodal learning
       -2. Eliminate dependence on closed-source supervision

2. High-Level Architecture of Perception Language Models (PLM)
   -a. Key Components:
       Component | Description
       LLM Backbone	| LLaMA 3.2 (1B, 3B) or LLaMA 3.1 (8B) used as the language core
       Vision Encoder (PE)	| Pretrained model used to extract embeddings from images/videos
       Projector (MLP)	| Two-layer MLP that projects visual embeddings into the LLM embedding space
   -b. Multimodal Integration Pipeline:
       -1. Image/video → PE → visual embeddings
       -2. Visual embeddings → Projector → LLM-compatible embeddings
       -3. Text prompt → Tokenizer → token embeddings
       -4. Concatenated embeddings → injected into LLM
           -1. Enables unified processing of multimodal input
           -2. Supports reasoning across vision and text

3. Model Variants and Scaling
   PLM Size	| Language Model	| Vision Encoder
   PLM-1B	| LLaMA 3.2 (1B)	| PE-L (0.3B)
   PLM-3B	| LLaMA 3.2 (3B)	| PE-L (0.3B)
   PLM-8B	| LLaMA 3.1 (8B)	| PE-G (1.9B)

4. High-Resolution Support via Dynamic Tiling
   -a. Challenge:
       -1. Vision encoders have resolution limits.
   -b. Solution:
       -1. Dynamic tiling:
           -1) Input image is split into smaller tiles if resolution exceeds threshold
           -2) Each tile is downsampled via 2×2 average pooling
       -2. Same applies to video:
           -1) 32 frames per video
           -2) Pooling applied frame-wise
   This enables PLM to process high-resolution inputs with limited GPU memory.

5. PLM Training Process: Three Stages
   -a. Stage 1: Warmup on Synthetic Image-Only Data
       -1. Objectives:
           -1) Train the projector only
           -2) Keep vision encoder and LLM frozen
           -3) Stabilize training with limited parameters
      -2. Dataset:
          -1) 1M images (low resolution, no tiling needed)
          -2) Each image is paired with synthetic captions
      -3. Synthetic Caption Generation: 
          -1) No closed VLM is used.
          -2) Captions generated by text-only LLM
              - Input: metadata, original captions, and OCR-extracted text from the image
              - Avoids dependency on black-box vision supervision

   -b. Stage 2: Midtraining on Synthetic Image & Video Data
       -1. Objectives:
           -1) Unfreeze all components (PE, Projector, LLM)
           -2) Jointly train the full system
       -2. Dataset:
           -1) 64.7M multimodal samples
               - Images and videos
               - Includes synthetic questions and answers generated by LLMs
       -3. Processing:
           -1) Images: up to 16 tiles
           -2) Videos: up to 16 frames
           -3) Pooling applied to all tiles/frames
      -4. Limitation:
          -1) On hard video QA, error remains high throughout training
              - Indicates synthetic data is insufficient for fine-grained temporal understanding

   -c. Stage 3: Supervised Fine-Tuning (SFT) on Human-Annotated Data
       -1. Objectives:
           -1) Final model tuning using real, diverse, high-quality annotated data
       -2. Dataset:
           -1) 14M human-annotated samples
       -3. Includes:
           -1) Complex video Q&A
           -2) Event timestamp annotation
           -3) Fine-grained video understanding tasks
       -4. Processing:
           -1) Images: up to 36 tiles
           -2) Videos: 32 frames, pooled
       -5. Training Objective:
           -1) Shift from next-token prediction → instruction-following / supervised generation
               - Predict full answer given question and visual context
       -6. Example Tasks:
           -1) Fine-grained spatial-temporal QA (e.g., direction of paper fold)
           -2) Event annotation tasks:
               - Given video + partial annotations → predict missing info (e.g., assign timestamps, generate captions)

6. Summary of Contributions
   -a. Open-source, from-scratch VLM with no closed-model distillation
   -b. Three-stage training pipeline:
       -1. Warmup on frozen components with synthetic captions
       -2. Joint training with synthetic QA pairs
       -3. Final tuning on human-annotated, high-complexity data
   -c. Support for high-resolution multimodal input via dynamic tiling
   -d. Human-annotated video reasoning dataset with challenging benchmarks
   -e. Competitive performance across vision-language tasks


