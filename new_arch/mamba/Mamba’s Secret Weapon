### From https://medium.com/@swarnenduiitb2020/mambas-secret-weapon-the-mathematical-elegance-of-the-parallel-associative-scan-e9617f2644fa

0. Overview: “The RNN is back—and faster than a Transformer”
   -a. Claim: Mamba (a selective, input-dependent SSM) parallelizes a seemingly sequential recurrence via an associative prefix scan, 
              delivering O(n log n) parallel time (with total work O(n)) and beating Transformer attention’s O(n²) in speed and memory on long sequences.
   -b. Three pillars:
       -1. Input-dependent (selective) SSM: Δ, B, C depend on x_t
       -2. Associativity → parallel scan to break recurrence
       -3. Hardware-aware placement of hot tensors in SRAM

1. Traffic-light parable & the bottleneck
   -a. Plain RNN: process tokens one by one → O(n) with poor GPU parallelism.
   -b. Transformer: compute all pairwise interactions → O(n²).
   -c. Goal: keep sequential semantics but achieve parallel throughput → enter Mamba.

2. State Space Models (SSMs) 101
   -a. Continuous-time control:
       h’(t) = A h(t) + B x(t), y(t) = C h(t) + D x(t)
   -b. ZOH discretization:
       h_t = Ā h_{t-1} + B̄ x_t, y_t = C h_t + D x_t
       with Ā = exp(AΔ), B̄ = (exp(AΔ) − I) A⁻¹ B
   -c. Classic limitation: fixed A, B, C, D → no context adaptivity.

3. Mamba: selective (input-dependent) SSM
   -a. Make parameters depend on input:
       Δ_t = f_Δ(x_t), B_t = f_B(x_t), C_t = f_C(x_t)
       → adaptive memory length & input injection strength.
   -b. But input dependence breaks the “convolution trick” that enabled full parallelization for fixed SSMs.

4. The associative-scan trick
   -a. Recurrence: h_t = Ā_t h_{t-1} + B̄_t x_t
   -b. Bundle as pairs (Ā, b) and define associative operator ⊗:
       (A₂, b₂) ⊗ (A₁, b₁) = (A₂A₁, A₂ b₁ + b₂)
   -c. Then computing all h_t reduces to an associative prefix scan over {(Ā_t, B̄_t x_t)}.
   -d. Complexity:
       -1. RNN: O(n) sequential
       -2. Transformer: O(n²)
       -3. Mamba: O(n) total work with O(log n) parallel depth via scan primitives

5. Hardware-aware implementation
   -a. Memory hierarchy: SRAM (fast, small) vs HBM (large, slow).
   -b. Place hot state and transition (h, Ā) in SRAM; stream Δ, B, C, x through HBM.
   -c. Core kernel = selective scan + tensorized cumsums; minimizing data motion yields 5–10× speedups at long sequence lengths.

6. Practical impact
   -a. Inference speed (e.g., n=2048): Transformer needs O(n²) attention scores; Mamba updates states in parallel → 5–10× faster.
   -b. Memory footprint: Transformer stores O(n² d); Mamba stores O(n d) state → single-GPU feasibility for n=100k contexts.
   -c. Long-context scaling: Mamba scales linearly; Transformer degrades quadratically.

7. Trade-offs and open questions
   -a. ZOH discretization error: piecewise-constant assumption can introduce error for rapidly varying inputs; higher-order methods cost more. 
       Empirically acceptable so far; still an open research point.
   -b. Global random access: arbitrary cross-document lookups may favor attention’s all-to-all visibility.
   -c. Hardware dependence: speedups rely on efficient scans, SRAM residency, low-latency matmuls; gains shrink on non-optimized stacks.

8. Geometric intuition
   -a. h_t traces a trajectory in ℝ^d:
       -1. fixed SSM → fixed dynamics; Mamba → input-conditioned dynamics.
   -b. Attractor view: good SSMs learn meaningful attractors (tense, subject/object, discourse). Mamba’s selectivity activates different attractors based on content.

9. Bottom line (math, not hype)
   -a. Mamba = control theory (SSMs) + parallel algorithms (associative scan) + systems engineering (SRAM/HBM orchestration). 
   -b. Net effect: a sequence model that is faster, more memory-efficient, and linearly scalable with sequence length—while retaining recurrent inductive bia










