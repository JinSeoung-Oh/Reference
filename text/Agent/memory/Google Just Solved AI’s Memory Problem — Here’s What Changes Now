### From https://generativeai.pub/google-just-solved-ais-memory-problem-here-s-what-changes-now-d52bc7fac3f9

1. Problem statement: the fundamental limitation of AI that forgets the beginning of long conversations
   There is a fatal structural flaw in every conversation we have had with large language model assistants so far.
   It is not hallucinations or simple mistakes. Rather, it is the fact that as the conversation grows longer, 
   the model gradually loses the information from the beginning. 
   This is not due to poor design, but due to a fundamental constraint of the Transformer architecture itself.
 
   For example, if you make an AI read an entire novel and then ask questions about a character mentioned early on, 
   by the later chapters that character becomes a vague presence. 
   This does not happen because the model “forgets” like humans do, but because it cannot keep the early context due to computational and memory limits.

2. The structural limitation of Transformers
   Transformers use self-attention, which compares every token with every other token.
   As a result, the computational complexity grows quadratically (O(n²)) with context length.
   -a. 10 words → 100 comparisons
   -b. 100 words → 10,000 comparisons
   -c. 100,000 words → 10 billion comparisons
   Maintaining long contexts therefore requires enormous resources, and in practice the system must truncate or compress earlier context. 
   This is why ChatGPT, Claude, and Gemini often lose coherence in extended conversations.

   Prior attempts fell into two categories:
   -a. Compress everything into a fixed-size memory → loss of critical detail
   -b. Brute-force longer context windows → unrealistic computational cost
   Neither solves the root problem.

3. Titans: a three-layer memory architecture inspired by the human brain
   DeepMind’s Titans does not simply “store more.” It introduces a structure that remembers intelligently.
   The core idea is to directly adopt a three-layer memory architecture, mirroring how human memory works.

   3.1 Long-Term Memory Module
       This top layer preserves and learns long-range context and patterns.
       -a. Traditional models: simple vector memories
       -b. Titans: MLP-based memory (a neural network inside the neural network)
       This memory does not passively record information. Instead, it learns patterns and forms connections between concepts.

       A key mechanism is the “surprise metric”:
       -a. routine content is filtered out
       -b. unexpected or important information is prioritized
       This mirrors human memory, which forgets mundane events but remembers unusual ones.

   3.2 Core Attention Layer
       This middle layer uses the standard attention mechanism to function as short-term “working memory.”

       It handles:
       -a. recent utterances
       -b. immediate question answering
       -c. exact recall of nearby context

       Questions like “What was the last word I said?” are answered by this layer.
       It avoids attention’s exponential burden by choosing selectively between:
       -a. the long-term memory’s compressed “summary reports”
       -b. the current immediate context

   3.3 Persistent Memory Layer
       This is the unchanging base knowledge that does not update during conversation:
       -a. grammar and linguistic competence
       -b. common sense
       -c. semantic relationships
       -d. general knowledge learned during training
       It is analogous to “instinctive” knowledge in humans.

4. MIRAS: a theoretical framework that unifies sequence modeling architectures
   MIRAS is closer to a discovery than an implementation.
   It shows that Transformers, RNNs, Mamba, and related architectures are all solving the same underlying problem: 
     associative memory, but through different design choices.

   4.1 Four key dimensions MIRAS identifies
       -a. Memory Architecture
           -1. whether memory is represented as vectors, matrices, neural networks, etc.
       -b. Attentional Bias
           -1. what is prioritized as important
       -c. Retention Gate (forgetting mechanism)
           -1. what is discarded
           -2. forgetting is as essential as remembering
       -d. Memory Algorithm
           -1. the mathematical rule for integrating new information into memory

5. A core insight from the loss-function perspective
   Most successful models have relied on MSE (Mean Squared Error).
   -a. benefit: simple and effective
   -b. drawback: high sensitivity to outliers

   MIRAS enables systematic exploration of alternatives via experimental models:
   -a. YAAD: uses Huber loss → more robust to errors
   -b. Moneta: enforces stricter mathematical constraints → stability exploration
   -c. Memora: forces memory updates to behave like probability distributions → prevents chaotic states
   This opens a path toward test-time adaptation, where models learn during inference.

6. Performance results: collapse of baselines vs stability of Titans
   6.1 Perplexity across long sequences
       -a. Mamba degrades as sequence length grows
       -b. Titans remains stable and low even as sequences extend

       This is framed as the difference between shallow and deep memory:
       -a. shallow: “what happened”
       -b. deep: “why it matters and how it connects”

   6.2 BabaLong benchmark
       BabaLong hides facts across documents exceeding two million tokens, then tests retrieval and reasoning.
       -a. GPT-4 collapses sharply as length increases
       -b. Titans maintains strong performance even at extreme lengths
       A key point: Titans is substantially smaller than GPT-4, and the gains come from architecture rather than scale.

7. What this means for real applications
   Titans makes previously theoretical applications practically viable:
   -a. legal: contracts and precedents spanning hundreds of pages
   -b. medical: decades-long patient histories
   -c. scientific research: tracking massive bodies of literature
   -d. software: understanding large codebases
   -e. personal AI: remembering months of interactions

8. What this means for AGI
   Memory is not an optional feature; it is a core component of intelligence.
   Titans and MIRAS point to a shift away from “bigger models” toward better memory organization.

   This directly relates to overcoming catastrophic forgetting, and suggests a practical route toward lifelong learning.

9. Hope and Nested Learning: the next step
   A limitation of Titans is that it stays within a two-level learning structure.
   DeepMind therefore proposes Hope, based on a more ambitious paradigm called Nested Learning.

   Hope introduces a spectrum of memory modules with different update speeds:
   -a. fast-updating modules
   -b. medium-speed consolidation modules
   -c. slow-updating stable modules
   This mirrors brain findings about memory consolidation during wakefulness and sleep.
   Hope is presented as a system that can improve its own learning rules — a model that learns how to learn.

10. Conclusion
    Titans and MIRAS are not incremental improvements. They represent a structural turning point for:
    -a. long-context understanding
    -b. continual learning
    -c. self-adaptation
    -d. overcoming catastrophic forgetting
    They argue that on the path toward AGI, the key is not “bigger models,” but better memory architecture.
