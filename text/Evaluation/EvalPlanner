### From https://medium.com/@techsachin/evalplanner-a-thinking-llm-as-a-judge-model-that-learns-to-think-by-planning-and-reasoning-for-b7537822970d

1. Background & Motivation
   -a. Problem Statement:
       LLM-as-a-Judge models are designed to generate CoT sequences that reveal the step-by-step reasoning behind
       a final evaluation. However, there are two major challenges:
       -1. Data Scarcity: There is a lack of human-annotated CoTs, making it difficult to train models that reason explicitly.
       -2. Predefined Prompts Limitation: Relying on fixed evaluation prompts lowers accuracy, 
                                          and manually tweaking these instructions isnâ€™t scalable given the vast diversity of tasks.
   -b. Solution Overview:
       EvalPlanner introduces a preference optimization algorithm that breaks the evaluation process into three sequential components:
       -1. Unconstrained Evaluation Plan (z)
       -2. Plan Execution (e)
       -3. Final Verdict (y)
           This separation allows the model to first decide â€œhowâ€ to evaluate before actually executing the reasoning process,
           and finally arriving at a judgment.

2. Method Components
   -a. Evaluation Plan (z):
       -1. Purpose: 
           For a given instruction ğ‘¥, the evaluation plan outlines a â€œrecipeâ€ detailing the steps or criteria that will be used to 
           judge the quality of two responses.
       -2. Generation:
           A seed LLM (instruction-tuned) is prompted with a generic, unconstrained prompt asking for an evaluation plan. 
           The plan is demarcated by specific tags (e.g., â€œ[Start of Evaluation Plan]â€ and â€œ[End of Evaluation Plan]â€).
   -b. Execution of the Plan (e):
       -1. Purpose:
           This component is responsible for following the plan step-by-step to compare a pair of responses ğ‘ and ğ‘ given the instruction.
       -2. Process:
           The same seed model is used with a tailored prompt that includes both the userâ€™s instruction and the response pair. 
           The model reasons through the generated evaluation plan and then produces the final evaluation.
       -3. Final Verdict (y):
           -1) Outcome:
               Based on the executed plan, the model produces a final judgment in a structured formatâ€”e.g., â€œ[[A]]â€ 
               if response A is better, or â€œ[[B]]â€ if response B is better.
   -c. Generative Process:
       The overall process is modeled as generating a final verdict ğ‘¦ given an instruction ğ‘¥ where both the plan ğ‘§ and its execution 
       ğ‘’ are latent variables.

3. Workflow Overview
   -a. Plan Sampling:
       For a given instruction and a seed model, multiple evaluation plans ğ‘§ are generated by sampling from a set ğ‘ƒ
   -b. Execution Sampling:
       For each generated plan and for a given pair of responses, multiple executions ğ‘’ are sampled from a set ğ¸
       Each execution might lead to either a correct or incorrect final verdict.
   -c. Self-Training Loop:
       -1. Using the generated data (pairs of plans, executions, and verdicts), a self-training loop is established. 
           This loop optimizes the LLM-as-a-Judge model over both the planning and execution stages.
       -2. At test time, the model outputs a complete CoT structured as ğ‘¦~=(ğ‘§~,ğ‘’~,ğ‘¦~)

4. Synthetic Training Data Generation
   -a. Prompt Selection & Response Pair Generation:
       -1. General Instruction-Following:
           -1) Original instructions are â€œnoisedâ€ to create a variant.
           -2) The response to the original instruction becomes the â€œchosenâ€ (correct) response, and the one to the noisy
               instruction becomes the â€œrejectedâ€ response.
       -2. Mathematical Reasoning:
           -1) Multiple candidate responses are generated; those that solve the problem correctly are marked as chosen, 
               while incorrect ones are rejected.
   -b. Generating Evaluation Plans:
       -1. A generic prompt (with placeholders like {instruction}) is used to have a seed model output an unconstrained evaluation plan.
       -2. This plan is later refined through self-training.
   -c. Generating Plan Executions:
       -1. The seed model is then prompted with both the instruction and a response pair, asking it to follow the earlier 
           generated plan and produce a verdict.
       -2. This step ensures that the model â€œreasonsâ€ through the plan in a step-by-step fashion.
   -d. Building Preference Pairs:
       -1. For each instruction, multiple plans (|P|) and, for each plan, multiple executions (|E|) are sampled.
       -2. To counteract order biases, the response pairs are considered in both orders (i.e., (a, b) and (b, a)), resulting in 
           2Ã—âˆ£ğ‘ƒâˆ£Ã—âˆ£ğ¸âˆ£ complete CoTs per instruction.
       -3. Each triplet (plan, execution, judgment) is labeled as correct or incorrect based on whether it leads to 
           the right verdict, and these labels form the basis of â€œchosenâ€ vs. â€œrejectedâ€ pairs for preference tuning.

5. Preference Optimization Pipeline
   The overall training pipeline consists of a self-training loop with the following stages:

   -a. Supervised Fine-Tuning (SFT) â€“ Model ğ‘€_1^(SFT)
       -1. Start from a seed model ğ‘€_0 and a subset ğ·_1^ğ‘ containing only the â€œchosenâ€ (correct) CoTs.
       -2. Fine-tune the model on these examples to enforce the correct pattern (plan + execution + verdict).
   -b. Direct Preference Optimization (DPO) â€“ First Iteration ğ‘€_1^(DPO)
       -1. Initialize from ğ‘€_1^(SFT) and perform DPO on the full dataset ğ·_1, which contains both chosen and rejected CoTs.
       -2. This step helps the model contrast correct and incorrect evaluation processes, learning to distinguish between 
           effective and flawed plans/executions.
   -c. Direct Preference Optimization â€“ Second Iteration ğ‘€_2^(DPO)
       -1. A fresh subset of instructions and response pairs is used.
       -2. The model (now ğ‘€_1^(DPO)) is used to generate new CoTs; these are then separated into plans and executions, 
           and a second round of DPO further refines the modelâ€™s performance.

6. Experimental Setup
   -a. Datasets:
       -1. WildChat Dataset:
           Uses synthetic responses generated by Self-Taught Evaluators.
       -2. MATH Dataset:
           Uses a Mixtral 22Bx8 Instruct model to produce multiple candidate solutions; correct answers are chosen while incorrect
           ones are rejected.
       -3. The training data comprises thousands of unique triples (instruction, chosen response, rejected response).
   -b. Training Configuration:
       -1. A subset of around 5K instructions (split equally between WildChat and MATH) is used for the initial SFT and 
           first DPO iteration, with the rest reserved for a second DPO iteration.
       -2. For each instruction, the model samples a fixed number of plans (e.g., 5) and for each plan, 
           a fixed number of executions (e.g., 8, balanced for response order) are generated.
       -3. Models like Llama-3.1â€“70B-Instruct or Llama-3.3â€“70B-Instruct serve as seed models to validate the generality 
           of the approach.
   -c. Baselines:
       EvalPlanner is compared against various baselines, including:
       -1. Zero-shot evaluation using powerful open-source and closed-source LLMs.
       -2. Reward models that generate scalar scores and critiques.
       -3. State-of-the-art generative reward models listed on the RewardBench leaderboard.

7. Key Benefits of the EvalPlanner Approach
   -a. Disentanglement of Reasoning Stages:
       -1. Separating the planning from execution enforces that the reasoning process follows a clearly defined evaluation plan.
   -b. Diversity in Training Data:
       -1. Sampling multiple plans and executions per instruction increases the diversity of training data, which helps in better generalization over varied tasks.
   -c. Preference-Based Optimization:
       -1. By constructing â€œchosenâ€ versus â€œrejectedâ€ preference pairs based on correctness, the model learns to contrast and improve both its planning and execution steps.
   -d. Self-Training Loop:
       -1. Iterative refinement through SFT and DPO stages gradually enhances the modelâ€™s ability to generate reliable and well-structured CoTs, ultimately leading to more accurate final judgments.

In summary:
EvalPlanner proposes a novel, structured approach for LLM-as-a-Judge models that generates an unconstrained evaluation plan,
executes that plan to assess response pairs, and then produces a final verdict.
This multi-step processâ€”combined with synthetic training data generation, preference pair construction, 
and iterative optimization via SFT and DPOâ€”allows the model to learn effective evaluation strategies without relying solely 
on manually designed prompts. The result is a more robust, scalable, and accurate method for generating chain-of-thought sequences\
that underpin final judgments in diverse and complex tasks.


