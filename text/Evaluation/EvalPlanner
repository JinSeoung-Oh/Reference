### From https://medium.com/@techsachin/evalplanner-a-thinking-llm-as-a-judge-model-that-learns-to-think-by-planning-and-reasoning-for-b7537822970d

1. Background & Motivation
   -a. Problem Statement:
       LLM-as-a-Judge models are designed to generate CoT sequences that reveal the step-by-step reasoning behind
       a final evaluation. However, there are two major challenges:
       -1. Data Scarcity: There is a lack of human-annotated CoTs, making it difficult to train models that reason explicitly.
       -2. Predefined Prompts Limitation: Relying on fixed evaluation prompts lowers accuracy, 
                                          and manually tweaking these instructions isn’t scalable given the vast diversity of tasks.
   -b. Solution Overview:
       EvalPlanner introduces a preference optimization algorithm that breaks the evaluation process into three sequential components:
       -1. Unconstrained Evaluation Plan (z)
       -2. Plan Execution (e)
       -3. Final Verdict (y)
           This separation allows the model to first decide “how” to evaluate before actually executing the reasoning process,
           and finally arriving at a judgment.

2. Method Components
   -a. Evaluation Plan (z):
       -1. Purpose: 
           For a given instruction 𝑥, the evaluation plan outlines a “recipe” detailing the steps or criteria that will be used to 
           judge the quality of two responses.
       -2. Generation:
           A seed LLM (instruction-tuned) is prompted with a generic, unconstrained prompt asking for an evaluation plan. 
           The plan is demarcated by specific tags (e.g., “[Start of Evaluation Plan]” and “[End of Evaluation Plan]”).
   -b. Execution of the Plan (e):
       -1. Purpose:
           This component is responsible for following the plan step-by-step to compare a pair of responses 𝑎 and 𝑏 given the instruction.
       -2. Process:
           The same seed model is used with a tailored prompt that includes both the user’s instruction and the response pair. 
           The model reasons through the generated evaluation plan and then produces the final evaluation.
       -3. Final Verdict (y):
           -1) Outcome:
               Based on the executed plan, the model produces a final judgment in a structured format—e.g., “[[A]]” 
               if response A is better, or “[[B]]” if response B is better.
   -c. Generative Process:
       The overall process is modeled as generating a final verdict 𝑦 given an instruction 𝑥 where both the plan 𝑧 and its execution 
       𝑒 are latent variables.

3. Workflow Overview
   -a. Plan Sampling:
       For a given instruction and a seed model, multiple evaluation plans 𝑧 are generated by sampling from a set 𝑃
   -b. Execution Sampling:
       For each generated plan and for a given pair of responses, multiple executions 𝑒 are sampled from a set 𝐸
       Each execution might lead to either a correct or incorrect final verdict.
   -c. Self-Training Loop:
       -1. Using the generated data (pairs of plans, executions, and verdicts), a self-training loop is established. 
           This loop optimizes the LLM-as-a-Judge model over both the planning and execution stages.
       -2. At test time, the model outputs a complete CoT structured as 𝑦~=(𝑧~,𝑒~,𝑦~)

4. Synthetic Training Data Generation
   -a. Prompt Selection & Response Pair Generation:
       -1. General Instruction-Following:
           -1) Original instructions are “noised” to create a variant.
           -2) The response to the original instruction becomes the “chosen” (correct) response, and the one to the noisy
               instruction becomes the “rejected” response.
       -2. Mathematical Reasoning:
           -1) Multiple candidate responses are generated; those that solve the problem correctly are marked as chosen, 
               while incorrect ones are rejected.
   -b. Generating Evaluation Plans:
       -1. A generic prompt (with placeholders like {instruction}) is used to have a seed model output an unconstrained evaluation plan.
       -2. This plan is later refined through self-training.
   -c. Generating Plan Executions:
       -1. The seed model is then prompted with both the instruction and a response pair, asking it to follow the earlier 
           generated plan and produce a verdict.
       -2. This step ensures that the model “reasons” through the plan in a step-by-step fashion.
   -d. Building Preference Pairs:
       -1. For each instruction, multiple plans (|P|) and, for each plan, multiple executions (|E|) are sampled.
       -2. To counteract order biases, the response pairs are considered in both orders (i.e., (a, b) and (b, a)), resulting in 
           2×∣𝑃∣×∣𝐸∣ complete CoTs per instruction.
       -3. Each triplet (plan, execution, judgment) is labeled as correct or incorrect based on whether it leads to 
           the right verdict, and these labels form the basis of “chosen” vs. “rejected” pairs for preference tuning.

5. Preference Optimization Pipeline
   The overall training pipeline consists of a self-training loop with the following stages:

   -a. Supervised Fine-Tuning (SFT) – Model 𝑀_1^(SFT)
       -1. Start from a seed model 𝑀_0 and a subset 𝐷_1^𝑐 containing only the “chosen” (correct) CoTs.
       -2. Fine-tune the model on these examples to enforce the correct pattern (plan + execution + verdict).
   -b. Direct Preference Optimization (DPO) – First Iteration 𝑀_1^(DPO)
       -1. Initialize from 𝑀_1^(SFT) and perform DPO on the full dataset 𝐷_1, which contains both chosen and rejected CoTs.
       -2. This step helps the model contrast correct and incorrect evaluation processes, learning to distinguish between 
           effective and flawed plans/executions.
   -c. Direct Preference Optimization – Second Iteration 𝑀_2^(DPO)
       -1. A fresh subset of instructions and response pairs is used.
       -2. The model (now 𝑀_1^(DPO)) is used to generate new CoTs; these are then separated into plans and executions, 
           and a second round of DPO further refines the model’s performance.

6. Experimental Setup
   -a. Datasets:
       -1. WildChat Dataset:
           Uses synthetic responses generated by Self-Taught Evaluators.
       -2. MATH Dataset:
           Uses a Mixtral 22Bx8 Instruct model to produce multiple candidate solutions; correct answers are chosen while incorrect
           ones are rejected.
       -3. The training data comprises thousands of unique triples (instruction, chosen response, rejected response).
   -b. Training Configuration:
       -1. A subset of around 5K instructions (split equally between WildChat and MATH) is used for the initial SFT and 
           first DPO iteration, with the rest reserved for a second DPO iteration.
       -2. For each instruction, the model samples a fixed number of plans (e.g., 5) and for each plan, 
           a fixed number of executions (e.g., 8, balanced for response order) are generated.
       -3. Models like Llama-3.1–70B-Instruct or Llama-3.3–70B-Instruct serve as seed models to validate the generality 
           of the approach.
   -c. Baselines:
       EvalPlanner is compared against various baselines, including:
       -1. Zero-shot evaluation using powerful open-source and closed-source LLMs.
       -2. Reward models that generate scalar scores and critiques.
       -3. State-of-the-art generative reward models listed on the RewardBench leaderboard.

7. Key Benefits of the EvalPlanner Approach
   -a. Disentanglement of Reasoning Stages:
       -1. Separating the planning from execution enforces that the reasoning process follows a clearly defined evaluation plan.
   -b. Diversity in Training Data:
       -1. Sampling multiple plans and executions per instruction increases the diversity of training data, which helps in better generalization over varied tasks.
   -c. Preference-Based Optimization:
       -1. By constructing “chosen” versus “rejected” preference pairs based on correctness, the model learns to contrast and improve both its planning and execution steps.
   -d. Self-Training Loop:
       -1. Iterative refinement through SFT and DPO stages gradually enhances the model’s ability to generate reliable and well-structured CoTs, ultimately leading to more accurate final judgments.

In summary:
EvalPlanner proposes a novel, structured approach for LLM-as-a-Judge models that generates an unconstrained evaluation plan,
executes that plan to assess response pairs, and then produces a final verdict.
This multi-step process—combined with synthetic training data generation, preference pair construction, 
and iterative optimization via SFT and DPO—allows the model to learn effective evaluation strategies without relying solely 
on manually designed prompts. The result is a more robust, scalable, and accurate method for generating chain-of-thought sequences\
that underpin final judgments in diverse and complex tasks.


