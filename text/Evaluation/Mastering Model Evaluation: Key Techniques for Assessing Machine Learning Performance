### From https://ai.plainenglish.io/mastering-model-evaluation-key-techniques-for-assessing-machine-learning-performance-78cbe7a42595

1. Overview
   Creating a good machine learning model is only the first step. Equally important is evaluating its predictive quality 
   and ensuring it generalizes well to new, unseen data. Model evaluation comprises processes and metrics that check 
   how well the model performs on both the training data and, more importantly, on data that simulates real-world applications. 
   This helps avoid misleading predictions and minimizes risks when the model is deployed.

2. What is Model Evaluation?
   Model evaluation is the systematic process of checking a model’s performance to determine:
   -a. Prediction Quality: How close the model’s predictions are to the actual values.
   -b. Generalization Capability: Whether the model can correctly predict new data that it has not seen during training.
   The ultimate goal is to simulate the final application during model creation, ensuring that the model will work effectively 
   in practice.

3. Key Goals of Model Evaluation
   -a. Performance Evaluation:
       Measure and compare the performance of different model architectures using standardized metrics.
   -b. Robustness:
       Ensure that the model not only fits the training data but also performs well when encountering new, 
       slightly different data.
   -c. Generalization Capability:
       Determine if the model has truly learned the underlying patterns in the data rather than merely memorizing the training
       samples (overfitting) or failing to capture important details (underfitting).

4. Evaluation Methods
   Two widely used methods for simulating a real-world use case with unseen data are:
   -a. Train/Test Split:
       -1. Training Data: Used during model training to adjust parameters and capture underlying data structures.
       -2. Test Data: Held out during training to simulate new data and evaluate the model’s predictive performance.
       This method checks if the model generalizes well by comparing performance on training data versus unseen test data.
   -b. Cross-Validation:
       -1. K-Fold Cross-Validation: The dataset is split into k equal parts (folds). The model is trained k times, 
                                    each time using a different fold as the test set and the remaining folds as training data.
       -2. Leave-One-Out Cross-Validation: An extreme form where each test fold consists of one data point.
       Cross-validation provides more reliable performance estimates, especially when data is limited, 
       by testing various training–test splits.

5. Metrics for Model Evaluation
   The article describes key metrics used for evaluating different types of models:
   -a. For Regression Models
       -1. Mean Absolute Error (MAE):
           - Calculates the average absolute difference between predicted and actual values.
           - Advantages: Simple to interpret and robust to outliers.
           - Disadvantages: Treats all errors equally, regardless of their magnitude.
       -2. Mean Squared Error (MSE):
           - Squares the differences before averaging, which penalizes larger errors more strongly.
           - Advantages: Provides mathematical properties useful for optimization (differentiable).
           - Disadvantages: More sensitive to outliers due to the squaring effect.
   -b. For Classification Models
       -1. Accuracy:
           - The ratio of correctly classified instances to the total number of instances.
           - Limitation: Can be misleading on unbalanced datasets.
       -2. Precision:
           - Measures the proportion of positive predictions that are truly positive. Critical when false positives are particularly
             undesirable (e.g., in spam detection).
       -3. Recall (Sensitivity):
           - Measures the proportion of actual positive instances that the model correctly identified. 
             Essential in contexts where missing a positive (false negative) is more critical (e.g., disease detection).
       -4. F1 Score:
           - The harmonic mean of precision and recall. Useful when both false positives and false negatives need to be balanced,
             especially in unbalanced datasets.
   -c. For Clustering Models
       -1. Silhouette Score:
           Evaluates the consistency within clusters compared to other clusters.
           - a(i): Average intra-cluster distance for data point i.
           - b(i): Average distance to the nearest cluster for data point i.
           Values range from -1 to 1, where higher values indicate well-clustered data.
       -2. Adjusted Rand Index (ARI):
           Compares the predicted clusters with a ground-truth classification, assessing the similarity between 
           the two assignments while correcting for chance.

6. Evaluating Generalization Ability
   The article also explains two methods to assess whether a model generalizes well:
   -a. Train-Test Gap:
       The difference in performance between the training dataset and the test dataset.
       - A small gap indicates that the model performs similarly on seen and unseen data.
       - A large gap can signal overfitting (excellent performance on training data but poor on test data) or 
         underfitting (poor performance on both).
   -b. Out-of-Sample Testing:
       Involves evaluating the model on entirely new data that wasn’t used during training or validation, 
       often collected from different time periods or regions. This method tests whether the model can maintain performance
       despite slight shifts in data distribution.

7. Conclusion
   Model evaluation is a critical step in machine learning that ensures models are not only accurate on the training data 
   but also robust and generalizable to new data. The process uses various methods—such as train/test splits 
   and cross-validation—and relies on specific metrics tailored to regression, classification, or clustering tasks.
   Evaluating the generalization ability through metrics like the train-test gap and out-of-sample testing is essential 
   to avoid overfitting and to build models that perform reliably in real-world applications.
