### From https://cdn.prod.website-files.com/67160e0e6bd76875921fe9a7/67d3e9e44cdcbabf47c1bc16_7a1f7f7d4b4c1e218586375fb60a0078_A_Feature_Level_Approach_to_Mitigating_Bias_and_Censorship_in_LLMs.pdf

1. Context & Motivation
   Large-scale LLMs often over-censor or exhibit unwanted biases because alignment training and imbalanced data embed filters 
   and cultural/political slants. Traditional fixesâ€”fine-tuning or RLHFâ€”are costly, static, and can introduce new biases. 
   Prompt engineering offers only coarse control. 
   Recent interpretability work hints at latent features (â€œmonosemanticâ€ neurons) tied to behaviors, 
   but extracting or steering them at scale is prohibitive.

   This work proposes a lightweight, fine-grained framework that directly locates and modifies the internal features responsible
   for censorship (or other biases) at inference time, with zero retraining.

2. Core Idea
   LLMsâ€™ hidden states contain latent variables (neurons or subspace directions) aligned with high-level behaviorsâ€”e.g., 
   a â€œcensorship trigger.â€ 
   By (a) identifying those features, 
      (b) verifying they causally drive refusals, 
      and (c) inserting a tiny intervention module that shifts activations, 
  one can toggle or tune censorship on the fly without touching model weights.

3. Methodology Overview
   The procedure has three stages:
   -a. Feature Identification
   -b. Feature Isolation & Characterization
   -c. Dynamic Feature Modification at Runtime

   3.1 Feature Identification
       -a. Data Collection: Assemble two prompt sets:
           -1. Trigger prompts that reliably induce refusal or safe-completions 
               (e.g. â€œTell me about Tiananmen Square protestâ€, â€œHow to bypass censorship?â€).
           -2. Control prompts on similar topics that should be answered normally.
      -b. Activation Recording: Run the base model on all prompts, log hidden activations (across layers) and final refusal vs. answer.
      -c. Cluster & Correlate: Find mid/penultimate-layer representations where trigger prompts cluster separately. 
          Identify candidate features ğ‘“_ğ‘– whose average activation differs strongly between trigger vs. control. 
          Formally, compute
          Î”_ğ‘–=ğ¸[ğ‘“_ğ‘–âˆ£refusal]âˆ’ğ¸[ğ‘“_ğ‘–âˆ£allowed]
          and correlate ğ‘“_ğ‘– with the logit or probability of the â€œ[REFUSAL]â€ token.
   3.2 Feature Isolation & Characterization
       -a. Causality Test: Manually amplify or suppress a candidate feature in the modelâ€™s activations and observe output:
                           boosting it should force refusal; damping it yields normal answers.
       -b. Concept Vector: Solve for a unit direction ğ‘£_censor in hidden space that best separates censored vs. allowed activations 
           (e.g., via logistic regression or PCA on activations). Denote that direction as the â€œcensorship concept.â€
       -c. Equation: To neutralize the feature at runtime, project and subtract:
           â„â€²=â„âˆ’ğ›¼(â„â‹…ğ‘£_censor),
           where â„ is the hidden state, and ğ›¼ controls intervention strength (0 â‰¤ Î± â‰¤ 1 for partial to full suppression).
   3.3 Dynamic Modification at Runtime
       -a. Insertion Point: Add a tiny module in the inference pipeline at the layer housing ğ‘£_censor
       -b. Adaptive Triggering:
           -1. Unconditional mode: Always apply intervention for an â€œuncensoredâ€ setting.
           -2. Heuristic mode: Monitor â„â‹…ğ‘£_censor; if above a threshold (i.e. censorship spike), apply the subtraction.
       -c. Tunability & Reversibility: No weight changesâ€”just a runtime transform. Users can adjust Î± 
           (e.g., a â€œstrictness sliderâ€) per request or jurisdiction.
       -d. Overhead: Negligible latency; immediate effect on the very next token.

4. Extensions & Benefits
   -a. Bias Mitigation: Apply the same pipeline to identify â€œgender-biasâ€ or other demographic concept vectors and neutralize them.
   -b. Multi-Concept Steering: Maintain a set of concept vectors (censorship, toxicity, style, demographic biases), 
                               and apply simultaneous adjustments in hidden space for complex behavioral control.
   -c. Advantages over Retraining:
       -1. Immediate: No hours/days of fine-tuning.
       -2. Lightweight: Modest compute and no new data.
       -3. Reversible: Toggle or tune at runtime without altering core model.
       -4. Fineâ€Grained: Direct control over specific behaviors, preserving overall model capacity and factual accuracy.
