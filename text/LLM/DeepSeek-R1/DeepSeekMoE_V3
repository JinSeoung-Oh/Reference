### From https://pub.towardsai.net/deepseek-v3-part-2-deepseekmoe-f3ae6ff9e296

1 Mixture-of-Experts (MoE) inside Transformer LLMs
  -a. Architecture swap
      -1. Dense baseline: every layer = MHA â†’ FFN.
      -2. MoE variant: in alternating (or otherwise configurable) layers the FFN sub-layer is replaced by an MoE layer.
  -b. Inside one MoE layer
      -1. Gating module computes a route for each token.
      -2. Experts are independent FFNs; only K of N are activated per token, making the forward pass sparse and cheap.
  -c. Classical MoE equations (using original numbering 5 â†’ 4 â†’ 3)
      (5) ğ‘ _(ğ‘–,ğ‘¡) = softmax(ğ‘¢^ğ‘™_ğ‘¡â‹…ğ‘’^ğ‘™_ğ‘–)              similarityÂ toÂ expertÂ centroid
      (4) ğ‘”_(ğ‘–,ğ‘¡) = TopK(ğ‘ _(ğ‘–,ğ‘¡))                   sparseÂ gateÂ (KÂ non-zeroÂ values)
      (3) â„^ğ‘™_ğ‘¡ =âˆ‘(ğ‘–=1 to ğ‘–=ğ‘)ğ‘”_(ğ‘–,ğ‘¡)FFN_ğ‘–(ğ‘¢^ğ‘™_ğ‘¡)     tokenÂ output
      Only K experts expend compute, although the model holds Nâ‰«K full FFNsâ€”e.g. 236 B parameters total, 21 B active.

2 Why MoE? Benefits and Challenges
  Benefit	| Explanation
  Capacity â†‘ with cost â‰ˆ constant	| Total weights grow; per-token FLOPs stay low (only K experts run).
  Real-world analogy	| Many specialist chefs + head-chef router vs. one â€œdo-everythingâ€ chef.

  ----------------------------------------------------------------------------------------------------

  Challenge	| Symptom
  Expert-collapse / load imbalance	| Few experts overused, others under-trained.
  Instability	| Wrongly routed tokens hit poorly trained experts.
  Specialization â†” Sharing trade-off	| Over-specialization hurts coverage; redundancy wastes parameters.

3 DeepSeekMoE Architecture
  DeepSeekMoE adds two mechanisms to balance specialization and sharing without raising compute:
  3.1 Fine-Grained Expert Segmentation
      Multiply experts ğ‘â†’ğ‘šğ‘; shrink each FFN width 1/ğ‘š; route each token to ğ‘šğ¾ experts.
      Result: same FLOPs as classic MoE but greater decomposition of knowledge â†’ stronger specialization.
  3.2 Shared-Expert Isolation
      Reserve ğ¾_ğ‘  shared experts that every token always visits (no routing).
      Router then selects (ğ‘šğ¾âˆ’ğ¾_ğ‘ ) specialist experts from (ğ‘šğ‘âˆ’ğ¾_ğ‘ )
      Common knowledge is centralized; redundancy among specialists drops.

4 DeepSeekMoE Equationsâ€”Complete Explanation
  Eqn.	| Formula |	Meaning in the text	| Key points
  (11)	| ğ‘ _(ğ‘–,ğ‘¡)=softmax(ğ‘¢_ğ‘¡â‹…ğ‘’_ğ‘–)	| Computes tokenâ€“expert similarity scores and normalizes them into a probability distribution. Captures how similar the current token ğ‘¢_ğ‘¡ is to expert ğ‘–â€™s historical centroid ğ‘’_ğ‘– | Generates the raw routing signal; identical to classic Eq.(5).
  (10)	| ğ‘”_(ğ‘–,ğ‘¡)=TopK_(ğ‘šğ¾âˆ’ğ¾_ğ‘ )(ğ‘ _(ğ‘–,ğ‘¡) (ğ‘–âˆˆnon-shared) | 	From those similarity scores, keep only the top (ğ‘šğ¾âˆ’ğ¾_ğ‘ ) among the non-shared experts. The gate values for all other non-shared experts are set to zero. Shared experts are excluded from this Top-K step because they are always active.	| Determines which specialist experts are activated for this token.
  (9)	  | â„_ğ‘¡=âˆ‘(ğ‘–âˆˆshared)FFN_ğ‘–(ğ‘¢_ğ‘¡) +â€…âˆ‘(ğ‘–âˆˆrouted)ğ‘”_(ğ‘–,ğ‘¡)FFN_ğ‘–(ğ‘¢_ğ‘¡) | The tokenâ€™s final hidden state â„_ğ‘¡ is the sum of two disjoint contributions: (1) all shared experts (always on), and (2) the gated set of specialist experts. The total number of active experts equals ğ¾_ğ‘ +(ğ‘šğ¾âˆ’ğ¾_ğ‘ )=ğ‘šğ¾, exactly matching the compute budget implied by fine-grained segmentation | Implements a two-path design that (i) concentrates universal knowledge in shared experts, (ii) allows specialists to focus on niche knowledge, and (iii) preserves constant per-token FLOPs.
                  âŸ                              âŸ
          shared-expertÂ path               specialistÂ path

  -a. Flow of computation
      -1. Eqn 11 â€“ score similarities â†’ probability distribution.
      -2. Eqn 10 â€“ pick top (ğ‘šğ¾âˆ’ğ¾_ğ‘ ) specialist routes (shared experts already selected).
      -3. Eqn 9 â€“ add outputs of shared + specialist experts to produce the token representation.

5 Empirical Findings (Fig. 7â€“8)
  -a. Overall quality
      -1. With equal activated parameters, MoE â‰« Dense.
      -2. DeepSeekMoE outperforms GShard at the same compute budget.
  -b. Specialization test (disable top-routed experts)
      -1. Pile loss increases faster for DeepSeekMoE â†’ its experts are more specialized and less replaceable.
  -c. Redundancy test (remove shared expert, add one extra specialist)
      -1. Pile loss jumps 1.808 â†’ 2.414 â†’ shared expert holds unique general knowledge not replicated by specialists.

6 Key Takeaways from the Text
  -a. MoE principle: grow total capacity massively but keep per-token compute small via sparse expert activation.
  -b. DeepSeekMoE advances:
      -1. Fine-grained segmentation spreads knowledge across many small experts, boosting specialization.
      -2. Shared-expert isolation funnels general knowledge into a fixed subset, cutting redundancy among specialists.
  -c. Result: better benchmark scores and demonstrably stronger specialization/sharing balance than previous MoE variantsâ€”without 
              increasing computational cost per token.

