### From https://ai.gopubby.com/how-deepseek-r1-pushes-the-limits-of-language-models-a-mathematical-dive-into-group-relative-79dba9906f94

1. Overview and Motivation
   DeepSeek-R1 is an open‐source large language model that rivals top proprietary systems in complex reasoning. 
   Its distinguishing factor is a novel reinforcement learning algorithm called Group Relative Policy Optimization (GRPO). 
   Unlike traditional methods that depend on a pipeline of supervised fine-tuning (SFT) and extensive human feedback, 
   GRPO enhances reasoning abilities without needing a massive value network or large-scale human annotations.

   The post is organized into three main parts:
   -a. Derivation Deep-Dive: Explaining GRPO’s relationship to the policy gradient theorem, why the group mean is subtracted, 
                             and how token-level versus sequence-level treatment is handled.
   -b. Sensitivity to Key Hyperparameters: Exploring how parameters like group size (G), the KL penalty coefficient (β), 
                                           and reward variance influence training stability and performance.
   -c. Geometric Perspective: Discussing GRPO’s connection to trust regions, Fisher information, and natural gradients,
                              which together underpin its exceptionally stable updates.

2. GRPO: Core Equation and Mechanism
   -a. Sampling and Scoring
       -1. Sampling Outputs: For each prompt (or question), the model samples a group of candidate outputs using its current 
                             (or “old”) policy.
       -2. Reward Assignment: A reward function scores each candidate output, providing a quantitative measure
                              (e.g., based on accuracy or conciseness).
   -b. Baseline and Advantage Computation
       -1. Group-Based Baseline:
           -1) The average reward of the sampled group is used as a local baseline.
           -2) This baseline serves as an empirical estimate of the expected reward for that prompt.
       -2. Advantage Estimation:
           -1) Each output’s advantage is calculated as the difference between its reward and the group’s mean, 
               normalized by the group’s standard deviation.
           -2) All tokens in an output receive the same advantage since the reward is assigned at the sequence level.
   -c. GRPO Objective
       The loss function is designed to maximize the probability of outputs that score above the baseline while ensuring 
       stability by:
       -1. Clipping: Similar to Proximal Policy Optimization (PPO), clipping restricts the probability ratio to prevent 
                     overly large updates.
       -2. KL Regularization: A KL divergence term penalizes deviations from a reference policy 
                              (often the initial model or a prior checkpoint), keeping the updated policy within a “trust region.”
   -d. Why It Works:
       By comparing each output against its peers, the model effectively “competes with itself.” 
       Outputs that exceed the group average receive positive reinforcement, while those below the average are discouraged. 
       This method eliminates the need for a separately learned value function, 
       streamlining training and reducing computational resources.

3. In-Depth Derivation
   -a. Relation to the Policy Gradient Theorem
       -1. Classic Policy Gradient:
           In traditional policy gradient methods (e.g., REINFORCE), the gradient is computed as the expectation
           over trajectories with an advantage term 𝐴(𝜏)=𝑄(𝜏)−𝑏, where 𝑏 is any baseline.
       -2. GRPO’s Adaptation:
           GRPO uses the group mean reward as the baseline:
           𝐴(𝜏)=𝑟_𝑖−mean(𝑟)
           This approach is then normalized by the group’s reward standard deviation to further reduce variance. 
           The result is mathematically equivalent to standard policy gradient methods but with a locally computed, 
           sample-based baseline.
   -b. Why Subtract the Group Mean?
       -1. Variance Reduction:
           Subtracting the group mean provides an unbiased, prompt-specific baseline. 
           If all outputs in the group are similar, the differences become small, significantly reducing the variance 
           of the advantage estimates.
       -2. No Need for a Learned Critic:
           Unlike traditional actor-critic methods that require a separate critic network, 
           GRPO leverages this group-based baseline to approximate the expected reward, 
           making the training process simpler and more efficient.
   -c. Token-Level vs. Sequence-Level Treatment
       -1. Uniform Advantage Across Tokens:
           Since the reward is typically assigned at the end of the sequence, every token in an output is scaled 
           by the same advantage value.
       -2. Token-Wise Gradient Scaling:
           Although the advantage is constant across tokens in one output, the individual contribution of each token 
           (e.g., through probability ratios) can vary. 
           The final gradient is an aggregate that benefits from both the token-level detail and the sequence-level advantage.

4. Sensitivity to Key Hyperparameters
   -a. Group Size (G)
       -1. Definition and Impact:
           Group size 𝐺 is the number of candidate outputs sampled per prompt. 
           A larger group provides a more accurate estimate of the expected reward, reducing the variance in advantage estimates.
       -2. Learning Signal:
           -1) Small 𝐺 (e.g., 𝐺=1): The baseline equals the reward, yielding zero advantage and no learning signal.
           -2) Moderate to Large 𝐺: Increasing 𝐺 (e.g., 8–16, or even 64 in some math reasoning challenges) reduces 
                                    variance and results in more stable gradients. However, returns diminish with very large 
                                    𝐺 due to computational costs versus incremental gains in variance reduction.
   -b. KL Penalty Coefficient (β)
       -1. Role of β:
           β controls how strongly the policy is penalized for diverging from a reference policy 
           (typically the pre-trained model). 
           It essentially balances:
           -1) Exploration: A lower β allows the policy to deviate more from the reference to maximize reward.
           -2) Stability: A higher β forces conservative updates, ensuring the model remains close to its initial behavior.
       -2. Practical Trade-Offs:
           -1) High β: Leads to very safe, incremental policy updates, reducing the risk of reward hacking but potentially 
                       slowing down improvement.
           -2) Low β: Enables faster reward optimization but can cause instability and significant drift from 
                      the reference policy.
       -3. Example:
           In some DeepSeek-Math experiments, a β of 0.04 was used to achieve a gentle regularization effect without 
           stalling learning.

5. Second-Order Optimization and Geometric Insights
   -a. Trust Region Policy Optimization (TRPO) Connection
       -1. TRPO Overview:
           TRPO aims to maximize the policy advantage while constraining the update within a trust region defined by 
           KL divergence limits.
       -2. GRPO’s Analogy:
           GRPO uses PPO-style clipping and includes a KL penalty in its objective, 
           which together enforce a trust region effect. 
           This means that each update is both safe and incremental, similar to the constrained optimization of TRPO.
   -b. Anchoring the Policy
       -1. Fixed Reference Policy:
           In DeepSeek-R1, the reference policy (𝜋_ref) is typically the initial pre-trained model. 
           This “anchor” ensures that each update is evaluated in the context of the original model, 
           helping maintain consistent behavior and avoiding drastic shifts.
   -c. Fisher Information and Natural Gradients
       -1. Fisher Information Matrix (FIM):
           Near the reference policy, the Hessian of the KL divergence approximates the FIM. 
           Directions in parameter space that lead to large changes in token probabilities are penalized more heavily. 
       -2. Natural Gradient Interpretation:
           This relationship means that GRPO’s updates are adjusted according to the local geometry of the probability space,
           leading to more stable, parameterization-invariant updates akin to natural gradient methods.
   -d. Riemannian Geometry Perspective
       -1. Local Metric:
           KL divergence can be seen as defining a local metric on the space of policies. 
           This geometric interpretation underlies GRPO’s ability to take updates that are small in distribution space—even 
           if they appear large in raw parameter space—ensuring stability throughout training.

6. Conclusion: The Impact of GRPO’s Mathematical Foundations
   -a. Group-Based Advantage:
       GRPO’s use of a local, group-based baseline eliminates the need for a separate critic network, 
       simplifying the learning process.
   -b. Variance Reduction:
       Normalizing by the group’s standard deviation reduces noise in the gradient estimates, 
       leading to more reliable updates.
   -c. Clipping & KL Regularization:
       These mechanisms act as double safeguards:
       -1. Clipping limits excessive changes to the policy.
       -2. KL Penalty ensures the new policy remains anchored to the reference, 
           preventing reward hacking and catastrophic drifts.
   -d. Second-Order Benefits:
       By connecting the KL term to natural gradient methods and the Fisher Information Matrix, 
       GRPO implicitly accounts for the curvature of the probability space, 
       resulting in highly stable and incremental policy improvements.
   -e. Practical Advantages:
       This blend of techniques makes GRPO a practical and efficient alternative to standard RLHF approaches, 
       particularly for fine-tuning large language models where final rewards are clear 
       (e.g., correct versus incorrect responses). 
       The method achieves strong reasoning capabilities with reduced computational complexity and training noise.
   In essence, GRPO can be viewed as “PPO with a locally computed group baseline” that leverages both first- and second-order
   optimization principles to enable stable, efficient, and effective training of advanced language models like DeepSeek-R1.


