# RLHF(Reinforcement Learning with Human Feedfack)
The RLHF pipline taskes a pretrained model and finetunes it in a supervised fashion and further aligns it with proximal policy optimization
step 1. Supervised finetuning of the pretrained model
step 2. Creating a reward model
step 3. Finetuning via proximal policy optimization

Step 1 is a supervised finetuning step to create the base model for futher RLHF finetuning
create or sample prompts and ask humans to write good-quality responses. Then use this dataset to finetune the pretrained base model in a supervised manner

Step 2, use step 1 model, create a reward model
For each prompt, generate four to nine responses from the finetuned LLM(step 1 model). An individual then ranks theses responses based on their(human) preference. 
Upon compiling a dataset with these rankings, we can design a reward model that outputs a reward score for the optimization subsequent stage
This reward model generally originates from the LLM(step 1 model)

Step3, updating the supervised finetuning model using proximal policy optimization based on the reward scores from the reward model(step 2)

# RLHF alternatives
1. Constitutional AI
In this paper, researchers propose a self-tarining mechanism based on a list of rules humans provide. Similar to the InstructGPT paper, he proposed method uses a reinforcement learning approach
--> https://arxiv.org/abs/2212.08073

2. The Widsom of Hindsight makes Language Models Better Instrunction Followers
In this, researchers propose a relabeling-based supervised approach for finetuning that outperorms RLHF on 12 BigBench tasks
HIR(Hindsight Instruction Labeling) consists of two steps
Step 1. Sampling --> prompts and instructions are fed to the LLM to collect the responses. Based on an alignment scroe, the instruction is relabled where appropriate in the training phase
Step 2. training --> the relabled instructions and the original prompts are used to finetune the LLM. Using this relabeling approach, the researchers effectively turn failure cases into useful training data for supervised learning
--> https://arxiv.arg/abs/2302.05206

3. Direct Perference Optimization 
Driect perference Optimization(DPO) is an alternative to RLHF with PPO whre the reserarchers show that the cross entropy loss for fitting the reward model in RLHF can be used directly to finetune the LLM
--> https://arxiv.org/abs/2305.18290

4. Reinforced self-training(ReST) for Language Modeling
ReST uses a sampling approach to create an improved dataset, iteratively training on increasingly higher-quality subsets to refine its reward function.
--> https://arxiv.org/abs/2308.08998

5. RLAIF: scaling reinforcement learning from human feedback with Al feedback
The recent reinforcement learning with AI feedback study shows that the ratings for the reward model training in RLHF don't necessarily have to be provided by human but can be generated by an LLM
--> https://arxiv.org/abs/2309.00267
